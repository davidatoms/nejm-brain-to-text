2025-07-29 22:46:41,211: Requested GPU 1 not available. Using GPU 0 instead.
2025-07-29 22:46:41,375: Using device: cuda:0
2025-07-29 22:46:42,200: Using torch.compile
2025-07-29 22:48:21,986: Requested GPU 1 not available. Using GPU 0 instead.
2025-07-29 22:48:22,153: Using device: cuda:0
2025-07-29 22:48:22,784: Using torch.compile
2025-07-29 22:48:22,784: Initialized RNN decoding model
2025-07-29 22:48:22,785: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_weights): ParameterList(
      (0): Parameter containing: [torch.float32 of size 512x512]
      (1): Parameter containing: [torch.float32 of size 512x512]
      (2): Parameter containing: [torch.float32 of size 512x512]
      (3): Parameter containing: [torch.float32 of size 512x512]
      (4): Parameter containing: [torch.float32 of size 512x512]
      (5): Parameter containing: [torch.float32 of size 512x512]
      (6): Parameter containing: [torch.float32 of size 512x512]
      (7): Parameter containing: [torch.float32 of size 512x512]
      (8): Parameter containing: [torch.float32 of size 512x512]
      (9): Parameter containing: [torch.float32 of size 512x512]
      (10): Parameter containing: [torch.float32 of size 512x512]
      (11): Parameter containing: [torch.float32 of size 512x512]
      (12): Parameter containing: [torch.float32 of size 512x512]
      (13): Parameter containing: [torch.float32 of size 512x512]
      (14): Parameter containing: [torch.float32 of size 512x512]
      (15): Parameter containing: [torch.float32 of size 512x512]
      (16): Parameter containing: [torch.float32 of size 512x512]
      (17): Parameter containing: [torch.float32 of size 512x512]
      (18): Parameter containing: [torch.float32 of size 512x512]
      (19): Parameter containing: [torch.float32 of size 512x512]
      (20): Parameter containing: [torch.float32 of size 512x512]
      (21): Parameter containing: [torch.float32 of size 512x512]
      (22): Parameter containing: [torch.float32 of size 512x512]
      (23): Parameter containing: [torch.float32 of size 512x512]
      (24): Parameter containing: [torch.float32 of size 512x512]
      (25): Parameter containing: [torch.float32 of size 512x512]
      (26): Parameter containing: [torch.float32 of size 512x512]
      (27): Parameter containing: [torch.float32 of size 512x512]
      (28): Parameter containing: [torch.float32 of size 512x512]
      (29): Parameter containing: [torch.float32 of size 512x512]
      (30): Parameter containing: [torch.float32 of size 512x512]
      (31): Parameter containing: [torch.float32 of size 512x512]
      (32): Parameter containing: [torch.float32 of size 512x512]
      (33): Parameter containing: [torch.float32 of size 512x512]
      (34): Parameter containing: [torch.float32 of size 512x512]
      (35): Parameter containing: [torch.float32 of size 512x512]
      (36): Parameter containing: [torch.float32 of size 512x512]
      (37): Parameter containing: [torch.float32 of size 512x512]
      (38): Parameter containing: [torch.float32 of size 512x512]
      (39): Parameter containing: [torch.float32 of size 512x512]
      (40): Parameter containing: [torch.float32 of size 512x512]
      (41): Parameter containing: [torch.float32 of size 512x512]
      (42): Parameter containing: [torch.float32 of size 512x512]
      (43): Parameter containing: [torch.float32 of size 512x512]
      (44): Parameter containing: [torch.float32 of size 512x512]
  )
  (day_biases): ParameterList(
      (0): Parameter containing: [torch.float32 of size 1x512]
      (1): Parameter containing: [torch.float32 of size 1x512]
      (2): Parameter containing: [torch.float32 of size 1x512]
      (3): Parameter containing: [torch.float32 of size 1x512]
      (4): Parameter containing: [torch.float32 of size 1x512]
      (5): Parameter containing: [torch.float32 of size 1x512]
      (6): Parameter containing: [torch.float32 of size 1x512]
      (7): Parameter containing: [torch.float32 of size 1x512]
      (8): Parameter containing: [torch.float32 of size 1x512]
      (9): Parameter containing: [torch.float32 of size 1x512]
      (10): Parameter containing: [torch.float32 of size 1x512]
      (11): Parameter containing: [torch.float32 of size 1x512]
      (12): Parameter containing: [torch.float32 of size 1x512]
      (13): Parameter containing: [torch.float32 of size 1x512]
      (14): Parameter containing: [torch.float32 of size 1x512]
      (15): Parameter containing: [torch.float32 of size 1x512]
      (16): Parameter containing: [torch.float32 of size 1x512]
      (17): Parameter containing: [torch.float32 of size 1x512]
      (18): Parameter containing: [torch.float32 of size 1x512]
      (19): Parameter containing: [torch.float32 of size 1x512]
      (20): Parameter containing: [torch.float32 of size 1x512]
      (21): Parameter containing: [torch.float32 of size 1x512]
      (22): Parameter containing: [torch.float32 of size 1x512]
      (23): Parameter containing: [torch.float32 of size 1x512]
      (24): Parameter containing: [torch.float32 of size 1x512]
      (25): Parameter containing: [torch.float32 of size 1x512]
      (26): Parameter containing: [torch.float32 of size 1x512]
      (27): Parameter containing: [torch.float32 of size 1x512]
      (28): Parameter containing: [torch.float32 of size 1x512]
      (29): Parameter containing: [torch.float32 of size 1x512]
      (30): Parameter containing: [torch.float32 of size 1x512]
      (31): Parameter containing: [torch.float32 of size 1x512]
      (32): Parameter containing: [torch.float32 of size 1x512]
      (33): Parameter containing: [torch.float32 of size 1x512]
      (34): Parameter containing: [torch.float32 of size 1x512]
      (35): Parameter containing: [torch.float32 of size 1x512]
      (36): Parameter containing: [torch.float32 of size 1x512]
      (37): Parameter containing: [torch.float32 of size 1x512]
      (38): Parameter containing: [torch.float32 of size 1x512]
      (39): Parameter containing: [torch.float32 of size 1x512]
      (40): Parameter containing: [torch.float32 of size 1x512]
      (41): Parameter containing: [torch.float32 of size 1x512]
      (42): Parameter containing: [torch.float32 of size 1x512]
      (43): Parameter containing: [torch.float32 of size 1x512]
      (44): Parameter containing: [torch.float32 of size 1x512]
  )
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.4)
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2025-07-29 22:48:22,788: Model has 44,315,177 parameters
2025-07-29 22:48:22,788: Model has 11,819,520 day-specific parameters | 26.67% of total parameters
2025-07-29 22:48:36,983: Successfully initialized datasets
2025-07-29 22:48:40,854: Train batch 0: loss: 755.13 grad norm: 293.68 time: 2.062
2025-07-29 22:48:40,854: Running test after training batch: 0
2025-07-29 22:48:54,446: Val batch 0: PER (avg): 1.2158 CTC Loss (avg): 714.9869 time: 13.591
2025-07-29 22:48:54,446: t15.2023.08.13 val PER: 1.1227
2025-07-29 22:48:54,446: t15.2023.08.18 val PER: 1.1450
2025-07-29 22:48:54,446: t15.2023.08.20 val PER: 1.1525
2025-07-29 22:48:54,446: t15.2023.08.25 val PER: 1.1642
2025-07-29 22:48:54,446: t15.2023.08.27 val PER: 1.0675
2025-07-29 22:48:54,447: t15.2023.09.01 val PER: 1.2248
2025-07-29 22:48:54,447: t15.2023.09.03 val PER: 1.1176
2025-07-29 22:48:54,447: t15.2023.09.24 val PER: 1.3313
2025-07-29 22:48:54,447: t15.2023.09.29 val PER: 1.2425
2025-07-29 22:48:54,447: t15.2023.10.01 val PER: 1.0555
2025-07-29 22:48:54,447: t15.2023.10.06 val PER: 1.2487
2025-07-29 22:48:54,447: t15.2023.10.08 val PER: 1.0433
2025-07-29 22:48:54,447: t15.2023.10.13 val PER: 1.1614
2025-07-29 22:48:54,448: t15.2023.10.15 val PER: 1.2037
2025-07-29 22:48:54,448: t15.2023.10.20 val PER: 1.3557
2025-07-29 22:48:54,448: t15.2023.10.22 val PER: 1.2906
2025-07-29 22:48:54,448: t15.2023.11.03 val PER: 1.2999
2025-07-29 22:48:54,448: t15.2023.11.04 val PER: 1.3788
2025-07-29 22:48:54,448: t15.2023.11.17 val PER: 1.6470
2025-07-29 22:48:54,448: t15.2023.11.19 val PER: 1.4371
2025-07-29 22:48:54,448: t15.2023.11.26 val PER: 1.2616
2025-07-29 22:48:54,449: t15.2023.12.03 val PER: 1.1838
2025-07-29 22:48:54,449: t15.2023.12.08 val PER: 1.2557
2025-07-29 22:48:54,449: t15.2023.12.10 val PER: 1.3219
2025-07-29 22:48:54,449: t15.2023.12.17 val PER: 1.0707
2025-07-29 22:48:54,449: t15.2023.12.29 val PER: 1.1627
2025-07-29 22:48:54,449: t15.2024.02.25 val PER: 1.1306
2025-07-29 22:48:54,449: t15.2024.03.08 val PER: 1.1593
2025-07-29 22:48:54,449: t15.2024.03.15 val PER: 1.1107
2025-07-29 22:48:54,450: t15.2024.03.17 val PER: 1.1729
2025-07-29 22:48:54,450: t15.2024.05.10 val PER: 1.2051
2025-07-29 22:48:54,450: t15.2024.06.14 val PER: 1.4101
2025-07-29 22:48:54,450: t15.2024.07.19 val PER: 0.9835
2025-07-29 22:48:54,450: t15.2024.07.21 val PER: 1.4214
2025-07-29 22:48:54,450: t15.2024.07.28 val PER: 1.4684
2025-07-29 22:48:54,450: t15.2025.01.10 val PER: 0.9532
2025-07-29 22:48:54,450: t15.2025.01.12 val PER: 1.4211
2025-07-29 22:48:54,451: t15.2025.03.14 val PER: 1.0207
2025-07-29 22:48:54,451: t15.2025.03.16 val PER: 1.4202
2025-07-29 22:48:54,451: t15.2025.03.30 val PER: 1.0805
2025-07-29 22:48:54,451: t15.2025.04.13 val PER: 1.3024
2025-07-29 22:48:54,451: New best test PER inf --> 1.2158
2025-07-29 22:48:54,451: Checkpointing model
2025-07-29 22:48:55,185: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-07-29 22:49:25,555: Train batch 200: loss: 93.92 grad norm: 21.95 time: 0.086
2025-07-29 22:49:55,434: Train batch 400: loss: 74.98 grad norm: 41.57 time: 0.103
2025-07-29 22:50:25,353: Train batch 600: loss: 58.92 grad norm: 39.96 time: 0.069
2025-07-29 22:50:55,755: Train batch 800: loss: 51.98 grad norm: 54.37 time: 0.102
2025-07-29 22:51:25,772: Train batch 1000: loss: 45.07 grad norm: 46.01 time: 0.087
2025-07-29 22:51:55,475: Train batch 1200: loss: 28.62 grad norm: 38.78 time: 0.106
2025-07-29 22:52:25,340: Train batch 1400: loss: 16.52 grad norm: 27.26 time: 0.085
2025-07-29 22:52:55,746: Train batch 1600: loss: 28.87 grad norm: 40.48 time: 0.093
2025-07-29 22:53:25,532: Train batch 1800: loss: 25.93 grad norm: 43.37 time: 0.070
2025-07-29 22:53:56,182: Train batch 2000: loss: 13.09 grad norm: 29.22 time: 0.086
2025-07-29 22:53:56,183: Running test after training batch: 2000
2025-07-29 22:54:09,471: Val batch 2000: PER (avg): 0.2270 CTC Loss (avg): 22.8548 time: 13.288
2025-07-29 22:54:09,472: t15.2023.08.13 val PER: 0.1840
2025-07-29 22:54:09,472: t15.2023.08.18 val PER: 0.1844
2025-07-29 22:54:09,472: t15.2023.08.20 val PER: 0.1739
2025-07-29 22:54:09,472: t15.2023.08.25 val PER: 0.1581
2025-07-29 22:54:09,472: t15.2023.08.27 val PER: 0.2605
2025-07-29 22:54:09,472: t15.2023.09.01 val PER: 0.1356
2025-07-29 22:54:09,472: t15.2023.09.03 val PER: 0.2340
2025-07-29 22:54:09,472: t15.2023.09.24 val PER: 0.1820
2025-07-29 22:54:09,472: t15.2023.09.29 val PER: 0.1870
2025-07-29 22:54:09,473: t15.2023.10.01 val PER: 0.2305
2025-07-29 22:54:09,473: t15.2023.10.06 val PER: 0.1572
2025-07-29 22:54:09,473: t15.2023.10.08 val PER: 0.3085
2025-07-29 22:54:09,473: t15.2023.10.13 val PER: 0.2925
2025-07-29 22:54:09,473: t15.2023.10.15 val PER: 0.2169
2025-07-29 22:54:09,473: t15.2023.10.20 val PER: 0.2047
2025-07-29 22:54:09,473: t15.2023.10.22 val PER: 0.1860
2025-07-29 22:54:09,473: t15.2023.11.03 val PER: 0.2368
2025-07-29 22:54:09,473: t15.2023.11.04 val PER: 0.0444
2025-07-29 22:54:09,473: t15.2023.11.17 val PER: 0.0995
2025-07-29 22:54:09,474: t15.2023.11.19 val PER: 0.0978
2025-07-29 22:54:09,474: t15.2023.11.26 val PER: 0.2471
2025-07-29 22:54:09,474: t15.2023.12.03 val PER: 0.2069
2025-07-29 22:54:09,474: t15.2023.12.08 val PER: 0.2197
2025-07-29 22:54:09,474: t15.2023.12.10 val PER: 0.1708
2025-07-29 22:54:09,474: t15.2023.12.17 val PER: 0.2422
2025-07-29 22:54:09,474: t15.2023.12.29 val PER: 0.2203
2025-07-29 22:54:09,474: t15.2024.02.25 val PER: 0.1798
2025-07-29 22:54:09,474: t15.2024.03.08 val PER: 0.3073
2025-07-29 22:54:09,474: t15.2024.03.15 val PER: 0.2983
2025-07-29 22:54:09,475: t15.2024.03.17 val PER: 0.2385
2025-07-29 22:54:09,475: t15.2024.05.10 val PER: 0.2259
2025-07-29 22:54:09,475: t15.2024.06.14 val PER: 0.2303
2025-07-29 22:54:09,475: t15.2024.07.19 val PER: 0.3111
2025-07-29 22:54:09,475: t15.2024.07.21 val PER: 0.1524
2025-07-29 22:54:09,475: t15.2024.07.28 val PER: 0.2191
2025-07-29 22:54:09,475: t15.2025.01.10 val PER: 0.3953
2025-07-29 22:54:09,475: t15.2025.01.12 val PER: 0.2286
2025-07-29 22:54:09,475: t15.2025.03.14 val PER: 0.4186
2025-07-29 22:54:09,475: t15.2025.03.16 val PER: 0.2644
2025-07-29 22:54:09,476: t15.2025.03.30 val PER: 0.3460
2025-07-29 22:54:09,476: t15.2025.04.13 val PER: 0.2825
2025-07-29 22:54:09,476: New best test PER 1.2158 --> 0.2270
2025-07-29 22:54:09,476: Checkpointing model
2025-07-29 22:54:10,641: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-07-29 22:54:39,803: Train batch 2200: loss: 17.49 grad norm: 38.29 time: 0.094
2025-07-29 22:55:09,831: Train batch 2400: loss: 16.37 grad norm: 37.65 time: 0.092
2025-07-29 22:55:39,477: Train batch 2600: loss: 9.61 grad norm: 24.74 time: 0.071
2025-07-29 22:56:09,792: Train batch 2800: loss: 10.90 grad norm: 31.82 time: 0.111
2025-07-29 22:56:40,023: Train batch 3000: loss: 13.02 grad norm: 38.17 time: 0.088
2025-07-29 22:57:09,329: Train batch 3200: loss: 10.16 grad norm: 29.16 time: 0.074
2025-07-29 22:57:38,937: Train batch 3400: loss: 7.32 grad norm: 26.17 time: 0.090
2025-07-29 22:58:09,237: Train batch 3600: loss: 7.21 grad norm: 26.94 time: 0.097
2025-07-29 22:58:38,924: Train batch 3800: loss: 7.35 grad norm: 32.17 time: 0.082
2025-07-29 22:59:08,657: Train batch 4000: loss: 3.99 grad norm: 19.64 time: 0.084
2025-07-29 22:59:08,658: Running test after training batch: 4000
2025-07-29 22:59:22,091: Val batch 4000: PER (avg): 0.1606 CTC Loss (avg): 17.8132 time: 13.433
2025-07-29 22:59:22,091: t15.2023.08.13 val PER: 0.1320
2025-07-29 22:59:22,092: t15.2023.08.18 val PER: 0.1207
2025-07-29 22:59:22,092: t15.2023.08.20 val PER: 0.1025
2025-07-29 22:59:22,092: t15.2023.08.25 val PER: 0.1069
2025-07-29 22:59:22,092: t15.2023.08.27 val PER: 0.1994
2025-07-29 22:59:22,092: t15.2023.09.01 val PER: 0.0787
2025-07-29 22:59:22,092: t15.2023.09.03 val PER: 0.1485
2025-07-29 22:59:22,092: t15.2023.09.24 val PER: 0.1189
2025-07-29 22:59:22,092: t15.2023.09.29 val PER: 0.1455
2025-07-29 22:59:22,093: t15.2023.10.01 val PER: 0.1896
2025-07-29 22:59:22,093: t15.2023.10.06 val PER: 0.1227
2025-07-29 22:59:22,093: t15.2023.10.08 val PER: 0.2382
2025-07-29 22:59:22,093: t15.2023.10.13 val PER: 0.2312
2025-07-29 22:59:22,093: t15.2023.10.15 val PER: 0.1622
2025-07-29 22:59:22,093: t15.2023.10.20 val PER: 0.2114
2025-07-29 22:59:22,093: t15.2023.10.22 val PER: 0.1347
2025-07-29 22:59:22,093: t15.2023.11.03 val PER: 0.1825
2025-07-29 22:59:22,094: t15.2023.11.04 val PER: 0.0307
2025-07-29 22:59:22,094: t15.2023.11.17 val PER: 0.0358
2025-07-29 22:59:22,094: t15.2023.11.19 val PER: 0.0539
2025-07-29 22:59:22,094: t15.2023.11.26 val PER: 0.1275
2025-07-29 22:59:22,094: t15.2023.12.03 val PER: 0.1029
2025-07-29 22:59:22,094: t15.2023.12.08 val PER: 0.1032
2025-07-29 22:59:22,094: t15.2023.12.10 val PER: 0.1051
2025-07-29 22:59:22,094: t15.2023.12.17 val PER: 0.1383
2025-07-29 22:59:22,094: t15.2023.12.29 val PER: 0.1366
2025-07-29 22:59:22,095: t15.2024.02.25 val PER: 0.1222
2025-07-29 22:59:22,095: t15.2024.03.08 val PER: 0.2304
2025-07-29 22:59:22,095: t15.2024.03.15 val PER: 0.2326
2025-07-29 22:59:22,095: t15.2024.03.17 val PER: 0.1653
2025-07-29 22:59:22,095: t15.2024.05.10 val PER: 0.1605
2025-07-29 22:59:22,095: t15.2024.06.14 val PER: 0.1593
2025-07-29 22:59:22,095: t15.2024.07.19 val PER: 0.2301
2025-07-29 22:59:22,095: t15.2024.07.21 val PER: 0.0979
2025-07-29 22:59:22,096: t15.2024.07.28 val PER: 0.1478
2025-07-29 22:59:22,096: t15.2025.01.10 val PER: 0.3058
2025-07-29 22:59:22,096: t15.2025.01.12 val PER: 0.1640
2025-07-29 22:59:22,096: t15.2025.03.14 val PER: 0.3772
2025-07-29 22:59:22,096: t15.2025.03.16 val PER: 0.2186
2025-07-29 22:59:22,096: t15.2025.03.30 val PER: 0.2885
2025-07-29 22:59:22,096: t15.2025.04.13 val PER: 0.2311
2025-07-29 22:59:22,096: New best test PER 0.2270 --> 0.1606
2025-07-29 22:59:22,096: Checkpointing model
2025-07-29 22:59:23,235: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-07-29 22:59:52,920: Train batch 4200: loss: 5.30 grad norm: 21.96 time: 0.104
2025-07-29 23:00:22,830: Train batch 4400: loss: 7.06 grad norm: 31.52 time: 0.097
2025-07-29 23:00:52,219: Train batch 4600: loss: 3.87 grad norm: 23.67 time: 0.090
2025-07-29 23:01:22,771: Train batch 4800: loss: 7.33 grad norm: 29.40 time: 0.107
2025-07-29 23:01:52,836: Train batch 5000: loss: 5.71 grad norm: 28.31 time: 0.087
2025-07-29 23:02:22,886: Train batch 5200: loss: 7.82 grad norm: 31.13 time: 0.087
2025-07-29 23:02:52,761: Train batch 5400: loss: 1.98 grad norm: 16.76 time: 0.103
2025-07-29 23:03:22,285: Train batch 5600: loss: 5.10 grad norm: 24.09 time: 0.101
2025-07-29 23:03:52,423: Train batch 5800: loss: 3.16 grad norm: 21.26 time: 0.095
2025-07-29 23:04:22,844: Train batch 6000: loss: 3.97 grad norm: 25.20 time: 0.097
2025-07-29 23:04:22,844: Running test after training batch: 6000
2025-07-29 23:04:36,809: Val batch 6000: PER (avg): 0.1414 CTC Loss (avg): 17.4628 time: 13.965
2025-07-29 23:04:36,809: t15.2023.08.13 val PER: 0.1133
2025-07-29 23:04:36,809: t15.2023.08.18 val PER: 0.1098
2025-07-29 23:04:36,809: t15.2023.08.20 val PER: 0.0898
2025-07-29 23:04:36,809: t15.2023.08.25 val PER: 0.0889
2025-07-29 23:04:36,810: t15.2023.08.27 val PER: 0.1704
2025-07-29 23:04:36,810: t15.2023.09.01 val PER: 0.0666
2025-07-29 23:04:36,810: t15.2023.09.03 val PER: 0.1295
2025-07-29 23:04:36,810: t15.2023.09.24 val PER: 0.1007
2025-07-29 23:04:36,810: t15.2023.09.29 val PER: 0.1321
2025-07-29 23:04:36,810: t15.2023.10.01 val PER: 0.1625
2025-07-29 23:04:36,810: t15.2023.10.06 val PER: 0.1066
2025-07-29 23:04:36,810: t15.2023.10.08 val PER: 0.2449
2025-07-29 23:04:36,811: t15.2023.10.13 val PER: 0.1978
2025-07-29 23:04:36,811: t15.2023.10.15 val PER: 0.1483
2025-07-29 23:04:36,811: t15.2023.10.20 val PER: 0.1879
2025-07-29 23:04:36,811: t15.2023.10.22 val PER: 0.1236
2025-07-29 23:04:36,811: t15.2023.11.03 val PER: 0.1676
2025-07-29 23:04:36,811: t15.2023.11.04 val PER: 0.0205
2025-07-29 23:04:36,811: t15.2023.11.17 val PER: 0.0280
2025-07-29 23:04:36,811: t15.2023.11.19 val PER: 0.0359
2025-07-29 23:04:36,812: t15.2023.11.26 val PER: 0.1181
2025-07-29 23:04:36,812: t15.2023.12.03 val PER: 0.0809
2025-07-29 23:04:36,812: t15.2023.12.08 val PER: 0.0826
2025-07-29 23:04:36,812: t15.2023.12.10 val PER: 0.0736
2025-07-29 23:04:36,812: t15.2023.12.17 val PER: 0.1299
2025-07-29 23:04:36,812: t15.2023.12.29 val PER: 0.1057
2025-07-29 23:04:36,812: t15.2024.02.25 val PER: 0.0997
2025-07-29 23:04:36,812: t15.2024.03.08 val PER: 0.2233
2025-07-29 23:04:36,812: t15.2024.03.15 val PER: 0.2039
2025-07-29 23:04:36,813: t15.2024.03.17 val PER: 0.1325
2025-07-29 23:04:36,813: t15.2024.05.10 val PER: 0.1486
2025-07-29 23:04:36,813: t15.2024.06.14 val PER: 0.1562
2025-07-29 23:04:36,813: t15.2024.07.19 val PER: 0.2070
2025-07-29 23:04:36,813: t15.2024.07.21 val PER: 0.0814
2025-07-29 23:04:36,813: t15.2024.07.28 val PER: 0.1301
2025-07-29 23:04:36,813: t15.2025.01.10 val PER: 0.2617
2025-07-29 23:04:36,813: t15.2025.01.12 val PER: 0.1455
2025-07-29 23:04:36,814: t15.2025.03.14 val PER: 0.3595
2025-07-29 23:04:36,814: t15.2025.03.16 val PER: 0.1885
2025-07-29 23:04:36,814: t15.2025.03.30 val PER: 0.2517
2025-07-29 23:04:36,814: t15.2025.04.13 val PER: 0.2197
2025-07-29 23:04:36,814: New best test PER 0.1606 --> 0.1414
2025-07-29 23:04:36,814: Checkpointing model
2025-07-29 23:04:38,048: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-07-29 23:05:08,035: Train batch 6200: loss: 5.16 grad norm: 28.79 time: 0.122
2025-07-29 23:05:38,584: Train batch 6400: loss: 6.70 grad norm: 36.92 time: 0.107
2025-07-29 23:06:09,214: Train batch 6600: loss: 6.36 grad norm: 41.66 time: 0.100
2025-07-29 23:06:38,761: Train batch 6800: loss: 2.37 grad norm: 19.15 time: 0.072
2025-07-29 23:07:08,698: Train batch 7000: loss: 4.05 grad norm: 30.36 time: 0.094
2025-07-29 23:07:38,567: Train batch 7200: loss: 2.21 grad norm: 21.33 time: 0.113
2025-07-29 23:08:08,903: Train batch 7400: loss: 3.89 grad norm: 26.91 time: 0.130
2025-07-29 23:08:38,918: Train batch 7600: loss: 2.86 grad norm: 19.70 time: 0.099
2025-07-29 23:09:08,985: Train batch 7800: loss: 2.75 grad norm: 24.18 time: 0.107
2025-07-29 23:09:38,933: Train batch 8000: loss: 1.96 grad norm: 17.11 time: 0.074
2025-07-29 23:09:38,934: Running test after training batch: 8000
2025-07-29 23:09:52,564: Val batch 8000: PER (avg): 0.1315 CTC Loss (avg): 17.7065 time: 13.630
2025-07-29 23:09:52,564: t15.2023.08.13 val PER: 0.0946
2025-07-29 23:09:52,564: t15.2023.08.18 val PER: 0.0981
2025-07-29 23:09:52,564: t15.2023.08.20 val PER: 0.0699
2025-07-29 23:09:52,564: t15.2023.08.25 val PER: 0.0843
2025-07-29 23:09:52,565: t15.2023.08.27 val PER: 0.1897
2025-07-29 23:09:52,565: t15.2023.09.01 val PER: 0.0552
2025-07-29 23:09:52,565: t15.2023.09.03 val PER: 0.1306
2025-07-29 23:09:52,565: t15.2023.09.24 val PER: 0.1044
2025-07-29 23:09:52,565: t15.2023.09.29 val PER: 0.1187
2025-07-29 23:09:52,565: t15.2023.10.01 val PER: 0.1585
2025-07-29 23:09:52,565: t15.2023.10.06 val PER: 0.1012
2025-07-29 23:09:52,565: t15.2023.10.08 val PER: 0.2057
2025-07-29 23:09:52,566: t15.2023.10.13 val PER: 0.1839
2025-07-29 23:09:52,566: t15.2023.10.15 val PER: 0.1378
2025-07-29 23:09:52,566: t15.2023.10.20 val PER: 0.1913
2025-07-29 23:09:52,566: t15.2023.10.22 val PER: 0.1158
2025-07-29 23:09:52,566: t15.2023.11.03 val PER: 0.1445
2025-07-29 23:09:52,566: t15.2023.11.04 val PER: 0.0239
2025-07-29 23:09:52,566: t15.2023.11.17 val PER: 0.0280
2025-07-29 23:09:52,566: t15.2023.11.19 val PER: 0.0319
2025-07-29 23:09:52,567: t15.2023.11.26 val PER: 0.0957
2025-07-29 23:09:52,567: t15.2023.12.03 val PER: 0.0777
2025-07-29 23:09:52,567: t15.2023.12.08 val PER: 0.0692
2025-07-29 23:09:52,567: t15.2023.12.10 val PER: 0.0578
2025-07-29 23:09:52,567: t15.2023.12.17 val PER: 0.1154
2025-07-29 23:09:52,567: t15.2023.12.29 val PER: 0.1057
2025-07-29 23:09:52,567: t15.2024.02.25 val PER: 0.1053
2025-07-29 23:09:52,567: t15.2024.03.08 val PER: 0.2148
2025-07-29 23:09:52,568: t15.2024.03.15 val PER: 0.2020
2025-07-29 23:09:52,568: t15.2024.03.17 val PER: 0.1144
2025-07-29 23:09:52,568: t15.2024.05.10 val PER: 0.1367
2025-07-29 23:09:52,568: t15.2024.06.14 val PER: 0.1530
2025-07-29 23:09:52,568: t15.2024.07.19 val PER: 0.1945
2025-07-29 23:09:52,568: t15.2024.07.21 val PER: 0.0724
2025-07-29 23:09:52,569: t15.2024.07.28 val PER: 0.1125
2025-07-29 23:09:52,569: t15.2025.01.10 val PER: 0.2769
2025-07-29 23:09:52,569: t15.2025.01.12 val PER: 0.1278
2025-07-29 23:09:52,569: t15.2025.03.14 val PER: 0.3254
2025-07-29 23:09:52,569: t15.2025.03.16 val PER: 0.1702
2025-07-29 23:09:52,569: t15.2025.03.30 val PER: 0.2563
2025-07-29 23:09:52,569: t15.2025.04.13 val PER: 0.2311
2025-07-29 23:09:52,569: New best test PER 0.1414 --> 0.1315
2025-07-29 23:09:52,569: Checkpointing model
2025-07-29 23:09:53,717: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-07-29 23:10:23,723: Train batch 8200: loss: 3.27 grad norm: 31.68 time: 0.107
2025-07-29 23:10:53,770: Train batch 8400: loss: 5.32 grad norm: 32.94 time: 0.103
2025-07-29 23:11:24,422: Train batch 8600: loss: 2.52 grad norm: 19.55 time: 0.079
2025-07-29 23:11:54,402: Train batch 8800: loss: 2.40 grad norm: 20.82 time: 0.074
2025-07-29 23:12:24,998: Train batch 9000: loss: 2.76 grad norm: 22.81 time: 0.075
2025-07-29 23:12:55,478: Train batch 9200: loss: 1.50 grad norm: 15.15 time: 0.080
2025-07-29 23:13:25,641: Train batch 9400: loss: 2.01 grad norm: 25.11 time: 0.106
2025-07-29 23:13:56,370: Train batch 9600: loss: 2.04 grad norm: 23.05 time: 0.081
2025-07-29 23:14:26,343: Train batch 9800: loss: 1.77 grad norm: 18.22 time: 0.086
2025-07-29 23:14:56,551: Train batch 10000: loss: 4.04 grad norm: 25.41 time: 0.097
2025-07-29 23:14:56,551: Running test after training batch: 10000
2025-07-29 23:15:10,019: Val batch 10000: PER (avg): 0.1263 CTC Loss (avg): 18.8222 time: 13.468
2025-07-29 23:15:10,019: t15.2023.08.13 val PER: 0.0925
2025-07-29 23:15:10,020: t15.2023.08.18 val PER: 0.0872
2025-07-29 23:15:10,020: t15.2023.08.20 val PER: 0.0747
2025-07-29 23:15:10,020: t15.2023.08.25 val PER: 0.0798
2025-07-29 23:15:10,020: t15.2023.08.27 val PER: 0.1608
2025-07-29 23:15:10,020: t15.2023.09.01 val PER: 0.0536
2025-07-29 23:15:10,020: t15.2023.09.03 val PER: 0.1330
2025-07-29 23:15:10,021: t15.2023.09.24 val PER: 0.1080
2025-07-29 23:15:10,021: t15.2023.09.29 val PER: 0.1142
2025-07-29 23:15:10,021: t15.2023.10.01 val PER: 0.1440
2025-07-29 23:15:10,021: t15.2023.10.06 val PER: 0.0829
2025-07-29 23:15:10,021: t15.2023.10.08 val PER: 0.2097
2025-07-29 23:15:10,021: t15.2023.10.13 val PER: 0.1823
2025-07-29 23:15:10,022: t15.2023.10.15 val PER: 0.1206
2025-07-29 23:15:10,022: t15.2023.10.20 val PER: 0.1544
2025-07-29 23:15:10,022: t15.2023.10.22 val PER: 0.1091
2025-07-29 23:15:10,022: t15.2023.11.03 val PER: 0.1594
2025-07-29 23:15:10,022: t15.2023.11.04 val PER: 0.0239
2025-07-29 23:15:10,022: t15.2023.11.17 val PER: 0.0280
2025-07-29 23:15:10,022: t15.2023.11.19 val PER: 0.0319
2025-07-29 23:15:10,023: t15.2023.11.26 val PER: 0.0899
2025-07-29 23:15:10,023: t15.2023.12.03 val PER: 0.0704
2025-07-29 23:15:10,023: t15.2023.12.08 val PER: 0.0559
2025-07-29 23:15:10,023: t15.2023.12.10 val PER: 0.0499
2025-07-29 23:15:10,023: t15.2023.12.17 val PER: 0.1112
2025-07-29 23:15:10,023: t15.2023.12.29 val PER: 0.0899
2025-07-29 23:15:10,024: t15.2024.02.25 val PER: 0.0787
2025-07-29 23:15:10,024: t15.2024.03.08 val PER: 0.2290
2025-07-29 23:15:10,024: t15.2024.03.15 val PER: 0.2020
2025-07-29 23:15:10,024: t15.2024.03.17 val PER: 0.1018
2025-07-29 23:15:10,024: t15.2024.05.10 val PER: 0.1545
2025-07-29 23:15:10,024: t15.2024.06.14 val PER: 0.1199
2025-07-29 23:15:10,024: t15.2024.07.19 val PER: 0.1918
2025-07-29 23:15:10,025: t15.2024.07.21 val PER: 0.0731
2025-07-29 23:15:10,025: t15.2024.07.28 val PER: 0.1169
2025-07-29 23:15:10,025: t15.2025.01.10 val PER: 0.2645
2025-07-29 23:15:10,025: t15.2025.01.12 val PER: 0.1255
2025-07-29 23:15:10,025: t15.2025.03.14 val PER: 0.3402
2025-07-29 23:15:10,025: t15.2025.03.16 val PER: 0.1662
2025-07-29 23:15:10,025: t15.2025.03.30 val PER: 0.2575
2025-07-29 23:15:10,026: t15.2025.04.13 val PER: 0.2054
2025-07-29 23:15:10,026: New best test PER 0.1315 --> 0.1263
2025-07-29 23:15:10,026: Checkpointing model
2025-07-29 23:15:11,193: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-07-29 23:15:40,551: Train batch 10200: loss: 1.05 grad norm: 14.40 time: 0.100
2025-07-29 23:16:10,568: Train batch 10400: loss: 1.45 grad norm: 19.34 time: 0.100
2025-07-29 23:16:40,236: Train batch 10600: loss: 1.75 grad norm: 23.62 time: 0.086
2025-07-29 23:17:10,372: Train batch 10800: loss: 1.59 grad norm: 20.33 time: 0.093
2025-07-29 23:17:40,434: Train batch 11000: loss: 1.62 grad norm: 26.32 time: 0.111
2025-07-29 23:18:10,065: Train batch 11200: loss: 3.19 grad norm: 28.93 time: 0.084
2025-07-29 23:18:40,247: Train batch 11400: loss: 2.06 grad norm: 19.35 time: 0.070
2025-07-29 23:19:10,572: Train batch 11600: loss: 1.86 grad norm: 22.85 time: 0.072
2025-07-29 23:19:40,654: Train batch 11800: loss: 1.29 grad norm: 17.83 time: 0.107
2025-07-29 23:20:10,332: Train batch 12000: loss: 1.55 grad norm: 22.68 time: 0.107
2025-07-29 23:20:10,332: Running test after training batch: 12000
2025-07-29 23:20:24,429: Val batch 12000: PER (avg): 0.1229 CTC Loss (avg): 19.3497 time: 14.096
2025-07-29 23:20:24,429: t15.2023.08.13 val PER: 0.0800
2025-07-29 23:20:24,429: t15.2023.08.18 val PER: 0.0788
2025-07-29 23:20:24,429: t15.2023.08.20 val PER: 0.0635
2025-07-29 23:20:24,430: t15.2023.08.25 val PER: 0.0889
2025-07-29 23:20:24,430: t15.2023.08.27 val PER: 0.1672
2025-07-29 23:20:24,430: t15.2023.09.01 val PER: 0.0536
2025-07-29 23:20:24,430: t15.2023.09.03 val PER: 0.1283
2025-07-29 23:20:24,430: t15.2023.09.24 val PER: 0.1056
2025-07-29 23:20:24,430: t15.2023.09.29 val PER: 0.1174
2025-07-29 23:20:24,430: t15.2023.10.01 val PER: 0.1513
2025-07-29 23:20:24,430: t15.2023.10.06 val PER: 0.0926
2025-07-29 23:20:24,430: t15.2023.10.08 val PER: 0.1949
2025-07-29 23:20:24,431: t15.2023.10.13 val PER: 0.1769
2025-07-29 23:20:24,431: t15.2023.10.15 val PER: 0.1285
2025-07-29 23:20:24,431: t15.2023.10.20 val PER: 0.1544
2025-07-29 23:20:24,431: t15.2023.10.22 val PER: 0.1069
2025-07-29 23:20:24,431: t15.2023.11.03 val PER: 0.1465
2025-07-29 23:20:24,431: t15.2023.11.04 val PER: 0.0205
2025-07-29 23:20:24,431: t15.2023.11.17 val PER: 0.0202
2025-07-29 23:20:24,431: t15.2023.11.19 val PER: 0.0200
2025-07-29 23:20:24,431: t15.2023.11.26 val PER: 0.0862
2025-07-29 23:20:24,431: t15.2023.12.03 val PER: 0.0662
2025-07-29 23:20:24,432: t15.2023.12.08 val PER: 0.0646
2025-07-29 23:20:24,432: t15.2023.12.10 val PER: 0.0473
2025-07-29 23:20:24,432: t15.2023.12.17 val PER: 0.0852
2025-07-29 23:20:24,432: t15.2023.12.29 val PER: 0.0837
2025-07-29 23:20:24,432: t15.2024.02.25 val PER: 0.0815
2025-07-29 23:20:24,432: t15.2024.03.08 val PER: 0.2176
2025-07-29 23:20:24,432: t15.2024.03.15 val PER: 0.2008
2025-07-29 23:20:24,432: t15.2024.03.17 val PER: 0.1074
2025-07-29 23:20:24,432: t15.2024.05.10 val PER: 0.1322
2025-07-29 23:20:24,433: t15.2024.06.14 val PER: 0.1388
2025-07-29 23:20:24,433: t15.2024.07.19 val PER: 0.1786
2025-07-29 23:20:24,433: t15.2024.07.21 val PER: 0.0724
2025-07-29 23:20:24,433: t15.2024.07.28 val PER: 0.1051
2025-07-29 23:20:24,433: t15.2025.01.10 val PER: 0.2617
2025-07-29 23:20:24,433: t15.2025.01.12 val PER: 0.1132
2025-07-29 23:20:24,433: t15.2025.03.14 val PER: 0.3254
2025-07-29 23:20:24,434: t15.2025.03.16 val PER: 0.1728
2025-07-29 23:20:24,434: t15.2025.03.30 val PER: 0.2517
2025-07-29 23:20:24,434: t15.2025.04.13 val PER: 0.2154
2025-07-29 23:20:24,434: New best test PER 0.1263 --> 0.1229
2025-07-29 23:20:24,434: Checkpointing model
2025-07-29 23:20:25,585: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-07-29 23:20:55,371: Train batch 12200: loss: 1.70 grad norm: 21.37 time: 0.100
2025-07-29 23:21:25,575: Train batch 12400: loss: 0.95 grad norm: 13.81 time: 0.129
2025-07-29 23:21:55,673: Train batch 12600: loss: 2.17 grad norm: 29.07 time: 0.093
2025-07-29 23:22:25,599: Train batch 12800: loss: 3.41 grad norm: 19.18 time: 0.112
2025-07-29 23:22:56,123: Train batch 13000: loss: 1.78 grad norm: 20.38 time: 0.121
2025-07-29 23:23:25,751: Train batch 13200: loss: 0.61 grad norm: 9.20 time: 0.083
2025-07-29 23:23:55,722: Train batch 13400: loss: 0.66 grad norm: 13.40 time: 0.084
2025-07-29 23:24:25,817: Train batch 13600: loss: 1.63 grad norm: 24.63 time: 0.112
2025-07-29 23:24:55,928: Train batch 13800: loss: 0.76 grad norm: 13.82 time: 0.087
2025-07-29 23:25:26,526: Train batch 14000: loss: 0.83 grad norm: 13.27 time: 0.087
2025-07-29 23:25:26,527: Running test after training batch: 14000
2025-07-29 23:25:39,906: Val batch 14000: PER (avg): 0.1224 CTC Loss (avg): 20.1265 time: 13.380
2025-07-29 23:25:39,907: t15.2023.08.13 val PER: 0.0873
2025-07-29 23:25:39,907: t15.2023.08.18 val PER: 0.0771
2025-07-29 23:25:39,907: t15.2023.08.20 val PER: 0.0683
2025-07-29 23:25:39,907: t15.2023.08.25 val PER: 0.0858
2025-07-29 23:25:39,907: t15.2023.08.27 val PER: 0.1559
2025-07-29 23:25:39,907: t15.2023.09.01 val PER: 0.0487
2025-07-29 23:25:39,907: t15.2023.09.03 val PER: 0.1306
2025-07-29 23:25:39,907: t15.2023.09.24 val PER: 0.0971
2025-07-29 23:25:39,907: t15.2023.09.29 val PER: 0.1181
2025-07-29 23:25:39,908: t15.2023.10.01 val PER: 0.1565
2025-07-29 23:25:39,908: t15.2023.10.06 val PER: 0.0926
2025-07-29 23:25:39,908: t15.2023.10.08 val PER: 0.1840
2025-07-29 23:25:39,908: t15.2023.10.13 val PER: 0.1784
2025-07-29 23:25:39,908: t15.2023.10.15 val PER: 0.1384
2025-07-29 23:25:39,908: t15.2023.10.20 val PER: 0.1745
2025-07-29 23:25:39,908: t15.2023.10.22 val PER: 0.0980
2025-07-29 23:25:39,908: t15.2023.11.03 val PER: 0.1574
2025-07-29 23:25:39,908: t15.2023.11.04 val PER: 0.0171
2025-07-29 23:25:39,908: t15.2023.11.17 val PER: 0.0218
2025-07-29 23:25:39,909: t15.2023.11.19 val PER: 0.0299
2025-07-29 23:25:39,909: t15.2023.11.26 val PER: 0.0710
2025-07-29 23:25:39,909: t15.2023.12.03 val PER: 0.0683
2025-07-29 23:25:39,909: t15.2023.12.08 val PER: 0.0579
2025-07-29 23:25:39,909: t15.2023.12.10 val PER: 0.0434
2025-07-29 23:25:39,909: t15.2023.12.17 val PER: 0.0946
2025-07-29 23:25:39,909: t15.2023.12.29 val PER: 0.0817
2025-07-29 23:25:39,909: t15.2024.02.25 val PER: 0.0843
2025-07-29 23:25:39,909: t15.2024.03.08 val PER: 0.2148
2025-07-29 23:25:39,910: t15.2024.03.15 val PER: 0.1876
2025-07-29 23:25:39,910: t15.2024.03.17 val PER: 0.0955
2025-07-29 23:25:39,910: t15.2024.05.10 val PER: 0.1159
2025-07-29 23:25:39,910: t15.2024.06.14 val PER: 0.1230
2025-07-29 23:25:39,910: t15.2024.07.19 val PER: 0.1773
2025-07-29 23:25:39,910: t15.2024.07.21 val PER: 0.0731
2025-07-29 23:25:39,910: t15.2024.07.28 val PER: 0.1103
2025-07-29 23:25:39,910: t15.2025.01.10 val PER: 0.2479
2025-07-29 23:25:39,910: t15.2025.01.12 val PER: 0.1378
2025-07-29 23:25:39,911: t15.2025.03.14 val PER: 0.3491
2025-07-29 23:25:39,911: t15.2025.03.16 val PER: 0.1754
2025-07-29 23:25:39,911: t15.2025.03.30 val PER: 0.2437
2025-07-29 23:25:39,911: t15.2025.04.13 val PER: 0.2097
2025-07-29 23:25:39,911: New best test PER 0.1229 --> 0.1224
2025-07-29 23:25:39,911: Checkpointing model
2025-07-29 23:25:41,063: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-07-29 23:26:10,836: Train batch 14200: loss: 0.61 grad norm: 11.93 time: 0.081
2025-07-29 23:26:40,475: Train batch 14400: loss: 0.40 grad norm: 10.58 time: 0.085
2025-07-29 23:27:10,464: Train batch 14600: loss: 0.81 grad norm: 12.54 time: 0.093
2025-07-29 23:27:39,967: Train batch 14800: loss: 1.10 grad norm: 14.69 time: 0.091
2025-07-29 23:28:10,091: Train batch 15000: loss: 2.01 grad norm: 16.91 time: 0.124
2025-07-29 23:28:39,716: Train batch 15200: loss: 1.02 grad norm: 19.43 time: 0.093
2025-07-29 23:29:09,755: Train batch 15400: loss: 0.35 grad norm: 7.73 time: 0.108
2025-07-29 23:29:40,086: Train batch 15600: loss: 1.15 grad norm: 22.14 time: 0.086
2025-07-29 23:30:09,650: Train batch 15800: loss: 0.83 grad norm: 11.73 time: 0.071
2025-07-29 23:30:39,987: Train batch 16000: loss: 1.78 grad norm: 20.74 time: 0.089
2025-07-29 23:30:39,988: Running test after training batch: 16000
2025-07-29 23:30:53,175: Val batch 16000: PER (avg): 0.1228 CTC Loss (avg): 20.7641 time: 13.187
2025-07-29 23:30:53,175: t15.2023.08.13 val PER: 0.0748
2025-07-29 23:30:53,175: t15.2023.08.18 val PER: 0.0763
2025-07-29 23:30:53,175: t15.2023.08.20 val PER: 0.0564
2025-07-29 23:30:53,176: t15.2023.08.25 val PER: 0.0798
2025-07-29 23:30:53,176: t15.2023.08.27 val PER: 0.1559
2025-07-29 23:30:53,176: t15.2023.09.01 val PER: 0.0576
2025-07-29 23:30:53,176: t15.2023.09.03 val PER: 0.1306
2025-07-29 23:30:53,176: t15.2023.09.24 val PER: 0.1032
2025-07-29 23:30:53,176: t15.2023.09.29 val PER: 0.1213
2025-07-29 23:30:53,176: t15.2023.10.01 val PER: 0.1684
2025-07-29 23:30:53,176: t15.2023.10.06 val PER: 0.0797
2025-07-29 23:30:53,176: t15.2023.10.08 val PER: 0.1989
2025-07-29 23:30:53,176: t15.2023.10.13 val PER: 0.1668
2025-07-29 23:30:53,177: t15.2023.10.15 val PER: 0.1272
2025-07-29 23:30:53,177: t15.2023.10.20 val PER: 0.1946
2025-07-29 23:30:53,177: t15.2023.10.22 val PER: 0.1069
2025-07-29 23:30:53,177: t15.2023.11.03 val PER: 0.1520
2025-07-29 23:30:53,177: t15.2023.11.04 val PER: 0.0171
2025-07-29 23:30:53,177: t15.2023.11.17 val PER: 0.0311
2025-07-29 23:30:53,177: t15.2023.11.19 val PER: 0.0259
2025-07-29 23:30:53,177: t15.2023.11.26 val PER: 0.0710
2025-07-29 23:30:53,177: t15.2023.12.03 val PER: 0.0767
2025-07-29 23:30:53,177: t15.2023.12.08 val PER: 0.0619
2025-07-29 23:30:53,178: t15.2023.12.10 val PER: 0.0460
2025-07-29 23:30:53,178: t15.2023.12.17 val PER: 0.0873
2025-07-29 23:30:53,178: t15.2023.12.29 val PER: 0.0789
2025-07-29 23:30:53,178: t15.2024.02.25 val PER: 0.0899
2025-07-29 23:30:53,178: t15.2024.03.08 val PER: 0.1778
2025-07-29 23:30:53,178: t15.2024.03.15 val PER: 0.1951
2025-07-29 23:30:53,178: t15.2024.03.17 val PER: 0.1046
2025-07-29 23:30:53,178: t15.2024.05.10 val PER: 0.1308
2025-07-29 23:30:53,178: t15.2024.06.14 val PER: 0.1562
2025-07-29 23:30:53,179: t15.2024.07.19 val PER: 0.1635
2025-07-29 23:30:53,179: t15.2024.07.21 val PER: 0.0807
2025-07-29 23:30:53,179: t15.2024.07.28 val PER: 0.1103
2025-07-29 23:30:53,179: t15.2025.01.10 val PER: 0.2631
2025-07-29 23:30:53,179: t15.2025.01.12 val PER: 0.1170
2025-07-29 23:30:53,179: t15.2025.03.14 val PER: 0.3447
2025-07-29 23:30:53,180: t15.2025.03.16 val PER: 0.1649
2025-07-29 23:30:53,180: t15.2025.03.30 val PER: 0.2621
2025-07-29 23:30:53,180: t15.2025.04.13 val PER: 0.2325
2025-07-29 23:31:22,697: Train batch 16200: loss: 0.75 grad norm: 14.09 time: 0.081
2025-07-29 23:31:52,574: Train batch 16400: loss: 0.70 grad norm: 11.03 time: 0.086
2025-07-29 23:32:22,771: Train batch 16600: loss: 0.53 grad norm: 15.44 time: 0.085
2025-07-29 23:32:53,364: Train batch 16800: loss: 0.75 grad norm: 15.92 time: 0.114
2025-07-29 23:33:23,581: Train batch 17000: loss: 0.41 grad norm: 10.67 time: 0.092
2025-07-29 23:33:53,430: Train batch 17200: loss: 1.29 grad norm: 23.61 time: 0.083
2025-07-29 23:34:24,025: Train batch 17400: loss: 0.54 grad norm: 10.03 time: 0.115
2025-07-29 23:34:53,999: Train batch 17600: loss: 0.59 grad norm: 12.86 time: 0.082
2025-07-29 23:35:24,195: Train batch 17800: loss: 0.76 grad norm: 24.38 time: 0.128
2025-07-29 23:35:54,342: Train batch 18000: loss: 1.23 grad norm: 33.94 time: 0.115
2025-07-29 23:35:54,342: Running test after training batch: 18000
2025-07-29 23:36:07,703: Val batch 18000: PER (avg): 0.1218 CTC Loss (avg): 21.2126 time: 13.361
2025-07-29 23:36:07,704: t15.2023.08.13 val PER: 0.0884
2025-07-29 23:36:07,704: t15.2023.08.18 val PER: 0.0712
2025-07-29 23:36:07,704: t15.2023.08.20 val PER: 0.0667
2025-07-29 23:36:07,704: t15.2023.08.25 val PER: 0.0768
2025-07-29 23:36:07,704: t15.2023.08.27 val PER: 0.1431
2025-07-29 23:36:07,704: t15.2023.09.01 val PER: 0.0455
2025-07-29 23:36:07,704: t15.2023.09.03 val PER: 0.1413
2025-07-29 23:36:07,704: t15.2023.09.24 val PER: 0.0825
2025-07-29 23:36:07,705: t15.2023.09.29 val PER: 0.1187
2025-07-29 23:36:07,705: t15.2023.10.01 val PER: 0.1552
2025-07-29 23:36:07,705: t15.2023.10.06 val PER: 0.0721
2025-07-29 23:36:07,705: t15.2023.10.08 val PER: 0.2111
2025-07-29 23:36:07,705: t15.2023.10.13 val PER: 0.1730
2025-07-29 23:36:07,705: t15.2023.10.15 val PER: 0.1252
2025-07-29 23:36:07,705: t15.2023.10.20 val PER: 0.1745
2025-07-29 23:36:07,705: t15.2023.10.22 val PER: 0.1091
2025-07-29 23:36:07,706: t15.2023.11.03 val PER: 0.1588
2025-07-29 23:36:07,706: t15.2023.11.04 val PER: 0.0307
2025-07-29 23:36:07,706: t15.2023.11.17 val PER: 0.0171
2025-07-29 23:36:07,706: t15.2023.11.19 val PER: 0.0319
2025-07-29 23:36:07,706: t15.2023.11.26 val PER: 0.0783
2025-07-29 23:36:07,706: t15.2023.12.03 val PER: 0.0588
2025-07-29 23:36:07,706: t15.2023.12.08 val PER: 0.0566
2025-07-29 23:36:07,706: t15.2023.12.10 val PER: 0.0381
2025-07-29 23:36:07,707: t15.2023.12.17 val PER: 0.1154
2025-07-29 23:36:07,707: t15.2023.12.29 val PER: 0.0830
2025-07-29 23:36:07,707: t15.2024.02.25 val PER: 0.0941
2025-07-29 23:36:07,707: t15.2024.03.08 val PER: 0.2205
2025-07-29 23:36:07,707: t15.2024.03.15 val PER: 0.1864
2025-07-29 23:36:07,707: t15.2024.03.17 val PER: 0.0983
2025-07-29 23:36:07,707: t15.2024.05.10 val PER: 0.1486
2025-07-29 23:36:07,707: t15.2024.06.14 val PER: 0.1183
2025-07-29 23:36:07,708: t15.2024.07.19 val PER: 0.1740
2025-07-29 23:36:07,708: t15.2024.07.21 val PER: 0.0752
2025-07-29 23:36:07,708: t15.2024.07.28 val PER: 0.1015
2025-07-29 23:36:07,708: t15.2025.01.10 val PER: 0.2617
2025-07-29 23:36:07,708: t15.2025.01.12 val PER: 0.1147
2025-07-29 23:36:07,708: t15.2025.03.14 val PER: 0.3358
2025-07-29 23:36:07,708: t15.2025.03.16 val PER: 0.1832
2025-07-29 23:36:07,708: t15.2025.03.30 val PER: 0.2356
2025-07-29 23:36:07,708: t15.2025.04.13 val PER: 0.2254
2025-07-29 23:36:07,709: New best test PER 0.1224 --> 0.1218
2025-07-29 23:36:07,709: Checkpointing model
2025-07-29 23:36:08,927: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-07-29 23:36:38,714: Train batch 18200: loss: 2.08 grad norm: 15.25 time: 0.111
2025-07-29 23:37:09,121: Train batch 18400: loss: 0.61 grad norm: 32.41 time: 0.107
2025-07-29 23:38:05,668: Requested GPU 1 not available. Using GPU 0 instead.
2025-07-29 23:38:05,833: Using device: cuda:0
2025-07-29 23:38:06,469: Using torch.compile
2025-07-29 23:38:06,469: Initialized RNN decoding model
2025-07-29 23:38:06,469: GRUDecoder(
  (day_layer_activation): Softsign()
  (day_weights): ParameterList(
      (0): Parameter containing: [torch.float32 of size 512x512]
      (1): Parameter containing: [torch.float32 of size 512x512]
      (2): Parameter containing: [torch.float32 of size 512x512]
      (3): Parameter containing: [torch.float32 of size 512x512]
      (4): Parameter containing: [torch.float32 of size 512x512]
      (5): Parameter containing: [torch.float32 of size 512x512]
      (6): Parameter containing: [torch.float32 of size 512x512]
      (7): Parameter containing: [torch.float32 of size 512x512]
      (8): Parameter containing: [torch.float32 of size 512x512]
      (9): Parameter containing: [torch.float32 of size 512x512]
      (10): Parameter containing: [torch.float32 of size 512x512]
      (11): Parameter containing: [torch.float32 of size 512x512]
      (12): Parameter containing: [torch.float32 of size 512x512]
      (13): Parameter containing: [torch.float32 of size 512x512]
      (14): Parameter containing: [torch.float32 of size 512x512]
      (15): Parameter containing: [torch.float32 of size 512x512]
      (16): Parameter containing: [torch.float32 of size 512x512]
      (17): Parameter containing: [torch.float32 of size 512x512]
      (18): Parameter containing: [torch.float32 of size 512x512]
      (19): Parameter containing: [torch.float32 of size 512x512]
      (20): Parameter containing: [torch.float32 of size 512x512]
      (21): Parameter containing: [torch.float32 of size 512x512]
      (22): Parameter containing: [torch.float32 of size 512x512]
      (23): Parameter containing: [torch.float32 of size 512x512]
      (24): Parameter containing: [torch.float32 of size 512x512]
      (25): Parameter containing: [torch.float32 of size 512x512]
      (26): Parameter containing: [torch.float32 of size 512x512]
      (27): Parameter containing: [torch.float32 of size 512x512]
      (28): Parameter containing: [torch.float32 of size 512x512]
      (29): Parameter containing: [torch.float32 of size 512x512]
      (30): Parameter containing: [torch.float32 of size 512x512]
      (31): Parameter containing: [torch.float32 of size 512x512]
      (32): Parameter containing: [torch.float32 of size 512x512]
      (33): Parameter containing: [torch.float32 of size 512x512]
      (34): Parameter containing: [torch.float32 of size 512x512]
      (35): Parameter containing: [torch.float32 of size 512x512]
      (36): Parameter containing: [torch.float32 of size 512x512]
      (37): Parameter containing: [torch.float32 of size 512x512]
      (38): Parameter containing: [torch.float32 of size 512x512]
      (39): Parameter containing: [torch.float32 of size 512x512]
      (40): Parameter containing: [torch.float32 of size 512x512]
      (41): Parameter containing: [torch.float32 of size 512x512]
      (42): Parameter containing: [torch.float32 of size 512x512]
      (43): Parameter containing: [torch.float32 of size 512x512]
      (44): Parameter containing: [torch.float32 of size 512x512]
  )
  (day_biases): ParameterList(
      (0): Parameter containing: [torch.float32 of size 1x512]
      (1): Parameter containing: [torch.float32 of size 1x512]
      (2): Parameter containing: [torch.float32 of size 1x512]
      (3): Parameter containing: [torch.float32 of size 1x512]
      (4): Parameter containing: [torch.float32 of size 1x512]
      (5): Parameter containing: [torch.float32 of size 1x512]
      (6): Parameter containing: [torch.float32 of size 1x512]
      (7): Parameter containing: [torch.float32 of size 1x512]
      (8): Parameter containing: [torch.float32 of size 1x512]
      (9): Parameter containing: [torch.float32 of size 1x512]
      (10): Parameter containing: [torch.float32 of size 1x512]
      (11): Parameter containing: [torch.float32 of size 1x512]
      (12): Parameter containing: [torch.float32 of size 1x512]
      (13): Parameter containing: [torch.float32 of size 1x512]
      (14): Parameter containing: [torch.float32 of size 1x512]
      (15): Parameter containing: [torch.float32 of size 1x512]
      (16): Parameter containing: [torch.float32 of size 1x512]
      (17): Parameter containing: [torch.float32 of size 1x512]
      (18): Parameter containing: [torch.float32 of size 1x512]
      (19): Parameter containing: [torch.float32 of size 1x512]
      (20): Parameter containing: [torch.float32 of size 1x512]
      (21): Parameter containing: [torch.float32 of size 1x512]
      (22): Parameter containing: [torch.float32 of size 1x512]
      (23): Parameter containing: [torch.float32 of size 1x512]
      (24): Parameter containing: [torch.float32 of size 1x512]
      (25): Parameter containing: [torch.float32 of size 1x512]
      (26): Parameter containing: [torch.float32 of size 1x512]
      (27): Parameter containing: [torch.float32 of size 1x512]
      (28): Parameter containing: [torch.float32 of size 1x512]
      (29): Parameter containing: [torch.float32 of size 1x512]
      (30): Parameter containing: [torch.float32 of size 1x512]
      (31): Parameter containing: [torch.float32 of size 1x512]
      (32): Parameter containing: [torch.float32 of size 1x512]
      (33): Parameter containing: [torch.float32 of size 1x512]
      (34): Parameter containing: [torch.float32 of size 1x512]
      (35): Parameter containing: [torch.float32 of size 1x512]
      (36): Parameter containing: [torch.float32 of size 1x512]
      (37): Parameter containing: [torch.float32 of size 1x512]
      (38): Parameter containing: [torch.float32 of size 1x512]
      (39): Parameter containing: [torch.float32 of size 1x512]
      (40): Parameter containing: [torch.float32 of size 1x512]
      (41): Parameter containing: [torch.float32 of size 1x512]
      (42): Parameter containing: [torch.float32 of size 1x512]
      (43): Parameter containing: [torch.float32 of size 1x512]
      (44): Parameter containing: [torch.float32 of size 1x512]
  )
  (day_layer_dropout): Dropout(p=0.2, inplace=False)
  (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.4)
  (out): Linear(in_features=768, out_features=41, bias=True)
)
2025-07-29 23:38:06,471: Model has 44,315,177 parameters
2025-07-29 23:38:06,471: Model has 11,819,520 day-specific parameters | 26.67% of total parameters
2025-07-29 23:38:20,661: Successfully initialized datasets
2025-07-29 23:38:24,572: Train batch 0: loss: 755.13 grad norm: 293.68 time: 2.072
2025-07-29 23:38:24,572: Running test after training batch: 0
2025-07-29 23:38:38,070: Val batch 0: PER (avg): 1.2158 CTC Loss (avg): 714.9869 time: 13.498
2025-07-29 23:38:38,070: t15.2023.08.13 val PER: 1.1227
2025-07-29 23:38:38,071: t15.2023.08.18 val PER: 1.1450
2025-07-29 23:38:38,071: t15.2023.08.20 val PER: 1.1525
2025-07-29 23:38:38,071: t15.2023.08.25 val PER: 1.1642
2025-07-29 23:38:38,071: t15.2023.08.27 val PER: 1.0675
2025-07-29 23:38:38,071: t15.2023.09.01 val PER: 1.2248
2025-07-29 23:38:38,071: t15.2023.09.03 val PER: 1.1176
2025-07-29 23:38:38,071: t15.2023.09.24 val PER: 1.3313
2025-07-29 23:38:38,071: t15.2023.09.29 val PER: 1.2425
2025-07-29 23:38:38,072: t15.2023.10.01 val PER: 1.0555
2025-07-29 23:38:38,072: t15.2023.10.06 val PER: 1.2487
2025-07-29 23:38:38,072: t15.2023.10.08 val PER: 1.0433
2025-07-29 23:38:38,072: t15.2023.10.13 val PER: 1.1614
2025-07-29 23:38:38,072: t15.2023.10.15 val PER: 1.2037
2025-07-29 23:38:38,072: t15.2023.10.20 val PER: 1.3557
2025-07-29 23:38:38,072: t15.2023.10.22 val PER: 1.2906
2025-07-29 23:38:38,072: t15.2023.11.03 val PER: 1.2999
2025-07-29 23:38:38,073: t15.2023.11.04 val PER: 1.3788
2025-07-29 23:38:38,073: t15.2023.11.17 val PER: 1.6470
2025-07-29 23:38:38,073: t15.2023.11.19 val PER: 1.4371
2025-07-29 23:38:38,073: t15.2023.11.26 val PER: 1.2616
2025-07-29 23:38:38,073: t15.2023.12.03 val PER: 1.1838
2025-07-29 23:38:38,073: t15.2023.12.08 val PER: 1.2557
2025-07-29 23:38:38,073: t15.2023.12.10 val PER: 1.3219
2025-07-29 23:38:38,073: t15.2023.12.17 val PER: 1.0707
2025-07-29 23:38:38,074: t15.2023.12.29 val PER: 1.1627
2025-07-29 23:38:38,074: t15.2024.02.25 val PER: 1.1306
2025-07-29 23:38:38,074: t15.2024.03.08 val PER: 1.1593
2025-07-29 23:38:38,074: t15.2024.03.15 val PER: 1.1107
2025-07-29 23:38:38,074: t15.2024.03.17 val PER: 1.1729
2025-07-29 23:38:38,074: t15.2024.05.10 val PER: 1.2051
2025-07-29 23:38:38,074: t15.2024.06.14 val PER: 1.4101
2025-07-29 23:38:38,074: t15.2024.07.19 val PER: 0.9835
2025-07-29 23:38:38,075: t15.2024.07.21 val PER: 1.4214
2025-07-29 23:38:38,075: t15.2024.07.28 val PER: 1.4684
2025-07-29 23:38:38,075: t15.2025.01.10 val PER: 0.9532
2025-07-29 23:38:38,075: t15.2025.01.12 val PER: 1.4211
2025-07-29 23:38:38,075: t15.2025.03.14 val PER: 1.0207
2025-07-29 23:38:38,075: t15.2025.03.16 val PER: 1.4202
2025-07-29 23:38:38,075: t15.2025.03.30 val PER: 1.0805
2025-07-29 23:38:38,075: t15.2025.04.13 val PER: 1.3024
2025-07-29 23:38:38,075: New best test PER inf --> 1.2158
2025-07-29 23:38:38,076: Checkpointing model
2025-07-29 23:38:39,098: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-07-29 23:39:10,145: Train batch 200: loss: 93.92 grad norm: 22.30 time: 0.087
2025-07-29 23:39:40,408: Train batch 400: loss: 74.87 grad norm: 34.90 time: 0.104
2025-07-29 23:40:10,530: Train batch 600: loss: 59.28 grad norm: 47.82 time: 0.069
2025-07-29 23:40:41,277: Train batch 800: loss: 52.23 grad norm: 57.56 time: 0.102
2025-07-29 23:41:11,832: Train batch 1000: loss: 44.89 grad norm: 43.66 time: 0.087
2025-07-29 23:41:41,972: Train batch 1200: loss: 28.44 grad norm: 33.26 time: 0.107
2025-07-29 23:42:11,920: Train batch 1400: loss: 17.02 grad norm: 29.61 time: 0.085
2025-07-29 23:42:42,315: Train batch 1600: loss: 28.69 grad norm: 39.17 time: 0.092
2025-07-29 23:43:12,474: Train batch 1800: loss: 25.73 grad norm: 45.52 time: 0.071
2025-07-29 23:43:43,544: Train batch 2000: loss: 12.94 grad norm: 30.73 time: 0.087
2025-07-29 23:43:43,544: Running test after training batch: 2000
2025-07-29 23:43:57,033: Val batch 2000: PER (avg): 0.2278 CTC Loss (avg): 23.0198 time: 13.489
2025-07-29 23:43:57,033: t15.2023.08.13 val PER: 0.1965
2025-07-29 23:43:57,033: t15.2023.08.18 val PER: 0.1878
2025-07-29 23:43:57,033: t15.2023.08.20 val PER: 0.1724
2025-07-29 23:43:57,033: t15.2023.08.25 val PER: 0.1551
2025-07-29 23:43:57,033: t15.2023.08.27 val PER: 0.2749
2025-07-29 23:43:57,034: t15.2023.09.01 val PER: 0.1380
2025-07-29 23:43:57,034: t15.2023.09.03 val PER: 0.2292
2025-07-29 23:43:57,034: t15.2023.09.24 val PER: 0.1869
2025-07-29 23:43:57,034: t15.2023.09.29 val PER: 0.1985
2025-07-29 23:43:57,034: t15.2023.10.01 val PER: 0.2424
2025-07-29 23:43:57,034: t15.2023.10.06 val PER: 0.1679
2025-07-29 23:43:57,034: t15.2023.10.08 val PER: 0.3275
2025-07-29 23:43:57,034: t15.2023.10.13 val PER: 0.2940
2025-07-29 23:43:57,034: t15.2023.10.15 val PER: 0.2235
2025-07-29 23:43:57,035: t15.2023.10.20 val PER: 0.2483
2025-07-29 23:43:57,035: t15.2023.10.22 val PER: 0.1960
2025-07-29 23:43:57,035: t15.2023.11.03 val PER: 0.2422
2025-07-29 23:43:57,035: t15.2023.11.04 val PER: 0.0512
2025-07-29 23:43:57,035: t15.2023.11.17 val PER: 0.1198
2025-07-29 23:43:57,035: t15.2023.11.19 val PER: 0.0978
2025-07-29 23:43:57,035: t15.2023.11.26 val PER: 0.2558
2025-07-29 23:43:57,035: t15.2023.12.03 val PER: 0.2059
2025-07-29 23:43:57,035: t15.2023.12.08 val PER: 0.2037
2025-07-29 23:43:57,035: t15.2023.12.10 val PER: 0.1669
2025-07-29 23:43:57,036: t15.2023.12.17 val PER: 0.2516
2025-07-29 23:43:57,036: t15.2023.12.29 val PER: 0.2135
2025-07-29 23:43:57,036: t15.2024.02.25 val PER: 0.1742
2025-07-29 23:43:57,036: t15.2024.03.08 val PER: 0.2987
2025-07-29 23:43:57,036: t15.2024.03.15 val PER: 0.2896
2025-07-29 23:43:57,036: t15.2024.03.17 val PER: 0.2322
2025-07-29 23:43:57,036: t15.2024.05.10 val PER: 0.2303
2025-07-29 23:43:57,036: t15.2024.06.14 val PER: 0.2334
2025-07-29 23:43:57,036: t15.2024.07.19 val PER: 0.2953
2025-07-29 23:43:57,037: t15.2024.07.21 val PER: 0.1593
2025-07-29 23:43:57,037: t15.2024.07.28 val PER: 0.2088
2025-07-29 23:43:57,037: t15.2025.01.10 val PER: 0.3884
2025-07-29 23:43:57,037: t15.2025.01.12 val PER: 0.2256
2025-07-29 23:43:57,037: t15.2025.03.14 val PER: 0.3891
2025-07-29 23:43:57,037: t15.2025.03.16 val PER: 0.2552
2025-07-29 23:43:57,037: t15.2025.03.30 val PER: 0.3517
2025-07-29 23:43:57,037: t15.2025.04.13 val PER: 0.2825
2025-07-29 23:43:57,037: New best test PER 1.2158 --> 0.2278
2025-07-29 23:43:57,038: Checkpointing model
2025-07-29 23:43:58,199: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-07-29 23:44:27,749: Train batch 2200: loss: 18.19 grad norm: 37.16 time: 0.094
2025-07-29 23:44:57,941: Train batch 2400: loss: 15.88 grad norm: 37.08 time: 0.091
2025-07-29 23:45:27,868: Train batch 2600: loss: 9.69 grad norm: 24.58 time: 0.073
2025-07-29 23:45:58,484: Train batch 2800: loss: 10.39 grad norm: 28.64 time: 0.112
2025-07-29 23:46:28,893: Train batch 3000: loss: 13.36 grad norm: 37.29 time: 0.088
2025-07-29 23:46:58,457: Train batch 3200: loss: 9.97 grad norm: 28.42 time: 0.074
2025-07-29 23:47:28,275: Train batch 3400: loss: 7.09 grad norm: 25.79 time: 0.090
2025-07-29 23:47:58,804: Train batch 3600: loss: 7.24 grad norm: 27.80 time: 0.097
2025-07-29 23:48:28,969: Train batch 3800: loss: 7.61 grad norm: 29.35 time: 0.081
2025-07-29 23:48:59,188: Train batch 4000: loss: 4.63 grad norm: 21.84 time: 0.084
2025-07-29 23:48:59,188: Running test after training batch: 4000
2025-07-29 23:49:12,774: Val batch 4000: PER (avg): 0.1591 CTC Loss (avg): 17.4364 time: 13.586
2025-07-29 23:49:12,775: t15.2023.08.13 val PER: 0.1320
2025-07-29 23:49:12,775: t15.2023.08.18 val PER: 0.1232
2025-07-29 23:49:12,775: t15.2023.08.20 val PER: 0.0993
2025-07-29 23:49:12,775: t15.2023.08.25 val PER: 0.1099
2025-07-29 23:49:12,775: t15.2023.08.27 val PER: 0.1977
2025-07-29 23:49:12,775: t15.2023.09.01 val PER: 0.0771
2025-07-29 23:49:12,775: t15.2023.09.03 val PER: 0.1627
2025-07-29 23:49:12,776: t15.2023.09.24 val PER: 0.1189
2025-07-29 23:49:12,776: t15.2023.09.29 val PER: 0.1506
2025-07-29 23:49:12,776: t15.2023.10.01 val PER: 0.1783
2025-07-29 23:49:12,776: t15.2023.10.06 val PER: 0.1206
2025-07-29 23:49:12,776: t15.2023.10.08 val PER: 0.2422
2025-07-29 23:49:12,776: t15.2023.10.13 val PER: 0.2188
2025-07-29 23:49:12,776: t15.2023.10.15 val PER: 0.1668
2025-07-29 23:49:12,776: t15.2023.10.20 val PER: 0.2148
2025-07-29 23:49:12,776: t15.2023.10.22 val PER: 0.1347
2025-07-29 23:49:12,777: t15.2023.11.03 val PER: 0.1893
2025-07-29 23:49:12,777: t15.2023.11.04 val PER: 0.0341
2025-07-29 23:49:12,777: t15.2023.11.17 val PER: 0.0358
2025-07-29 23:49:12,777: t15.2023.11.19 val PER: 0.0519
2025-07-29 23:49:12,777: t15.2023.11.26 val PER: 0.1348
2025-07-29 23:49:12,777: t15.2023.12.03 val PER: 0.1155
2025-07-29 23:49:12,777: t15.2023.12.08 val PER: 0.1045
2025-07-29 23:49:12,777: t15.2023.12.10 val PER: 0.0946
2025-07-29 23:49:12,778: t15.2023.12.17 val PER: 0.1466
2025-07-29 23:49:12,778: t15.2023.12.29 val PER: 0.1325
2025-07-29 23:49:12,778: t15.2024.02.25 val PER: 0.1250
2025-07-29 23:49:12,778: t15.2024.03.08 val PER: 0.2333
2025-07-29 23:49:12,778: t15.2024.03.15 val PER: 0.2314
2025-07-29 23:49:12,778: t15.2024.03.17 val PER: 0.1639
2025-07-29 23:49:12,778: t15.2024.05.10 val PER: 0.1545
2025-07-29 23:49:12,778: t15.2024.06.14 val PER: 0.1609
2025-07-29 23:49:12,779: t15.2024.07.19 val PER: 0.2347
2025-07-29 23:49:12,779: t15.2024.07.21 val PER: 0.1007
2025-07-29 23:49:12,779: t15.2024.07.28 val PER: 0.1441
2025-07-29 23:49:12,779: t15.2025.01.10 val PER: 0.2865
2025-07-29 23:49:12,779: t15.2025.01.12 val PER: 0.1470
2025-07-29 23:49:12,779: t15.2025.03.14 val PER: 0.3565
2025-07-29 23:49:12,779: t15.2025.03.16 val PER: 0.2029
2025-07-29 23:49:12,779: t15.2025.03.30 val PER: 0.2667
2025-07-29 23:49:12,780: t15.2025.04.13 val PER: 0.2240
2025-07-29 23:49:12,780: New best test PER 0.2278 --> 0.1591
2025-07-29 23:49:12,780: Checkpointing model
2025-07-29 23:49:14,082: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-07-29 23:49:43,948: Train batch 4200: loss: 5.67 grad norm: 24.93 time: 0.104
2025-07-29 23:50:14,150: Train batch 4400: loss: 7.42 grad norm: 31.79 time: 0.096
2025-07-29 23:50:43,830: Train batch 4600: loss: 4.23 grad norm: 23.04 time: 0.090
2025-07-29 23:51:14,709: Train batch 4800: loss: 7.80 grad norm: 32.64 time: 0.108
2025-07-29 23:51:45,368: Train batch 5000: loss: 5.73 grad norm: 28.51 time: 0.086
2025-07-29 23:52:15,755: Train batch 5200: loss: 7.79 grad norm: 31.31 time: 0.087
2025-07-29 23:52:45,926: Train batch 5400: loss: 2.06 grad norm: 14.71 time: 0.104
2025-07-29 23:53:16,056: Train batch 5600: loss: 5.28 grad norm: 24.59 time: 0.101
2025-07-29 23:53:46,423: Train batch 5800: loss: 3.90 grad norm: 27.00 time: 0.095
2025-07-29 23:54:17,146: Train batch 6000: loss: 4.27 grad norm: 29.38 time: 0.098
2025-07-29 23:54:17,147: Running test after training batch: 6000
2025-07-29 23:54:30,582: Val batch 6000: PER (avg): 0.1393 CTC Loss (avg): 17.4084 time: 13.436
2025-07-29 23:54:30,583: t15.2023.08.13 val PER: 0.1091
2025-07-29 23:54:30,583: t15.2023.08.18 val PER: 0.1014
2025-07-29 23:54:30,583: t15.2023.08.20 val PER: 0.0842
2025-07-29 23:54:30,583: t15.2023.08.25 val PER: 0.0798
2025-07-29 23:54:30,583: t15.2023.08.27 val PER: 0.1752
2025-07-29 23:54:30,583: t15.2023.09.01 val PER: 0.0731
2025-07-29 23:54:30,583: t15.2023.09.03 val PER: 0.1485
2025-07-29 23:54:30,583: t15.2023.09.24 val PER: 0.0947
2025-07-29 23:54:30,584: t15.2023.09.29 val PER: 0.1321
2025-07-29 23:54:30,584: t15.2023.10.01 val PER: 0.1631
2025-07-29 23:54:30,584: t15.2023.10.06 val PER: 0.1012
2025-07-29 23:54:30,584: t15.2023.10.08 val PER: 0.2192
2025-07-29 23:54:30,584: t15.2023.10.13 val PER: 0.1862
2025-07-29 23:54:30,584: t15.2023.10.15 val PER: 0.1496
2025-07-29 23:54:30,584: t15.2023.10.20 val PER: 0.2114
2025-07-29 23:54:30,584: t15.2023.10.22 val PER: 0.1169
2025-07-29 23:54:30,585: t15.2023.11.03 val PER: 0.1649
2025-07-29 23:54:30,585: t15.2023.11.04 val PER: 0.0205
2025-07-29 23:54:30,585: t15.2023.11.17 val PER: 0.0342
2025-07-29 23:54:30,585: t15.2023.11.19 val PER: 0.0359
2025-07-29 23:54:30,585: t15.2023.11.26 val PER: 0.1116
2025-07-29 23:54:30,585: t15.2023.12.03 val PER: 0.0788
2025-07-29 23:54:30,585: t15.2023.12.08 val PER: 0.0792
2025-07-29 23:54:30,585: t15.2023.12.10 val PER: 0.0802
2025-07-29 23:54:30,585: t15.2023.12.17 val PER: 0.1372
2025-07-29 23:54:30,586: t15.2023.12.29 val PER: 0.1153
2025-07-29 23:54:30,586: t15.2024.02.25 val PER: 0.1011
2025-07-29 23:54:30,586: t15.2024.03.08 val PER: 0.2148
2025-07-29 23:54:30,586: t15.2024.03.15 val PER: 0.1976
2025-07-29 23:54:30,586: t15.2024.03.17 val PER: 0.1262
2025-07-29 23:54:30,586: t15.2024.05.10 val PER: 0.1545
2025-07-29 23:54:30,586: t15.2024.06.14 val PER: 0.1562
2025-07-29 23:54:30,586: t15.2024.07.19 val PER: 0.1833
2025-07-29 23:54:30,586: t15.2024.07.21 val PER: 0.0848
2025-07-29 23:54:30,586: t15.2024.07.28 val PER: 0.1235
2025-07-29 23:54:30,587: t15.2025.01.10 val PER: 0.2810
2025-07-29 23:54:30,587: t15.2025.01.12 val PER: 0.1401
2025-07-29 23:54:30,587: t15.2025.03.14 val PER: 0.3506
2025-07-29 23:54:30,587: t15.2025.03.16 val PER: 0.1963
2025-07-29 23:54:30,587: t15.2025.03.30 val PER: 0.2563
2025-07-29 23:54:30,587: t15.2025.04.13 val PER: 0.2111
2025-07-29 23:54:30,587: New best test PER 0.1591 --> 0.1393
2025-07-29 23:54:30,587: Checkpointing model
2025-07-29 23:54:31,788: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-07-29 23:55:02,059: Train batch 6200: loss: 5.10 grad norm: 29.10 time: 0.121
2025-07-29 23:55:32,615: Train batch 6400: loss: 7.65 grad norm: 45.80 time: 0.107
2025-07-29 23:56:03,214: Train batch 6600: loss: 6.94 grad norm: 42.75 time: 0.100
2025-07-29 23:56:33,150: Train batch 6800: loss: 2.41 grad norm: 16.44 time: 0.074
2025-07-29 23:57:03,165: Train batch 7000: loss: 4.27 grad norm: 21.53 time: 0.094
2025-07-29 23:57:33,220: Train batch 7200: loss: 2.34 grad norm: 20.93 time: 0.112
2025-07-29 23:58:03,798: Train batch 7400: loss: 3.88 grad norm: 24.95 time: 0.131
2025-07-29 23:58:34,114: Train batch 7600: loss: 2.56 grad norm: 19.31 time: 0.100
2025-07-29 23:59:04,478: Train batch 7800: loss: 2.78 grad norm: 20.83 time: 0.106
2025-07-29 23:59:34,641: Train batch 8000: loss: 2.19 grad norm: 16.69 time: 0.075
2025-07-29 23:59:34,641: Running test after training batch: 8000
2025-07-29 23:59:48,107: Val batch 8000: PER (avg): 0.1314 CTC Loss (avg): 17.7722 time: 13.466
2025-07-29 23:59:48,107: t15.2023.08.13 val PER: 0.0904
2025-07-29 23:59:48,108: t15.2023.08.18 val PER: 0.1039
2025-07-29 23:59:48,108: t15.2023.08.20 val PER: 0.0731
2025-07-29 23:59:48,108: t15.2023.08.25 val PER: 0.0904
2025-07-29 23:59:48,108: t15.2023.08.27 val PER: 0.1656
2025-07-29 23:59:48,108: t15.2023.09.01 val PER: 0.0552
2025-07-29 23:59:48,108: t15.2023.09.03 val PER: 0.1271
2025-07-29 23:59:48,108: t15.2023.09.24 val PER: 0.0922
2025-07-29 23:59:48,108: t15.2023.09.29 val PER: 0.1206
2025-07-29 23:59:48,109: t15.2023.10.01 val PER: 0.1572
2025-07-29 23:59:48,109: t15.2023.10.06 val PER: 0.0947
2025-07-29 23:59:48,109: t15.2023.10.08 val PER: 0.1922
2025-07-29 23:59:48,109: t15.2023.10.13 val PER: 0.1808
2025-07-29 23:59:48,109: t15.2023.10.15 val PER: 0.1397
2025-07-29 23:59:48,109: t15.2023.10.20 val PER: 0.1879
2025-07-29 23:59:48,109: t15.2023.10.22 val PER: 0.1192
2025-07-29 23:59:48,109: t15.2023.11.03 val PER: 0.1649
2025-07-29 23:59:48,109: t15.2023.11.04 val PER: 0.0273
2025-07-29 23:59:48,109: t15.2023.11.17 val PER: 0.0280
2025-07-29 23:59:48,110: t15.2023.11.19 val PER: 0.0379
2025-07-29 23:59:48,110: t15.2023.11.26 val PER: 0.0928
2025-07-29 23:59:48,110: t15.2023.12.03 val PER: 0.0861
2025-07-29 23:59:48,110: t15.2023.12.08 val PER: 0.0699
2025-07-29 23:59:48,110: t15.2023.12.10 val PER: 0.0591
2025-07-29 23:59:48,110: t15.2023.12.17 val PER: 0.1216
2025-07-29 23:59:48,110: t15.2023.12.29 val PER: 0.0927
2025-07-29 23:59:48,111: t15.2024.02.25 val PER: 0.0983
2025-07-29 23:59:48,111: t15.2024.03.08 val PER: 0.2020
2025-07-29 23:59:48,111: t15.2024.03.15 val PER: 0.1989
2025-07-29 23:59:48,111: t15.2024.03.17 val PER: 0.1123
2025-07-29 23:59:48,111: t15.2024.05.10 val PER: 0.1605
2025-07-29 23:59:48,111: t15.2024.06.14 val PER: 0.1546
2025-07-29 23:59:48,111: t15.2024.07.19 val PER: 0.1997
2025-07-29 23:59:48,111: t15.2024.07.21 val PER: 0.0779
2025-07-29 23:59:48,112: t15.2024.07.28 val PER: 0.1110
2025-07-29 23:59:48,112: t15.2025.01.10 val PER: 0.2755
2025-07-29 23:59:48,112: t15.2025.01.12 val PER: 0.1278
2025-07-29 23:59:48,112: t15.2025.03.14 val PER: 0.3506
2025-07-29 23:59:48,112: t15.2025.03.16 val PER: 0.1741
2025-07-29 23:59:48,112: t15.2025.03.30 val PER: 0.2345
2025-07-29 23:59:48,112: t15.2025.04.13 val PER: 0.2197
2025-07-29 23:59:48,112: New best test PER 0.1393 --> 0.1314
2025-07-29 23:59:48,112: Checkpointing model
2025-07-29 23:59:49,306: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-07-30 00:00:19,438: Train batch 8200: loss: 3.12 grad norm: 26.17 time: 0.108
2025-07-30 00:00:49,177: Train batch 8400: loss: 4.73 grad norm: 37.39 time: 0.104
2025-07-30 00:01:19,797: Train batch 8600: loss: 2.56 grad norm: 21.61 time: 0.080
2025-07-30 00:01:49,626: Train batch 8800: loss: 2.74 grad norm: 21.82 time: 0.074
2025-07-30 00:02:20,169: Train batch 9000: loss: 2.30 grad norm: 22.55 time: 0.075
2025-07-30 00:02:50,592: Train batch 9200: loss: 2.11 grad norm: 45.44 time: 0.081
2025-07-30 00:03:20,745: Train batch 9400: loss: 2.12 grad norm: 21.72 time: 0.105
2025-07-30 00:03:51,506: Train batch 9600: loss: 1.59 grad norm: 23.23 time: 0.081
2025-07-30 00:04:21,457: Train batch 9800: loss: 1.87 grad norm: 20.32 time: 0.086
2025-07-30 00:04:51,543: Train batch 10000: loss: 3.19 grad norm: 21.23 time: 0.098
2025-07-30 00:04:51,543: Running test after training batch: 10000
2025-07-30 00:05:04,805: Val batch 10000: PER (avg): 0.1315 CTC Loss (avg): 19.0998 time: 13.262
2025-07-30 00:05:04,805: t15.2023.08.13 val PER: 0.0936
2025-07-30 00:05:04,805: t15.2023.08.18 val PER: 0.0863
2025-07-30 00:05:04,805: t15.2023.08.20 val PER: 0.0723
2025-07-30 00:05:04,805: t15.2023.08.25 val PER: 0.0949
2025-07-30 00:05:04,805: t15.2023.08.27 val PER: 0.1849
2025-07-30 00:05:04,805: t15.2023.09.01 val PER: 0.0722
2025-07-30 00:05:04,805: t15.2023.09.03 val PER: 0.1473
2025-07-30 00:05:04,806: t15.2023.09.24 val PER: 0.0934
2025-07-30 00:05:04,806: t15.2023.09.29 val PER: 0.1206
2025-07-30 00:05:04,806: t15.2023.10.01 val PER: 0.1605
2025-07-30 00:05:04,806: t15.2023.10.06 val PER: 0.0926
2025-07-30 00:05:04,806: t15.2023.10.08 val PER: 0.2084
2025-07-30 00:05:04,806: t15.2023.10.13 val PER: 0.1870
2025-07-30 00:05:04,806: t15.2023.10.15 val PER: 0.1358
2025-07-30 00:05:04,806: t15.2023.10.20 val PER: 0.1946
2025-07-30 00:05:04,806: t15.2023.10.22 val PER: 0.1281
2025-07-30 00:05:04,807: t15.2023.11.03 val PER: 0.1689
2025-07-30 00:05:04,807: t15.2023.11.04 val PER: 0.0171
2025-07-30 00:05:04,807: t15.2023.11.17 val PER: 0.0342
2025-07-30 00:05:04,807: t15.2023.11.19 val PER: 0.0319
2025-07-30 00:05:04,807: t15.2023.11.26 val PER: 0.0884
2025-07-30 00:05:04,807: t15.2023.12.03 val PER: 0.0777
2025-07-30 00:05:04,807: t15.2023.12.08 val PER: 0.0619
2025-07-30 00:05:04,807: t15.2023.12.10 val PER: 0.0512
2025-07-30 00:05:04,808: t15.2023.12.17 val PER: 0.1195
2025-07-30 00:05:04,808: t15.2023.12.29 val PER: 0.0940
2025-07-30 00:05:04,808: t15.2024.02.25 val PER: 0.0815
2025-07-30 00:05:04,808: t15.2024.03.08 val PER: 0.2063
2025-07-30 00:05:04,808: t15.2024.03.15 val PER: 0.2114
2025-07-30 00:05:04,808: t15.2024.03.17 val PER: 0.1109
2025-07-30 00:05:04,808: t15.2024.05.10 val PER: 0.1456
2025-07-30 00:05:04,808: t15.2024.06.14 val PER: 0.1420
2025-07-30 00:05:04,808: t15.2024.07.19 val PER: 0.1866
2025-07-30 00:05:04,809: t15.2024.07.21 val PER: 0.0731
2025-07-30 00:05:04,809: t15.2024.07.28 val PER: 0.1184
2025-07-30 00:05:04,809: t15.2025.01.10 val PER: 0.2493
2025-07-30 00:05:04,809: t15.2025.01.12 val PER: 0.1216
2025-07-30 00:05:04,809: t15.2025.03.14 val PER: 0.3462
2025-07-30 00:05:04,809: t15.2025.03.16 val PER: 0.1950
2025-07-30 00:05:04,809: t15.2025.03.30 val PER: 0.2529
2025-07-30 00:05:04,809: t15.2025.04.13 val PER: 0.2111
2025-07-30 00:05:34,197: Train batch 10200: loss: 1.20 grad norm: 22.53 time: 0.100
2025-07-30 00:06:04,730: Train batch 10400: loss: 1.33 grad norm: 17.51 time: 0.101
2025-07-30 00:06:34,742: Train batch 10600: loss: 2.43 grad norm: 36.49 time: 0.085
2025-07-30 00:07:05,012: Train batch 10800: loss: 1.81 grad norm: 22.03 time: 0.094
2025-07-30 00:07:35,396: Train batch 11000: loss: 1.28 grad norm: 18.35 time: 0.113
2025-07-30 00:08:05,474: Train batch 11200: loss: 2.99 grad norm: 37.49 time: 0.085
2025-07-30 00:08:35,862: Train batch 11400: loss: 2.27 grad norm: 26.03 time: 0.071
2025-07-30 00:09:06,641: Train batch 11600: loss: 1.89 grad norm: 18.01 time: 0.073
2025-07-30 00:09:36,811: Train batch 11800: loss: 1.10 grad norm: 14.57 time: 0.108
2025-07-30 00:10:06,513: Train batch 12000: loss: 1.46 grad norm: 20.27 time: 0.108
2025-07-30 00:10:06,513: Running test after training batch: 12000
2025-07-30 00:10:20,576: Val batch 12000: PER (avg): 0.1258 CTC Loss (avg): 19.7908 time: 14.063
2025-07-30 00:10:20,577: t15.2023.08.13 val PER: 0.0904
2025-07-30 00:10:20,577: t15.2023.08.18 val PER: 0.0838
2025-07-30 00:10:20,577: t15.2023.08.20 val PER: 0.0627
2025-07-30 00:10:20,577: t15.2023.08.25 val PER: 0.0798
2025-07-30 00:10:20,577: t15.2023.08.27 val PER: 0.1801
2025-07-30 00:10:20,577: t15.2023.09.01 val PER: 0.0560
2025-07-30 00:10:20,577: t15.2023.09.03 val PER: 0.1390
2025-07-30 00:10:20,578: t15.2023.09.24 val PER: 0.0740
2025-07-30 00:10:20,578: t15.2023.09.29 val PER: 0.1104
2025-07-30 00:10:20,578: t15.2023.10.01 val PER: 0.1572
2025-07-30 00:10:20,578: t15.2023.10.06 val PER: 0.0904
2025-07-30 00:10:20,578: t15.2023.10.08 val PER: 0.2030
2025-07-30 00:10:20,578: t15.2023.10.13 val PER: 0.1761
2025-07-30 00:10:20,578: t15.2023.10.15 val PER: 0.1345
2025-07-30 00:10:20,578: t15.2023.10.20 val PER: 0.1879
2025-07-30 00:10:20,578: t15.2023.10.22 val PER: 0.1147
2025-07-30 00:10:20,578: t15.2023.11.03 val PER: 0.1520
2025-07-30 00:10:20,579: t15.2023.11.04 val PER: 0.0239
2025-07-30 00:10:20,579: t15.2023.11.17 val PER: 0.0218
2025-07-30 00:10:20,579: t15.2023.11.19 val PER: 0.0279
2025-07-30 00:10:20,579: t15.2023.11.26 val PER: 0.0797
2025-07-30 00:10:20,579: t15.2023.12.03 val PER: 0.0840
2025-07-30 00:10:20,579: t15.2023.12.08 val PER: 0.0686
2025-07-30 00:10:20,579: t15.2023.12.10 val PER: 0.0499
2025-07-30 00:10:20,579: t15.2023.12.17 val PER: 0.1081
2025-07-30 00:10:20,580: t15.2023.12.29 val PER: 0.0988
2025-07-30 00:10:20,580: t15.2024.02.25 val PER: 0.0857
2025-07-30 00:10:20,580: t15.2024.03.08 val PER: 0.1977
2025-07-30 00:10:20,580: t15.2024.03.15 val PER: 0.2045
2025-07-30 00:10:20,580: t15.2024.03.17 val PER: 0.1039
2025-07-30 00:10:20,580: t15.2024.05.10 val PER: 0.1441
2025-07-30 00:10:20,580: t15.2024.06.14 val PER: 0.1356
2025-07-30 00:10:20,580: t15.2024.07.19 val PER: 0.1833
2025-07-30 00:10:20,581: t15.2024.07.21 val PER: 0.0848
2025-07-30 00:10:20,581: t15.2024.07.28 val PER: 0.1088
2025-07-30 00:10:20,581: t15.2025.01.10 val PER: 0.2479
2025-07-30 00:10:20,581: t15.2025.01.12 val PER: 0.1170
2025-07-30 00:10:20,581: t15.2025.03.14 val PER: 0.3195
2025-07-30 00:10:20,581: t15.2025.03.16 val PER: 0.1911
2025-07-30 00:10:20,581: t15.2025.03.30 val PER: 0.2414
2025-07-30 00:10:20,581: t15.2025.04.13 val PER: 0.2054
2025-07-30 00:10:20,581: New best test PER 0.1314 --> 0.1258
2025-07-30 00:10:20,582: Checkpointing model
2025-07-30 00:10:21,797: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-07-30 00:10:51,598: Train batch 12200: loss: 1.55 grad norm: 21.47 time: 0.101
2025-07-30 00:11:22,112: Train batch 12400: loss: 0.99 grad norm: 15.45 time: 0.129
2025-07-30 00:11:52,555: Train batch 12600: loss: 2.00 grad norm: 18.80 time: 0.094
2025-07-30 00:12:22,703: Train batch 12800: loss: 3.81 grad norm: 20.21 time: 0.112
2025-07-30 00:12:53,490: Train batch 13000: loss: 1.49 grad norm: 17.22 time: 0.122
2025-07-30 00:13:23,328: Train batch 13200: loss: 0.89 grad norm: 16.90 time: 0.085
2025-07-30 00:13:53,654: Train batch 13400: loss: 0.70 grad norm: 13.63 time: 0.084
2025-07-30 00:14:23,962: Train batch 13600: loss: 1.91 grad norm: 40.20 time: 0.112
2025-07-30 00:14:54,551: Train batch 13800: loss: 0.93 grad norm: 16.95 time: 0.088
2025-07-30 00:15:25,368: Train batch 14000: loss: 0.93 grad norm: 15.69 time: 0.088
2025-07-30 00:15:25,368: Running test after training batch: 14000
2025-07-30 00:15:38,522: Val batch 14000: PER (avg): 0.1266 CTC Loss (avg): 20.9265 time: 13.154
2025-07-30 00:15:38,522: t15.2023.08.13 val PER: 0.0863
2025-07-30 00:15:38,522: t15.2023.08.18 val PER: 0.0889
2025-07-30 00:15:38,522: t15.2023.08.20 val PER: 0.0588
2025-07-30 00:15:38,523: t15.2023.08.25 val PER: 0.1009
2025-07-30 00:15:38,523: t15.2023.08.27 val PER: 0.1704
2025-07-30 00:15:38,523: t15.2023.09.01 val PER: 0.0568
2025-07-30 00:15:38,523: t15.2023.09.03 val PER: 0.1342
2025-07-30 00:15:38,523: t15.2023.09.24 val PER: 0.0947
2025-07-30 00:15:38,523: t15.2023.09.29 val PER: 0.1142
2025-07-30 00:15:38,523: t15.2023.10.01 val PER: 0.1559
2025-07-30 00:15:38,523: t15.2023.10.06 val PER: 0.0893
2025-07-30 00:15:38,524: t15.2023.10.08 val PER: 0.2070
2025-07-30 00:15:38,524: t15.2023.10.13 val PER: 0.1722
2025-07-30 00:15:38,524: t15.2023.10.15 val PER: 0.1285
2025-07-30 00:15:38,524: t15.2023.10.20 val PER: 0.1711
2025-07-30 00:15:38,524: t15.2023.10.22 val PER: 0.1192
2025-07-30 00:15:38,524: t15.2023.11.03 val PER: 0.1594
2025-07-30 00:15:38,524: t15.2023.11.04 val PER: 0.0137
2025-07-30 00:15:38,524: t15.2023.11.17 val PER: 0.0156
2025-07-30 00:15:38,525: t15.2023.11.19 val PER: 0.0279
2025-07-30 00:15:38,525: t15.2023.11.26 val PER: 0.0732
2025-07-30 00:15:38,525: t15.2023.12.03 val PER: 0.0851
2025-07-30 00:15:38,525: t15.2023.12.08 val PER: 0.0672
2025-07-30 00:15:38,525: t15.2023.12.10 val PER: 0.0447
2025-07-30 00:15:38,525: t15.2023.12.17 val PER: 0.1227
2025-07-30 00:15:38,525: t15.2023.12.29 val PER: 0.0927
2025-07-30 00:15:38,525: t15.2024.02.25 val PER: 0.0815
2025-07-30 00:15:38,526: t15.2024.03.08 val PER: 0.1863
2025-07-30 00:15:38,526: t15.2024.03.15 val PER: 0.1939
2025-07-30 00:15:38,526: t15.2024.03.17 val PER: 0.1004
2025-07-30 00:15:38,526: t15.2024.05.10 val PER: 0.1530
2025-07-30 00:15:38,526: t15.2024.06.14 val PER: 0.1309
2025-07-30 00:15:38,526: t15.2024.07.19 val PER: 0.1885
2025-07-30 00:15:38,527: t15.2024.07.21 val PER: 0.0717
2025-07-30 00:15:38,527: t15.2024.07.28 val PER: 0.1147
2025-07-30 00:15:38,527: t15.2025.01.10 val PER: 0.2645
2025-07-30 00:15:38,527: t15.2025.01.12 val PER: 0.1324
2025-07-30 00:15:38,527: t15.2025.03.14 val PER: 0.3536
2025-07-30 00:15:38,527: t15.2025.03.16 val PER: 0.1859
2025-07-30 00:15:38,527: t15.2025.03.30 val PER: 0.2540
2025-07-30 00:15:38,527: t15.2025.04.13 val PER: 0.2026
2025-07-30 00:16:08,746: Train batch 14200: loss: 0.77 grad norm: 23.25 time: 0.082
2025-07-30 00:16:38,629: Train batch 14400: loss: 0.36 grad norm: 10.27 time: 0.082
2025-07-30 00:17:08,815: Train batch 14600: loss: 0.98 grad norm: 14.59 time: 0.093
2025-07-30 00:17:38,859: Train batch 14800: loss: 2.18 grad norm: 18.27 time: 0.099
2025-07-30 00:18:09,375: Train batch 15000: loss: 1.02 grad norm: 14.23 time: 0.126
2025-07-30 00:18:39,269: Train batch 15200: loss: 0.93 grad norm: 21.72 time: 0.094
2025-07-30 00:19:09,655: Train batch 15400: loss: 0.37 grad norm: 10.73 time: 0.109
2025-07-30 00:19:40,332: Train batch 15600: loss: 1.45 grad norm: 22.19 time: 0.087
2025-07-30 00:20:10,201: Train batch 15800: loss: 0.84 grad norm: 21.93 time: 0.068
2025-07-30 00:20:40,576: Train batch 16000: loss: 1.68 grad norm: 19.10 time: 0.091
2025-07-30 00:20:40,576: Running test after training batch: 16000
2025-07-30 00:20:54,071: Val batch 16000: PER (avg): 0.1219 CTC Loss (avg): 20.6368 time: 13.494
2025-07-30 00:20:54,071: t15.2023.08.13 val PER: 0.0852
2025-07-30 00:20:54,071: t15.2023.08.18 val PER: 0.0805
2025-07-30 00:20:54,071: t15.2023.08.20 val PER: 0.0635
2025-07-30 00:20:54,071: t15.2023.08.25 val PER: 0.0813
2025-07-30 00:20:54,071: t15.2023.08.27 val PER: 0.1479
2025-07-30 00:20:54,072: t15.2023.09.01 val PER: 0.0568
2025-07-30 00:20:54,072: t15.2023.09.03 val PER: 0.1259
2025-07-30 00:20:54,072: t15.2023.09.24 val PER: 0.0862
2025-07-30 00:20:54,072: t15.2023.09.29 val PER: 0.1161
2025-07-30 00:20:54,072: t15.2023.10.01 val PER: 0.1506
2025-07-30 00:20:54,072: t15.2023.10.06 val PER: 0.0775
2025-07-30 00:20:54,072: t15.2023.10.08 val PER: 0.2003
2025-07-30 00:20:54,072: t15.2023.10.13 val PER: 0.1777
2025-07-30 00:20:54,073: t15.2023.10.15 val PER: 0.1292
2025-07-30 00:20:54,073: t15.2023.10.20 val PER: 0.1779
2025-07-30 00:20:54,073: t15.2023.10.22 val PER: 0.1091
2025-07-30 00:20:54,073: t15.2023.11.03 val PER: 0.1581
2025-07-30 00:20:54,073: t15.2023.11.04 val PER: 0.0205
2025-07-30 00:20:54,073: t15.2023.11.17 val PER: 0.0311
2025-07-30 00:20:54,073: t15.2023.11.19 val PER: 0.0220
2025-07-30 00:20:54,073: t15.2023.11.26 val PER: 0.0688
2025-07-30 00:20:54,073: t15.2023.12.03 val PER: 0.0756
2025-07-30 00:20:54,074: t15.2023.12.08 val PER: 0.0652
2025-07-30 00:20:54,074: t15.2023.12.10 val PER: 0.0512
2025-07-30 00:20:54,074: t15.2023.12.17 val PER: 0.1029
2025-07-30 00:20:54,074: t15.2023.12.29 val PER: 0.0872
2025-07-30 00:20:54,074: t15.2024.02.25 val PER: 0.0758
2025-07-30 00:20:54,074: t15.2024.03.08 val PER: 0.1920
2025-07-30 00:20:54,074: t15.2024.03.15 val PER: 0.1732
2025-07-30 00:20:54,074: t15.2024.03.17 val PER: 0.1018
2025-07-30 00:20:54,075: t15.2024.05.10 val PER: 0.1441
2025-07-30 00:20:54,075: t15.2024.06.14 val PER: 0.1293
2025-07-30 00:20:54,075: t15.2024.07.19 val PER: 0.1740
2025-07-30 00:20:54,075: t15.2024.07.21 val PER: 0.0745
2025-07-30 00:20:54,075: t15.2024.07.28 val PER: 0.1022
2025-07-30 00:20:54,075: t15.2025.01.10 val PER: 0.2466
2025-07-30 00:20:54,075: t15.2025.01.12 val PER: 0.1247
2025-07-30 00:20:54,075: t15.2025.03.14 val PER: 0.3536
2025-07-30 00:20:54,076: t15.2025.03.16 val PER: 0.1885
2025-07-30 00:20:54,076: t15.2025.03.30 val PER: 0.2448
2025-07-30 00:20:54,076: t15.2025.04.13 val PER: 0.2154
2025-07-30 00:20:54,076: New best test PER 0.1258 --> 0.1219
2025-07-30 00:20:54,076: Checkpointing model
2025-07-30 00:20:55,294: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-07-30 00:21:25,049: Train batch 16200: loss: 0.70 grad norm: 11.74 time: 0.082
2025-07-30 00:21:55,208: Train batch 16400: loss: 0.78 grad norm: 12.46 time: 0.087
2025-07-30 00:22:25,702: Train batch 16600: loss: 0.46 grad norm: 10.26 time: 0.087
2025-07-30 00:22:56,562: Train batch 16800: loss: 0.97 grad norm: 21.88 time: 0.115
2025-07-30 00:23:27,135: Train batch 17000: loss: 0.41 grad norm: 8.67 time: 0.094
2025-07-30 00:23:57,137: Train batch 17200: loss: 1.23 grad norm: 19.06 time: 0.083
2025-07-30 00:24:28,173: Train batch 17400: loss: 0.72 grad norm: 14.72 time: 0.119
2025-07-30 00:24:58,480: Train batch 17600: loss: 0.80 grad norm: 13.42 time: 0.081
2025-07-30 00:25:29,046: Train batch 17800: loss: 1.87 grad norm: 24.68 time: 0.132
2025-07-30 00:25:59,333: Train batch 18000: loss: 1.01 grad norm: 17.39 time: 0.116
2025-07-30 00:25:59,334: Running test after training batch: 18000
2025-07-30 00:26:12,470: Val batch 18000: PER (avg): 0.1222 CTC Loss (avg): 21.5780 time: 13.136
2025-07-30 00:26:12,470: t15.2023.08.13 val PER: 0.0748
2025-07-30 00:26:12,470: t15.2023.08.18 val PER: 0.0738
2025-07-30 00:26:12,470: t15.2023.08.20 val PER: 0.0643
2025-07-30 00:26:12,470: t15.2023.08.25 val PER: 0.0904
2025-07-30 00:26:12,470: t15.2023.08.27 val PER: 0.1511
2025-07-30 00:26:12,471: t15.2023.09.01 val PER: 0.0584
2025-07-30 00:26:12,471: t15.2023.09.03 val PER: 0.1390
2025-07-30 00:26:12,471: t15.2023.09.24 val PER: 0.1068
2025-07-30 00:26:12,471: t15.2023.09.29 val PER: 0.1225
2025-07-30 00:26:12,471: t15.2023.10.01 val PER: 0.1552
2025-07-30 00:26:12,471: t15.2023.10.06 val PER: 0.0721
2025-07-30 00:26:12,471: t15.2023.10.08 val PER: 0.1962
2025-07-30 00:26:12,471: t15.2023.10.13 val PER: 0.1730
2025-07-30 00:26:12,471: t15.2023.10.15 val PER: 0.1213
2025-07-30 00:26:12,471: t15.2023.10.20 val PER: 0.1779
2025-07-30 00:26:12,472: t15.2023.10.22 val PER: 0.1024
2025-07-30 00:26:12,472: t15.2023.11.03 val PER: 0.1635
2025-07-30 00:26:12,472: t15.2023.11.04 val PER: 0.0307
2025-07-30 00:26:12,472: t15.2023.11.17 val PER: 0.0233
2025-07-30 00:26:12,472: t15.2023.11.19 val PER: 0.0180
2025-07-30 00:26:12,472: t15.2023.11.26 val PER: 0.0652
2025-07-30 00:26:12,472: t15.2023.12.03 val PER: 0.0735
2025-07-30 00:26:12,472: t15.2023.12.08 val PER: 0.0626
2025-07-30 00:26:12,472: t15.2023.12.10 val PER: 0.0447
2025-07-30 00:26:12,473: t15.2023.12.17 val PER: 0.0967
2025-07-30 00:26:12,473: t15.2023.12.29 val PER: 0.0824
2025-07-30 00:26:12,473: t15.2024.02.25 val PER: 0.0857
2025-07-30 00:26:12,473: t15.2024.03.08 val PER: 0.2006
2025-07-30 00:26:12,473: t15.2024.03.15 val PER: 0.1864
2025-07-30 00:26:12,473: t15.2024.03.17 val PER: 0.1151
2025-07-30 00:26:12,473: t15.2024.05.10 val PER: 0.1456
2025-07-30 00:26:12,473: t15.2024.06.14 val PER: 0.1230
2025-07-30 00:26:12,474: t15.2024.07.19 val PER: 0.1806
2025-07-30 00:26:12,474: t15.2024.07.21 val PER: 0.0648
2025-07-30 00:26:12,474: t15.2024.07.28 val PER: 0.1132
2025-07-30 00:26:12,474: t15.2025.01.10 val PER: 0.2631
2025-07-30 00:26:12,474: t15.2025.01.12 val PER: 0.1201
2025-07-30 00:26:12,474: t15.2025.03.14 val PER: 0.3565
2025-07-30 00:26:12,474: t15.2025.03.16 val PER: 0.1675
2025-07-30 00:26:12,474: t15.2025.03.30 val PER: 0.2287
2025-07-30 00:26:12,475: t15.2025.04.13 val PER: 0.2026
2025-07-30 00:26:42,371: Train batch 18200: loss: 2.02 grad norm: 22.71 time: 0.111
2025-07-30 00:27:12,791: Train batch 18400: loss: 0.48 grad norm: 11.13 time: 0.106
2025-07-30 00:27:42,890: Train batch 18600: loss: 0.93 grad norm: 16.74 time: 0.099
2025-07-30 00:28:13,452: Train batch 18800: loss: 1.29 grad norm: 22.48 time: 0.093
2025-07-30 00:28:43,835: Train batch 19000: loss: 0.47 grad norm: 25.61 time: 0.105
2025-07-30 00:29:14,374: Train batch 19200: loss: 0.36 grad norm: 12.16 time: 0.100
2025-07-30 00:29:44,612: Train batch 19400: loss: 0.69 grad norm: 18.26 time: 0.103
2025-07-30 00:30:14,756: Train batch 19600: loss: 0.61 grad norm: 14.46 time: 0.094
2025-07-30 00:30:45,257: Train batch 19800: loss: 0.16 grad norm: 5.80 time: 0.076
2025-07-30 00:31:16,085: Train batch 20000: loss: 0.78 grad norm: 11.74 time: 0.086
2025-07-30 00:31:16,085: Running test after training batch: 20000
2025-07-30 00:31:29,499: Val batch 20000: PER (avg): 0.1179 CTC Loss (avg): 21.4238 time: 13.413
2025-07-30 00:31:29,499: t15.2023.08.13 val PER: 0.0811
2025-07-30 00:31:29,499: t15.2023.08.18 val PER: 0.0738
2025-07-30 00:31:29,499: t15.2023.08.20 val PER: 0.0620
2025-07-30 00:31:29,499: t15.2023.08.25 val PER: 0.0798
2025-07-30 00:31:29,500: t15.2023.08.27 val PER: 0.1688
2025-07-30 00:31:29,500: t15.2023.09.01 val PER: 0.0560
2025-07-30 00:31:29,500: t15.2023.09.03 val PER: 0.1354
2025-07-30 00:31:29,500: t15.2023.09.24 val PER: 0.0971
2025-07-30 00:31:29,500: t15.2023.09.29 val PER: 0.1130
2025-07-30 00:31:29,500: t15.2023.10.01 val PER: 0.1407
2025-07-30 00:31:29,500: t15.2023.10.06 val PER: 0.0753
2025-07-30 00:31:29,500: t15.2023.10.08 val PER: 0.1786
2025-07-30 00:31:29,500: t15.2023.10.13 val PER: 0.1777
2025-07-30 00:31:29,501: t15.2023.10.15 val PER: 0.1206
2025-07-30 00:31:29,501: t15.2023.10.20 val PER: 0.1544
2025-07-30 00:31:29,501: t15.2023.10.22 val PER: 0.1125
2025-07-30 00:31:29,501: t15.2023.11.03 val PER: 0.1621
2025-07-30 00:31:29,501: t15.2023.11.04 val PER: 0.0205
2025-07-30 00:31:29,501: t15.2023.11.17 val PER: 0.0171
2025-07-30 00:31:29,501: t15.2023.11.19 val PER: 0.0180
2025-07-30 00:31:29,501: t15.2023.11.26 val PER: 0.0645
2025-07-30 00:31:29,501: t15.2023.12.03 val PER: 0.0725
2025-07-30 00:31:29,501: t15.2023.12.08 val PER: 0.0426
2025-07-30 00:31:29,501: t15.2023.12.10 val PER: 0.0447
2025-07-30 00:31:29,502: t15.2023.12.17 val PER: 0.0936
2025-07-30 00:31:29,502: t15.2023.12.29 val PER: 0.0755
2025-07-30 00:31:29,502: t15.2024.02.25 val PER: 0.0857
2025-07-30 00:31:29,502: t15.2024.03.08 val PER: 0.1878
2025-07-30 00:31:29,502: t15.2024.03.15 val PER: 0.1789
2025-07-30 00:31:29,502: t15.2024.03.17 val PER: 0.0962
2025-07-30 00:31:29,502: t15.2024.05.10 val PER: 0.1530
2025-07-30 00:31:29,502: t15.2024.06.14 val PER: 0.1246
2025-07-30 00:31:29,502: t15.2024.07.19 val PER: 0.1767
2025-07-30 00:31:29,502: t15.2024.07.21 val PER: 0.0717
2025-07-30 00:31:29,503: t15.2024.07.28 val PER: 0.1059
2025-07-30 00:31:29,503: t15.2025.01.10 val PER: 0.2466
2025-07-30 00:31:29,503: t15.2025.01.12 val PER: 0.1232
2025-07-30 00:31:29,503: t15.2025.03.14 val PER: 0.3299
2025-07-30 00:31:29,503: t15.2025.03.16 val PER: 0.1780
2025-07-30 00:31:29,503: t15.2025.03.30 val PER: 0.2241
2025-07-30 00:31:29,503: t15.2025.04.13 val PER: 0.1954
2025-07-30 00:31:29,503: New best test PER 0.1219 --> 0.1179
2025-07-30 00:31:29,503: Checkpointing model
2025-07-30 00:31:30,677: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-07-30 00:32:00,540: Train batch 20200: loss: 0.65 grad norm: 12.18 time: 0.089
2025-07-30 00:32:30,985: Train batch 20400: loss: 0.20 grad norm: 5.41 time: 0.132
2025-07-30 00:33:01,432: Train batch 20600: loss: 0.60 grad norm: 24.36 time: 0.112
2025-07-30 00:33:31,317: Train batch 20800: loss: 1.14 grad norm: 16.63 time: 0.101
2025-07-30 00:34:01,637: Train batch 21000: loss: 0.30 grad norm: 12.49 time: 0.085
2025-07-30 00:34:32,050: Train batch 21200: loss: 0.18 grad norm: 7.27 time: 0.087
2025-07-30 00:35:02,501: Train batch 21400: loss: 0.62 grad norm: 16.55 time: 0.075
2025-07-30 00:35:32,251: Train batch 21600: loss: 0.73 grad norm: 14.60 time: 0.088
2025-07-30 00:36:02,983: Train batch 21800: loss: 0.28 grad norm: 7.60 time: 0.079
2025-07-30 00:36:33,329: Train batch 22000: loss: 0.78 grad norm: 16.04 time: 0.106
2025-07-30 00:36:33,329: Running test after training batch: 22000
2025-07-30 00:36:47,205: Val batch 22000: PER (avg): 0.1176 CTC Loss (avg): 21.8387 time: 13.876
2025-07-30 00:36:47,205: t15.2023.08.13 val PER: 0.0769
2025-07-30 00:36:47,206: t15.2023.08.18 val PER: 0.0712
2025-07-30 00:36:47,206: t15.2023.08.20 val PER: 0.0620
2025-07-30 00:36:47,206: t15.2023.08.25 val PER: 0.0949
2025-07-30 00:36:47,206: t15.2023.08.27 val PER: 0.1592
2025-07-30 00:36:47,206: t15.2023.09.01 val PER: 0.0593
2025-07-30 00:36:47,207: t15.2023.09.03 val PER: 0.1378
2025-07-30 00:36:47,207: t15.2023.09.24 val PER: 0.0825
2025-07-30 00:36:47,207: t15.2023.09.29 val PER: 0.1142
2025-07-30 00:36:47,207: t15.2023.10.01 val PER: 0.1460
2025-07-30 00:36:47,207: t15.2023.10.06 val PER: 0.0764
2025-07-30 00:36:47,207: t15.2023.10.08 val PER: 0.1800
2025-07-30 00:36:47,208: t15.2023.10.13 val PER: 0.1707
2025-07-30 00:36:47,208: t15.2023.10.15 val PER: 0.1292
2025-07-30 00:36:47,208: t15.2023.10.20 val PER: 0.1812
2025-07-30 00:36:47,208: t15.2023.10.22 val PER: 0.1024
2025-07-30 00:36:47,208: t15.2023.11.03 val PER: 0.1533
2025-07-30 00:36:47,208: t15.2023.11.04 val PER: 0.0205
2025-07-30 00:36:47,208: t15.2023.11.17 val PER: 0.0249
2025-07-30 00:36:47,209: t15.2023.11.19 val PER: 0.0299
2025-07-30 00:36:47,209: t15.2023.11.26 val PER: 0.0696
2025-07-30 00:36:47,209: t15.2023.12.03 val PER: 0.0662
2025-07-30 00:36:47,209: t15.2023.12.08 val PER: 0.0519
2025-07-30 00:36:47,209: t15.2023.12.10 val PER: 0.0526
2025-07-30 00:36:47,209: t15.2023.12.17 val PER: 0.0977
2025-07-30 00:36:47,209: t15.2023.12.29 val PER: 0.0885
2025-07-30 00:36:47,210: t15.2024.02.25 val PER: 0.0843
2025-07-30 00:36:47,210: t15.2024.03.08 val PER: 0.2119
2025-07-30 00:36:47,210: t15.2024.03.15 val PER: 0.1801
2025-07-30 00:36:47,210: t15.2024.03.17 val PER: 0.0893
2025-07-30 00:36:47,210: t15.2024.05.10 val PER: 0.1367
2025-07-30 00:36:47,210: t15.2024.06.14 val PER: 0.1025
2025-07-30 00:36:47,210: t15.2024.07.19 val PER: 0.1688
2025-07-30 00:36:47,211: t15.2024.07.21 val PER: 0.0593
2025-07-30 00:36:47,211: t15.2024.07.28 val PER: 0.1029
2025-07-30 00:36:47,211: t15.2025.01.10 val PER: 0.2686
2025-07-30 00:36:47,211: t15.2025.01.12 val PER: 0.1062
2025-07-30 00:36:47,211: t15.2025.03.14 val PER: 0.3136
2025-07-30 00:36:47,211: t15.2025.03.16 val PER: 0.1754
2025-07-30 00:36:47,212: t15.2025.03.30 val PER: 0.2391
2025-07-30 00:36:47,212: t15.2025.04.13 val PER: 0.1883
2025-07-30 00:36:47,212: New best test PER 0.1179 --> 0.1176
2025-07-30 00:36:47,212: Checkpointing model
2025-07-30 00:36:48,426: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-07-30 00:37:18,383: Train batch 22200: loss: 0.32 grad norm: 14.51 time: 0.078
2025-07-30 00:37:48,792: Train batch 22400: loss: 0.55 grad norm: 12.64 time: 0.110
2025-07-30 00:38:18,751: Train batch 22600: loss: 0.26 grad norm: 28.15 time: 0.094
2025-07-30 00:38:48,489: Train batch 22800: loss: 0.41 grad norm: 10.22 time: 0.087
2025-07-30 00:39:18,921: Train batch 23000: loss: 0.42 grad norm: 11.05 time: 0.121
2025-07-30 00:39:49,162: Train batch 23200: loss: 0.63 grad norm: 12.63 time: 0.099
2025-07-30 00:40:19,345: Train batch 23400: loss: 0.54 grad norm: 13.28 time: 0.088
2025-07-30 00:40:50,118: Train batch 23600: loss: 0.38 grad norm: 9.35 time: 0.110
2025-07-30 00:41:19,456: Train batch 23800: loss: 0.38 grad norm: 11.69 time: 0.091
2025-07-30 00:41:49,984: Train batch 24000: loss: 0.32 grad norm: 7.21 time: 0.100
2025-07-30 00:41:49,985: Running test after training batch: 24000
2025-07-30 00:42:02,994: Val batch 24000: PER (avg): 0.1185 CTC Loss (avg): 22.3851 time: 13.008
2025-07-30 00:42:02,994: t15.2023.08.13 val PER: 0.0863
2025-07-30 00:42:02,994: t15.2023.08.18 val PER: 0.0788
2025-07-30 00:42:02,994: t15.2023.08.20 val PER: 0.0620
2025-07-30 00:42:02,994: t15.2023.08.25 val PER: 0.0979
2025-07-30 00:42:02,994: t15.2023.08.27 val PER: 0.1576
2025-07-30 00:42:02,994: t15.2023.09.01 val PER: 0.0544
2025-07-30 00:42:02,994: t15.2023.09.03 val PER: 0.1295
2025-07-30 00:42:02,995: t15.2023.09.24 val PER: 0.0862
2025-07-30 00:42:02,995: t15.2023.09.29 val PER: 0.1091
2025-07-30 00:42:02,995: t15.2023.10.01 val PER: 0.1413
2025-07-30 00:42:02,995: t15.2023.10.06 val PER: 0.0829
2025-07-30 00:42:02,995: t15.2023.10.08 val PER: 0.2003
2025-07-30 00:42:02,995: t15.2023.10.13 val PER: 0.1629
2025-07-30 00:42:02,995: t15.2023.10.15 val PER: 0.1187
2025-07-30 00:42:02,995: t15.2023.10.20 val PER: 0.1711
2025-07-30 00:42:02,995: t15.2023.10.22 val PER: 0.1080
2025-07-30 00:42:02,996: t15.2023.11.03 val PER: 0.1757
2025-07-30 00:42:02,996: t15.2023.11.04 val PER: 0.0137
2025-07-30 00:42:02,996: t15.2023.11.17 val PER: 0.0264
2025-07-30 00:42:02,996: t15.2023.11.19 val PER: 0.0140
2025-07-30 00:42:02,996: t15.2023.11.26 val PER: 0.0616
2025-07-30 00:42:02,996: t15.2023.12.03 val PER: 0.0536
2025-07-30 00:42:02,996: t15.2023.12.08 val PER: 0.0513
2025-07-30 00:42:02,996: t15.2023.12.10 val PER: 0.0473
2025-07-30 00:42:02,997: t15.2023.12.17 val PER: 0.0977
2025-07-30 00:42:02,997: t15.2023.12.29 val PER: 0.0851
2025-07-30 00:42:02,997: t15.2024.02.25 val PER: 0.0702
2025-07-30 00:42:02,997: t15.2024.03.08 val PER: 0.2191
2025-07-30 00:42:02,997: t15.2024.03.15 val PER: 0.1970
2025-07-30 00:42:02,997: t15.2024.03.17 val PER: 0.0858
2025-07-30 00:42:02,997: t15.2024.05.10 val PER: 0.1367
2025-07-30 00:42:02,997: t15.2024.06.14 val PER: 0.1183
2025-07-30 00:42:02,998: t15.2024.07.19 val PER: 0.1688
2025-07-30 00:42:02,998: t15.2024.07.21 val PER: 0.0634
2025-07-30 00:42:02,998: t15.2024.07.28 val PER: 0.0971
2025-07-30 00:42:02,998: t15.2025.01.10 val PER: 0.2617
2025-07-30 00:42:02,998: t15.2025.01.12 val PER: 0.1139
2025-07-30 00:42:02,998: t15.2025.03.14 val PER: 0.3166
2025-07-30 00:42:02,998: t15.2025.03.16 val PER: 0.1898
2025-07-30 00:42:02,998: t15.2025.03.30 val PER: 0.2414
2025-07-30 00:42:02,999: t15.2025.04.13 val PER: 0.2054
2025-07-30 00:42:33,008: Train batch 24200: loss: 1.21 grad norm: 18.24 time: 0.118
2025-07-30 00:43:03,293: Train batch 24400: loss: 0.42 grad norm: 9.13 time: 0.122
2025-07-30 00:43:34,331: Train batch 24600: loss: 0.52 grad norm: 13.26 time: 0.128
2025-07-30 00:44:04,462: Train batch 24800: loss: 0.21 grad norm: 9.43 time: 0.077
2025-07-30 00:44:34,494: Train batch 25000: loss: 0.60 grad norm: 9.29 time: 0.104
2025-07-30 00:45:04,251: Train batch 25200: loss: 0.09 grad norm: 3.71 time: 0.098
2025-07-30 00:45:34,954: Train batch 25400: loss: 0.27 grad norm: 14.05 time: 0.092
2025-07-30 00:46:04,844: Train batch 25600: loss: 0.23 grad norm: 9.54 time: 0.089
2025-07-30 00:46:35,749: Train batch 25800: loss: 0.59 grad norm: 17.54 time: 0.110
2025-07-30 00:47:06,321: Train batch 26000: loss: 0.24 grad norm: 8.18 time: 0.080
2025-07-30 00:47:06,321: Running test after training batch: 26000
2025-07-30 00:47:19,719: Val batch 26000: PER (avg): 0.1184 CTC Loss (avg): 22.5220 time: 13.398
2025-07-30 00:47:19,720: t15.2023.08.13 val PER: 0.0780
2025-07-30 00:47:19,720: t15.2023.08.18 val PER: 0.0712
2025-07-30 00:47:19,720: t15.2023.08.20 val PER: 0.0620
2025-07-30 00:47:19,720: t15.2023.08.25 val PER: 0.0949
2025-07-30 00:47:19,720: t15.2023.08.27 val PER: 0.1624
2025-07-30 00:47:19,720: t15.2023.09.01 val PER: 0.0560
2025-07-30 00:47:19,720: t15.2023.09.03 val PER: 0.1330
2025-07-30 00:47:19,720: t15.2023.09.24 val PER: 0.0862
2025-07-30 00:47:19,721: t15.2023.09.29 val PER: 0.1066
2025-07-30 00:47:19,721: t15.2023.10.01 val PER: 0.1433
2025-07-30 00:47:19,721: t15.2023.10.06 val PER: 0.0710
2025-07-30 00:47:19,721: t15.2023.10.08 val PER: 0.2030
2025-07-30 00:47:19,721: t15.2023.10.13 val PER: 0.1552
2025-07-30 00:47:19,721: t15.2023.10.15 val PER: 0.1285
2025-07-30 00:47:19,721: t15.2023.10.20 val PER: 0.1946
2025-07-30 00:47:19,721: t15.2023.10.22 val PER: 0.1147
2025-07-30 00:47:19,721: t15.2023.11.03 val PER: 0.1493
2025-07-30 00:47:19,721: t15.2023.11.04 val PER: 0.0137
2025-07-30 00:47:19,722: t15.2023.11.17 val PER: 0.0280
2025-07-30 00:47:19,722: t15.2023.11.19 val PER: 0.0200
2025-07-30 00:47:19,722: t15.2023.11.26 val PER: 0.0659
2025-07-30 00:47:19,722: t15.2023.12.03 val PER: 0.0609
2025-07-30 00:47:19,722: t15.2023.12.08 val PER: 0.0566
2025-07-30 00:47:19,722: t15.2023.12.10 val PER: 0.0420
2025-07-30 00:47:19,722: t15.2023.12.17 val PER: 0.1071
2025-07-30 00:47:19,722: t15.2023.12.29 val PER: 0.0810
2025-07-30 00:47:19,723: t15.2024.02.25 val PER: 0.0899
2025-07-30 00:47:19,723: t15.2024.03.08 val PER: 0.2134
2025-07-30 00:47:19,723: t15.2024.03.15 val PER: 0.2008
2025-07-30 00:47:19,723: t15.2024.03.17 val PER: 0.0795
2025-07-30 00:47:19,723: t15.2024.05.10 val PER: 0.1218
2025-07-30 00:47:19,723: t15.2024.06.14 val PER: 0.1104
2025-07-30 00:47:19,723: t15.2024.07.19 val PER: 0.1786
2025-07-30 00:47:19,724: t15.2024.07.21 val PER: 0.0662
2025-07-30 00:47:19,724: t15.2024.07.28 val PER: 0.0941
2025-07-30 00:47:19,724: t15.2025.01.10 val PER: 0.2534
2025-07-30 00:47:19,724: t15.2025.01.12 val PER: 0.1247
2025-07-30 00:47:19,724: t15.2025.03.14 val PER: 0.3343
2025-07-30 00:47:19,724: t15.2025.03.16 val PER: 0.1780
2025-07-30 00:47:19,724: t15.2025.03.30 val PER: 0.2310
2025-07-30 00:47:19,724: t15.2025.04.13 val PER: 0.2083
2025-07-30 00:47:49,098: Train batch 26200: loss: 0.50 grad norm: 15.30 time: 0.093
2025-07-30 00:48:19,577: Train batch 26400: loss: 0.31 grad norm: 9.97 time: 0.125
2025-07-30 00:48:49,497: Train batch 26600: loss: 0.42 grad norm: 31.64 time: 0.082
2025-07-30 00:49:19,977: Train batch 26800: loss: 0.09 grad norm: 4.03 time: 0.073
2025-07-30 00:49:50,502: Train batch 27000: loss: 0.47 grad norm: 16.91 time: 0.086
2025-07-30 00:50:20,625: Train batch 27200: loss: 0.79 grad norm: 17.29 time: 0.089
2025-07-30 00:50:51,440: Train batch 27400: loss: 0.32 grad norm: 18.89 time: 0.088
2025-07-30 00:51:21,910: Train batch 27600: loss: 0.37 grad norm: 11.13 time: 0.107
2025-07-30 00:51:51,617: Train batch 27800: loss: 0.38 grad norm: 8.03 time: 0.086
2025-07-30 00:52:22,005: Train batch 28000: loss: 0.83 grad norm: 24.10 time: 0.094
2025-07-30 00:52:22,005: Running test after training batch: 28000
2025-07-30 00:52:35,422: Val batch 28000: PER (avg): 0.1175 CTC Loss (avg): 22.4220 time: 13.416
2025-07-30 00:52:35,422: t15.2023.08.13 val PER: 0.0759
2025-07-30 00:52:35,422: t15.2023.08.18 val PER: 0.0763
2025-07-30 00:52:35,422: t15.2023.08.20 val PER: 0.0627
2025-07-30 00:52:35,422: t15.2023.08.25 val PER: 0.0889
2025-07-30 00:52:35,422: t15.2023.08.27 val PER: 0.1576
2025-07-30 00:52:35,422: t15.2023.09.01 val PER: 0.0552
2025-07-30 00:52:35,422: t15.2023.09.03 val PER: 0.1188
2025-07-30 00:52:35,423: t15.2023.09.24 val PER: 0.0886
2025-07-30 00:52:35,423: t15.2023.09.29 val PER: 0.1008
2025-07-30 00:52:35,423: t15.2023.10.01 val PER: 0.1380
2025-07-30 00:52:35,423: t15.2023.10.06 val PER: 0.0872
2025-07-30 00:52:35,423: t15.2023.10.08 val PER: 0.1881
2025-07-30 00:52:35,423: t15.2023.10.13 val PER: 0.1583
2025-07-30 00:52:35,423: t15.2023.10.15 val PER: 0.1279
2025-07-30 00:52:35,423: t15.2023.10.20 val PER: 0.1846
2025-07-30 00:52:35,423: t15.2023.10.22 val PER: 0.1091
2025-07-30 00:52:35,423: t15.2023.11.03 val PER: 0.1649
2025-07-30 00:52:35,424: t15.2023.11.04 val PER: 0.0171
2025-07-30 00:52:35,424: t15.2023.11.17 val PER: 0.0156
2025-07-30 00:52:35,424: t15.2023.11.19 val PER: 0.0220
2025-07-30 00:52:35,424: t15.2023.11.26 val PER: 0.0543
2025-07-30 00:52:35,424: t15.2023.12.03 val PER: 0.0735
2025-07-30 00:52:35,424: t15.2023.12.08 val PER: 0.0526
2025-07-30 00:52:35,424: t15.2023.12.10 val PER: 0.0381
2025-07-30 00:52:35,424: t15.2023.12.17 val PER: 0.0977
2025-07-30 00:52:35,424: t15.2023.12.29 val PER: 0.0872
2025-07-30 00:52:35,424: t15.2024.02.25 val PER: 0.0885
2025-07-30 00:52:35,425: t15.2024.03.08 val PER: 0.1963
2025-07-30 00:52:35,425: t15.2024.03.15 val PER: 0.2045
2025-07-30 00:52:35,425: t15.2024.03.17 val PER: 0.0962
2025-07-30 00:52:35,425: t15.2024.05.10 val PER: 0.1233
2025-07-30 00:52:35,425: t15.2024.06.14 val PER: 0.1325
2025-07-30 00:52:35,425: t15.2024.07.19 val PER: 0.1655
2025-07-30 00:52:35,425: t15.2024.07.21 val PER: 0.0669
2025-07-30 00:52:35,425: t15.2024.07.28 val PER: 0.0956
2025-07-30 00:52:35,425: t15.2025.01.10 val PER: 0.2810
2025-07-30 00:52:35,426: t15.2025.01.12 val PER: 0.0962
2025-07-30 00:52:35,426: t15.2025.03.14 val PER: 0.3284
2025-07-30 00:52:35,426: t15.2025.03.16 val PER: 0.1898
2025-07-30 00:52:35,426: t15.2025.03.30 val PER: 0.2241
2025-07-30 00:52:35,426: t15.2025.04.13 val PER: 0.1997
2025-07-30 00:52:35,426: New best test PER 0.1176 --> 0.1175
2025-07-30 00:52:35,426: Checkpointing model
2025-07-30 00:52:36,731: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-07-30 00:53:06,551: Train batch 28200: loss: 0.13 grad norm: 3.73 time: 0.078
2025-07-30 00:53:37,118: Train batch 28400: loss: 0.29 grad norm: 8.13 time: 0.089
2025-07-30 00:54:07,482: Train batch 28600: loss: 0.32 grad norm: 9.88 time: 0.120
2025-07-30 00:54:37,512: Train batch 28800: loss: 0.37 grad norm: 11.10 time: 0.100
2025-07-30 00:55:07,439: Train batch 29000: loss: 0.52 grad norm: 22.14 time: 0.086
2025-07-30 00:55:37,465: Train batch 29200: loss: 0.52 grad norm: 15.08 time: 0.090
2025-07-30 00:56:08,169: Train batch 29400: loss: 0.43 grad norm: 17.17 time: 0.127
2025-07-30 00:56:38,197: Train batch 29600: loss: 0.13 grad norm: 4.43 time: 0.132
2025-07-30 00:57:08,293: Train batch 29800: loss: 0.13 grad norm: 12.28 time: 0.093
2025-07-30 00:57:38,159: Train batch 30000: loss: 0.31 grad norm: 9.58 time: 0.095
2025-07-30 00:57:38,159: Running test after training batch: 30000
2025-07-30 00:57:52,038: Val batch 30000: PER (avg): 0.1144 CTC Loss (avg): 22.5881 time: 13.878
2025-07-30 00:57:52,038: t15.2023.08.13 val PER: 0.0738
2025-07-30 00:57:52,038: t15.2023.08.18 val PER: 0.0763
2025-07-30 00:57:52,038: t15.2023.08.20 val PER: 0.0635
2025-07-30 00:57:52,038: t15.2023.08.25 val PER: 0.0889
2025-07-30 00:57:52,038: t15.2023.08.27 val PER: 0.1576
2025-07-30 00:57:52,039: t15.2023.09.01 val PER: 0.0528
2025-07-30 00:57:52,039: t15.2023.09.03 val PER: 0.1200
2025-07-30 00:57:52,039: t15.2023.09.24 val PER: 0.0813
2025-07-30 00:57:52,039: t15.2023.09.29 val PER: 0.1078
2025-07-30 00:57:52,039: t15.2023.10.01 val PER: 0.1453
2025-07-30 00:57:52,039: t15.2023.10.06 val PER: 0.0786
2025-07-30 00:57:52,039: t15.2023.10.08 val PER: 0.1800
2025-07-30 00:57:52,039: t15.2023.10.13 val PER: 0.1497
2025-07-30 00:57:52,040: t15.2023.10.15 val PER: 0.1187
2025-07-30 00:57:52,040: t15.2023.10.20 val PER: 0.1745
2025-07-30 00:57:52,040: t15.2023.10.22 val PER: 0.1058
2025-07-30 00:57:52,040: t15.2023.11.03 val PER: 0.1479
2025-07-30 00:57:52,040: t15.2023.11.04 val PER: 0.0171
2025-07-30 00:57:52,040: t15.2023.11.17 val PER: 0.0233
2025-07-30 00:57:52,040: t15.2023.11.19 val PER: 0.0240
2025-07-30 00:57:52,040: t15.2023.11.26 val PER: 0.0500
2025-07-30 00:57:52,041: t15.2023.12.03 val PER: 0.0651
2025-07-30 00:57:52,041: t15.2023.12.08 val PER: 0.0399
2025-07-30 00:57:52,041: t15.2023.12.10 val PER: 0.0420
2025-07-30 00:57:52,041: t15.2023.12.17 val PER: 0.1008
2025-07-30 00:57:52,041: t15.2023.12.29 val PER: 0.0803
2025-07-30 00:57:52,041: t15.2024.02.25 val PER: 0.0787
2025-07-30 00:57:52,041: t15.2024.03.08 val PER: 0.1963
2025-07-30 00:57:52,041: t15.2024.03.15 val PER: 0.1757
2025-07-30 00:57:52,042: t15.2024.03.17 val PER: 0.0941
2025-07-30 00:57:52,042: t15.2024.05.10 val PER: 0.1367
2025-07-30 00:57:52,042: t15.2024.06.14 val PER: 0.1151
2025-07-30 00:57:52,042: t15.2024.07.19 val PER: 0.1701
2025-07-30 00:57:52,042: t15.2024.07.21 val PER: 0.0607
2025-07-30 00:57:52,042: t15.2024.07.28 val PER: 0.1044
2025-07-30 00:57:52,042: t15.2025.01.10 val PER: 0.2590
2025-07-30 00:57:52,042: t15.2025.01.12 val PER: 0.1147
2025-07-30 00:57:52,043: t15.2025.03.14 val PER: 0.3166
2025-07-30 00:57:52,043: t15.2025.03.16 val PER: 0.1832
2025-07-30 00:57:52,043: t15.2025.03.30 val PER: 0.2241
2025-07-30 00:57:52,043: t15.2025.04.13 val PER: 0.2154
2025-07-30 00:57:52,043: New best test PER 0.1175 --> 0.1144
2025-07-30 00:57:52,043: Checkpointing model
2025-07-30 00:57:53,233: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-07-30 00:58:23,867: Train batch 30200: loss: 0.21 grad norm: 7.56 time: 0.094
2025-07-30 00:58:54,231: Train batch 30400: loss: 0.20 grad norm: 7.22 time: 0.113
2025-07-30 00:59:24,688: Train batch 30600: loss: 0.23 grad norm: 6.98 time: 0.069
2025-07-30 00:59:54,649: Train batch 30800: loss: 0.38 grad norm: 10.84 time: 0.089
2025-07-30 01:00:24,983: Train batch 31000: loss: 0.10 grad norm: 20.51 time: 0.085
2025-07-30 01:00:56,112: Train batch 31200: loss: 0.25 grad norm: 5.79 time: 0.130
2025-07-30 01:01:26,117: Train batch 31400: loss: 0.40 grad norm: 11.94 time: 0.089
2025-07-30 01:01:56,356: Train batch 31600: loss: 0.31 grad norm: 10.48 time: 0.094
2025-07-30 01:02:26,482: Train batch 31800: loss: 0.23 grad norm: 9.85 time: 0.078
2025-07-30 01:02:56,382: Train batch 32000: loss: 0.28 grad norm: 28.85 time: 0.116
2025-07-30 01:02:56,382: Running test after training batch: 32000
2025-07-30 01:03:10,175: Val batch 32000: PER (avg): 0.1131 CTC Loss (avg): 22.4148 time: 13.793
2025-07-30 01:03:10,175: t15.2023.08.13 val PER: 0.0769
2025-07-30 01:03:10,175: t15.2023.08.18 val PER: 0.0696
2025-07-30 01:03:10,176: t15.2023.08.20 val PER: 0.0596
2025-07-30 01:03:10,176: t15.2023.08.25 val PER: 0.0828
2025-07-30 01:03:10,176: t15.2023.08.27 val PER: 0.1495
2025-07-30 01:03:10,176: t15.2023.09.01 val PER: 0.0560
2025-07-30 01:03:10,176: t15.2023.09.03 val PER: 0.1200
2025-07-30 01:03:10,176: t15.2023.09.24 val PER: 0.0777
2025-07-30 01:03:10,176: t15.2023.09.29 val PER: 0.1078
2025-07-30 01:03:10,176: t15.2023.10.01 val PER: 0.1446
2025-07-30 01:03:10,176: t15.2023.10.06 val PER: 0.0829
2025-07-30 01:03:10,177: t15.2023.10.08 val PER: 0.1867
2025-07-30 01:03:10,177: t15.2023.10.13 val PER: 0.1575
2025-07-30 01:03:10,177: t15.2023.10.15 val PER: 0.1213
2025-07-30 01:03:10,177: t15.2023.10.20 val PER: 0.1644
2025-07-30 01:03:10,177: t15.2023.10.22 val PER: 0.1069
2025-07-30 01:03:10,177: t15.2023.11.03 val PER: 0.1601
2025-07-30 01:03:10,177: t15.2023.11.04 val PER: 0.0171
2025-07-30 01:03:10,177: t15.2023.11.17 val PER: 0.0249
2025-07-30 01:03:10,177: t15.2023.11.19 val PER: 0.0299
2025-07-30 01:03:10,177: t15.2023.11.26 val PER: 0.0594
2025-07-30 01:03:10,178: t15.2023.12.03 val PER: 0.0609
2025-07-30 01:03:10,178: t15.2023.12.08 val PER: 0.0459
2025-07-30 01:03:10,178: t15.2023.12.10 val PER: 0.0381
2025-07-30 01:03:10,178: t15.2023.12.17 val PER: 0.1008
2025-07-30 01:03:10,178: t15.2023.12.29 val PER: 0.0776
2025-07-30 01:03:10,178: t15.2024.02.25 val PER: 0.0772
2025-07-30 01:03:10,178: t15.2024.03.08 val PER: 0.1906
2025-07-30 01:03:10,178: t15.2024.03.15 val PER: 0.1689
2025-07-30 01:03:10,178: t15.2024.03.17 val PER: 0.0781
2025-07-30 01:03:10,179: t15.2024.05.10 val PER: 0.1218
2025-07-30 01:03:10,179: t15.2024.06.14 val PER: 0.1262
2025-07-30 01:03:10,179: t15.2024.07.19 val PER: 0.1707
2025-07-30 01:03:10,179: t15.2024.07.21 val PER: 0.0552
2025-07-30 01:03:10,179: t15.2024.07.28 val PER: 0.0949
2025-07-30 01:03:10,179: t15.2025.01.10 val PER: 0.2590
2025-07-30 01:03:10,179: t15.2025.01.12 val PER: 0.1070
2025-07-30 01:03:10,179: t15.2025.03.14 val PER: 0.3225
2025-07-30 01:03:10,180: t15.2025.03.16 val PER: 0.1636
2025-07-30 01:03:10,180: t15.2025.03.30 val PER: 0.2368
2025-07-30 01:03:10,180: t15.2025.04.13 val PER: 0.1926
2025-07-30 01:03:10,180: New best test PER 0.1144 --> 0.1131
2025-07-30 01:03:10,180: Checkpointing model
2025-07-30 01:03:11,351: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-07-30 01:03:41,373: Train batch 32200: loss: 0.28 grad norm: 7.87 time: 0.080
2025-07-30 01:04:11,398: Train batch 32400: loss: 0.27 grad norm: 9.91 time: 0.087
2025-07-30 01:04:41,810: Train batch 32600: loss: 0.28 grad norm: 10.42 time: 0.101
2025-07-30 01:05:12,044: Train batch 32800: loss: 0.11 grad norm: 5.31 time: 0.098
2025-07-30 01:05:41,819: Train batch 33000: loss: 0.32 grad norm: 19.88 time: 0.089
2025-07-30 01:06:11,731: Train batch 33200: loss: 0.83 grad norm: 10.12 time: 0.100
2025-07-30 01:06:42,684: Train batch 33400: loss: 0.22 grad norm: 25.11 time: 0.124
2025-07-30 01:07:12,953: Train batch 33600: loss: 0.47 grad norm: 10.21 time: 0.084
2025-07-30 01:07:42,793: Train batch 33800: loss: 0.33 grad norm: 12.69 time: 0.094
2025-07-30 01:08:13,029: Train batch 34000: loss: 0.10 grad norm: 6.14 time: 0.090
2025-07-30 01:08:13,029: Running test after training batch: 34000
2025-07-30 01:08:26,995: Val batch 34000: PER (avg): 0.1146 CTC Loss (avg): 23.9100 time: 13.965
2025-07-30 01:08:26,995: t15.2023.08.13 val PER: 0.0759
2025-07-30 01:08:26,995: t15.2023.08.18 val PER: 0.0738
2025-07-30 01:08:26,995: t15.2023.08.20 val PER: 0.0651
2025-07-30 01:08:26,995: t15.2023.08.25 val PER: 0.0889
2025-07-30 01:08:26,996: t15.2023.08.27 val PER: 0.1624
2025-07-30 01:08:26,996: t15.2023.09.01 val PER: 0.0495
2025-07-30 01:08:26,996: t15.2023.09.03 val PER: 0.1271
2025-07-30 01:08:26,996: t15.2023.09.24 val PER: 0.0850
2025-07-30 01:08:26,996: t15.2023.09.29 val PER: 0.1098
2025-07-30 01:08:26,996: t15.2023.10.01 val PER: 0.1486
2025-07-30 01:08:26,996: t15.2023.10.06 val PER: 0.0614
2025-07-30 01:08:26,996: t15.2023.10.08 val PER: 0.2016
2025-07-30 01:08:26,996: t15.2023.10.13 val PER: 0.1645
2025-07-30 01:08:26,996: t15.2023.10.15 val PER: 0.1154
2025-07-30 01:08:26,997: t15.2023.10.20 val PER: 0.1711
2025-07-30 01:08:26,997: t15.2023.10.22 val PER: 0.1080
2025-07-30 01:08:26,997: t15.2023.11.03 val PER: 0.1560
2025-07-30 01:08:26,997: t15.2023.11.04 val PER: 0.0171
2025-07-30 01:08:26,997: t15.2023.11.17 val PER: 0.0264
2025-07-30 01:08:26,997: t15.2023.11.19 val PER: 0.0200
2025-07-30 01:08:26,997: t15.2023.11.26 val PER: 0.0609
2025-07-30 01:08:26,997: t15.2023.12.03 val PER: 0.0630
2025-07-30 01:08:26,997: t15.2023.12.08 val PER: 0.0426
2025-07-30 01:08:26,997: t15.2023.12.10 val PER: 0.0499
2025-07-30 01:08:26,998: t15.2023.12.17 val PER: 0.0894
2025-07-30 01:08:26,998: t15.2023.12.29 val PER: 0.0796
2025-07-30 01:08:26,998: t15.2024.02.25 val PER: 0.0744
2025-07-30 01:08:26,998: t15.2024.03.08 val PER: 0.1878
2025-07-30 01:08:26,998: t15.2024.03.15 val PER: 0.1757
2025-07-30 01:08:26,998: t15.2024.03.17 val PER: 0.0865
2025-07-30 01:08:26,998: t15.2024.05.10 val PER: 0.1263
2025-07-30 01:08:26,998: t15.2024.06.14 val PER: 0.1009
2025-07-30 01:08:26,999: t15.2024.07.19 val PER: 0.1721
2025-07-30 01:08:26,999: t15.2024.07.21 val PER: 0.0593
2025-07-30 01:08:26,999: t15.2024.07.28 val PER: 0.0963
2025-07-30 01:08:26,999: t15.2025.01.10 val PER: 0.2603
2025-07-30 01:08:26,999: t15.2025.01.12 val PER: 0.1186
2025-07-30 01:08:26,999: t15.2025.03.14 val PER: 0.3269
2025-07-30 01:08:26,999: t15.2025.03.16 val PER: 0.1636
2025-07-30 01:08:26,999: t15.2025.03.30 val PER: 0.2368
2025-07-30 01:08:27,000: t15.2025.04.13 val PER: 0.2026
2025-07-30 01:08:56,966: Train batch 34200: loss: 0.22 grad norm: 7.96 time: 0.088
2025-07-30 01:09:27,350: Train batch 34400: loss: 0.18 grad norm: 6.02 time: 0.104
2025-07-30 01:09:58,296: Train batch 34600: loss: 0.29 grad norm: 8.41 time: 0.083
2025-07-30 01:10:28,370: Train batch 34800: loss: 0.11 grad norm: 3.67 time: 0.077
2025-07-30 01:10:58,620: Train batch 35000: loss: 0.11 grad norm: 5.03 time: 0.086
2025-07-30 01:11:28,686: Train batch 35200: loss: 0.07 grad norm: 3.72 time: 0.095
2025-07-30 01:11:58,621: Train batch 35400: loss: 0.13 grad norm: 14.09 time: 0.070
2025-07-30 01:12:28,864: Train batch 35600: loss: 0.24 grad norm: 7.39 time: 0.085
2025-07-30 01:12:59,104: Train batch 35800: loss: 0.03 grad norm: 2.06 time: 0.118
2025-07-30 01:13:29,350: Train batch 36000: loss: 0.09 grad norm: 3.54 time: 0.080
2025-07-30 01:13:29,350: Running test after training batch: 36000
2025-07-30 01:13:43,140: Val batch 36000: PER (avg): 0.1155 CTC Loss (avg): 23.3662 time: 13.790
2025-07-30 01:13:43,141: t15.2023.08.13 val PER: 0.0717
2025-07-30 01:13:43,141: t15.2023.08.18 val PER: 0.0813
2025-07-30 01:13:43,141: t15.2023.08.20 val PER: 0.0635
2025-07-30 01:13:43,141: t15.2023.08.25 val PER: 0.0693
2025-07-30 01:13:43,141: t15.2023.08.27 val PER: 0.1543
2025-07-30 01:13:43,141: t15.2023.09.01 val PER: 0.0576
2025-07-30 01:13:43,141: t15.2023.09.03 val PER: 0.1271
2025-07-30 01:13:43,141: t15.2023.09.24 val PER: 0.0777
2025-07-30 01:13:43,142: t15.2023.09.29 val PER: 0.1072
2025-07-30 01:13:43,142: t15.2023.10.01 val PER: 0.1308
2025-07-30 01:13:43,142: t15.2023.10.06 val PER: 0.0753
2025-07-30 01:13:43,142: t15.2023.10.08 val PER: 0.1678
2025-07-30 01:13:43,142: t15.2023.10.13 val PER: 0.1668
2025-07-30 01:13:43,142: t15.2023.10.15 val PER: 0.1193
2025-07-30 01:13:43,142: t15.2023.10.20 val PER: 0.1577
2025-07-30 01:13:43,142: t15.2023.10.22 val PER: 0.1091
2025-07-30 01:13:43,143: t15.2023.11.03 val PER: 0.1581
2025-07-30 01:13:43,143: t15.2023.11.04 val PER: 0.0205
2025-07-30 01:13:43,143: t15.2023.11.17 val PER: 0.0280
2025-07-30 01:13:43,143: t15.2023.11.19 val PER: 0.0180
2025-07-30 01:13:43,143: t15.2023.11.26 val PER: 0.0587
2025-07-30 01:13:43,143: t15.2023.12.03 val PER: 0.0588
2025-07-30 01:13:43,143: t15.2023.12.08 val PER: 0.0559
2025-07-30 01:13:43,143: t15.2023.12.10 val PER: 0.0526
2025-07-30 01:13:43,144: t15.2023.12.17 val PER: 0.1008
2025-07-30 01:13:43,144: t15.2023.12.29 val PER: 0.0721
2025-07-30 01:13:43,144: t15.2024.02.25 val PER: 0.0758
2025-07-30 01:13:43,144: t15.2024.03.08 val PER: 0.1977
2025-07-30 01:13:43,144: t15.2024.03.15 val PER: 0.1826
2025-07-30 01:13:43,144: t15.2024.03.17 val PER: 0.0914
2025-07-30 01:13:43,144: t15.2024.05.10 val PER: 0.1278
2025-07-30 01:13:43,144: t15.2024.06.14 val PER: 0.1120
2025-07-30 01:13:43,144: t15.2024.07.19 val PER: 0.1688
2025-07-30 01:13:43,145: t15.2024.07.21 val PER: 0.0676
2025-07-30 01:13:43,145: t15.2024.07.28 val PER: 0.1074
2025-07-30 01:13:43,145: t15.2025.01.10 val PER: 0.2658
2025-07-30 01:13:43,145: t15.2025.01.12 val PER: 0.1193
2025-07-30 01:13:43,145: t15.2025.03.14 val PER: 0.3195
2025-07-30 01:13:43,145: t15.2025.03.16 val PER: 0.1571
2025-07-30 01:13:43,145: t15.2025.03.30 val PER: 0.2379
2025-07-30 01:13:43,145: t15.2025.04.13 val PER: 0.2225
2025-07-30 01:14:12,968: Train batch 36200: loss: 0.21 grad norm: 7.54 time: 0.077
2025-07-30 01:14:43,404: Train batch 36400: loss: 0.11 grad norm: 5.48 time: 0.105
2025-07-30 01:15:13,378: Train batch 36600: loss: 0.29 grad norm: 10.89 time: 0.122
2025-07-30 01:15:43,461: Train batch 36800: loss: 0.20 grad norm: 6.41 time: 0.088
2025-07-30 01:16:13,637: Train batch 37000: loss: 0.38 grad norm: 5.35 time: 0.090
2025-07-30 01:16:43,950: Train batch 37200: loss: 0.11 grad norm: 4.99 time: 0.096
2025-07-30 01:17:14,215: Train batch 37400: loss: 0.07 grad norm: 4.90 time: 0.101
2025-07-30 01:17:44,429: Train batch 37600: loss: 0.21 grad norm: 7.65 time: 0.070
2025-07-30 01:18:14,534: Train batch 37800: loss: 0.14 grad norm: 5.73 time: 0.080
2025-07-30 01:18:45,305: Train batch 38000: loss: 0.17 grad norm: 6.05 time: 0.117
2025-07-30 01:18:45,305: Running test after training batch: 38000
2025-07-30 01:18:58,961: Val batch 38000: PER (avg): 0.1122 CTC Loss (avg): 23.2346 time: 13.656
2025-07-30 01:18:58,962: t15.2023.08.13 val PER: 0.0769
2025-07-30 01:18:58,962: t15.2023.08.18 val PER: 0.0771
2025-07-30 01:18:58,962: t15.2023.08.20 val PER: 0.0620
2025-07-30 01:18:58,962: t15.2023.08.25 val PER: 0.0919
2025-07-30 01:18:58,962: t15.2023.08.27 val PER: 0.1479
2025-07-30 01:18:58,962: t15.2023.09.01 val PER: 0.0455
2025-07-30 01:18:58,962: t15.2023.09.03 val PER: 0.1223
2025-07-30 01:18:58,962: t15.2023.09.24 val PER: 0.0898
2025-07-30 01:18:58,963: t15.2023.09.29 val PER: 0.1027
2025-07-30 01:18:58,963: t15.2023.10.01 val PER: 0.1308
2025-07-30 01:18:58,963: t15.2023.10.06 val PER: 0.0700
2025-07-30 01:18:58,963: t15.2023.10.08 val PER: 0.1867
2025-07-30 01:18:58,963: t15.2023.10.13 val PER: 0.1590
2025-07-30 01:18:58,963: t15.2023.10.15 val PER: 0.1213
2025-07-30 01:18:58,963: t15.2023.10.20 val PER: 0.1611
2025-07-30 01:18:58,963: t15.2023.10.22 val PER: 0.1136
2025-07-30 01:18:58,963: t15.2023.11.03 val PER: 0.1635
2025-07-30 01:18:58,964: t15.2023.11.04 val PER: 0.0171
2025-07-30 01:18:58,964: t15.2023.11.17 val PER: 0.0264
2025-07-30 01:18:58,964: t15.2023.11.19 val PER: 0.0220
2025-07-30 01:18:58,964: t15.2023.11.26 val PER: 0.0514
2025-07-30 01:18:58,964: t15.2023.12.03 val PER: 0.0494
2025-07-30 01:18:58,964: t15.2023.12.08 val PER: 0.0433
2025-07-30 01:18:58,964: t15.2023.12.10 val PER: 0.0447
2025-07-30 01:18:58,964: t15.2023.12.17 val PER: 0.1091
2025-07-30 01:18:58,964: t15.2023.12.29 val PER: 0.0824
2025-07-30 01:18:58,964: t15.2024.02.25 val PER: 0.0871
2025-07-30 01:18:58,965: t15.2024.03.08 val PER: 0.1906
2025-07-30 01:18:58,965: t15.2024.03.15 val PER: 0.1776
2025-07-30 01:18:58,965: t15.2024.03.17 val PER: 0.0872
2025-07-30 01:18:58,965: t15.2024.05.10 val PER: 0.1174
2025-07-30 01:18:58,965: t15.2024.06.14 val PER: 0.1041
2025-07-30 01:18:58,965: t15.2024.07.19 val PER: 0.1622
2025-07-30 01:18:58,965: t15.2024.07.21 val PER: 0.0579
2025-07-30 01:18:58,965: t15.2024.07.28 val PER: 0.0926
2025-07-30 01:18:58,965: t15.2025.01.10 val PER: 0.2466
2025-07-30 01:18:58,965: t15.2025.01.12 val PER: 0.0993
2025-07-30 01:18:58,966: t15.2025.03.14 val PER: 0.3254
2025-07-30 01:18:58,966: t15.2025.03.16 val PER: 0.1545
2025-07-30 01:18:58,966: t15.2025.03.30 val PER: 0.2345
2025-07-30 01:18:58,966: t15.2025.04.13 val PER: 0.1997
2025-07-30 01:18:58,966: New best test PER 0.1131 --> 0.1122
2025-07-30 01:18:58,966: Checkpointing model
2025-07-30 01:19:00,201: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-07-30 01:19:30,292: Train batch 38200: loss: 0.24 grad norm: 8.18 time: 0.101
2025-07-30 01:20:00,579: Train batch 38400: loss: 0.20 grad norm: 8.35 time: 0.083
2025-07-30 01:20:30,737: Train batch 38600: loss: 0.26 grad norm: 10.89 time: 0.104
2025-07-30 01:21:00,714: Train batch 38800: loss: 0.25 grad norm: 6.39 time: 0.092
2025-07-30 01:21:30,869: Train batch 39000: loss: 0.08 grad norm: 3.88 time: 0.119
2025-07-30 01:22:01,232: Train batch 39200: loss: 0.48 grad norm: 15.08 time: 0.077
2025-07-30 01:22:30,939: Train batch 39400: loss: 0.18 grad norm: 7.06 time: 0.086
2025-07-30 01:23:01,556: Train batch 39600: loss: 0.24 grad norm: 8.75 time: 0.099
2025-07-30 01:23:31,502: Train batch 39800: loss: 0.10 grad norm: 8.02 time: 0.085
2025-07-30 01:24:01,666: Train batch 40000: loss: 0.11 grad norm: 5.09 time: 0.096
2025-07-30 01:24:01,666: Running test after training batch: 40000
2025-07-30 01:24:14,999: Val batch 40000: PER (avg): 0.1128 CTC Loss (avg): 23.8078 time: 13.333
2025-07-30 01:24:15,000: t15.2023.08.13 val PER: 0.0748
2025-07-30 01:24:15,001: t15.2023.08.18 val PER: 0.0671
2025-07-30 01:24:15,001: t15.2023.08.20 val PER: 0.0620
2025-07-30 01:24:15,001: t15.2023.08.25 val PER: 0.0919
2025-07-30 01:24:15,001: t15.2023.08.27 val PER: 0.1720
2025-07-30 01:24:15,001: t15.2023.09.01 val PER: 0.0519
2025-07-30 01:24:15,002: t15.2023.09.03 val PER: 0.1223
2025-07-30 01:24:15,002: t15.2023.09.24 val PER: 0.0813
2025-07-30 01:24:15,002: t15.2023.09.29 val PER: 0.1034
2025-07-30 01:24:15,002: t15.2023.10.01 val PER: 0.1328
2025-07-30 01:24:15,002: t15.2023.10.06 val PER: 0.0624
2025-07-30 01:24:15,002: t15.2023.10.08 val PER: 0.1840
2025-07-30 01:24:15,003: t15.2023.10.13 val PER: 0.1575
2025-07-30 01:24:15,003: t15.2023.10.15 val PER: 0.1193
2025-07-30 01:24:15,003: t15.2023.10.20 val PER: 0.1779
2025-07-30 01:24:15,003: t15.2023.10.22 val PER: 0.1114
2025-07-30 01:24:15,003: t15.2023.11.03 val PER: 0.1438
2025-07-30 01:24:15,003: t15.2023.11.04 val PER: 0.0239
2025-07-30 01:24:15,004: t15.2023.11.17 val PER: 0.0264
2025-07-30 01:24:15,004: t15.2023.11.19 val PER: 0.0180
2025-07-30 01:24:15,004: t15.2023.11.26 val PER: 0.0609
2025-07-30 01:24:15,004: t15.2023.12.03 val PER: 0.0557
2025-07-30 01:24:15,004: t15.2023.12.08 val PER: 0.0506
2025-07-30 01:24:15,004: t15.2023.12.10 val PER: 0.0368
2025-07-30 01:24:15,004: t15.2023.12.17 val PER: 0.1091
2025-07-30 01:24:15,005: t15.2023.12.29 val PER: 0.0741
2025-07-30 01:24:15,005: t15.2024.02.25 val PER: 0.0871
2025-07-30 01:24:15,005: t15.2024.03.08 val PER: 0.1906
2025-07-30 01:24:15,005: t15.2024.03.15 val PER: 0.1845
2025-07-30 01:24:15,005: t15.2024.03.17 val PER: 0.0879
2025-07-30 01:24:15,005: t15.2024.05.10 val PER: 0.1278
2025-07-30 01:24:15,006: t15.2024.06.14 val PER: 0.1167
2025-07-30 01:24:15,006: t15.2024.07.19 val PER: 0.1661
2025-07-30 01:24:15,006: t15.2024.07.21 val PER: 0.0641
2025-07-30 01:24:15,006: t15.2024.07.28 val PER: 0.0926
2025-07-30 01:24:15,006: t15.2025.01.10 val PER: 0.2479
2025-07-30 01:24:15,006: t15.2025.01.12 val PER: 0.1093
2025-07-30 01:24:15,006: t15.2025.03.14 val PER: 0.3254
2025-07-30 01:24:15,007: t15.2025.03.16 val PER: 0.1636
2025-07-30 01:24:15,007: t15.2025.03.30 val PER: 0.2368
2025-07-30 01:24:15,007: t15.2025.04.13 val PER: 0.1783
2025-07-30 01:24:44,941: Train batch 40200: loss: 0.11 grad norm: 5.51 time: 0.080
2025-07-30 01:25:15,586: Train batch 40400: loss: 0.13 grad norm: 5.49 time: 0.071
2025-07-30 01:25:46,154: Train batch 40600: loss: 0.28 grad norm: 9.50 time: 0.091
2025-07-30 01:26:16,982: Train batch 40800: loss: 0.10 grad norm: 17.06 time: 0.117
2025-07-30 01:26:47,200: Train batch 41000: loss: 0.20 grad norm: 7.87 time: 0.121
2025-07-30 01:27:17,840: Train batch 41200: loss: 0.07 grad norm: 7.03 time: 0.111
2025-07-30 01:27:47,754: Train batch 41400: loss: 0.06 grad norm: 3.71 time: 0.075
2025-07-30 01:28:18,115: Train batch 41600: loss: 0.21 grad norm: 7.33 time: 0.085
2025-07-30 01:28:48,427: Train batch 41800: loss: 0.13 grad norm: 10.51 time: 0.123
2025-07-30 01:29:18,261: Train batch 42000: loss: 0.19 grad norm: 7.36 time: 0.075
2025-07-30 01:29:18,262: Running test after training batch: 42000
2025-07-30 01:29:31,549: Val batch 42000: PER (avg): 0.1124 CTC Loss (avg): 23.1988 time: 13.287
2025-07-30 01:29:31,549: t15.2023.08.13 val PER: 0.0717
2025-07-30 01:29:31,549: t15.2023.08.18 val PER: 0.0754
2025-07-30 01:29:31,549: t15.2023.08.20 val PER: 0.0548
2025-07-30 01:29:31,549: t15.2023.08.25 val PER: 0.0783
2025-07-30 01:29:31,550: t15.2023.08.27 val PER: 0.1592
2025-07-30 01:29:31,550: t15.2023.09.01 val PER: 0.0422
2025-07-30 01:29:31,550: t15.2023.09.03 val PER: 0.1200
2025-07-30 01:29:31,550: t15.2023.09.24 val PER: 0.0765
2025-07-30 01:29:31,550: t15.2023.09.29 val PER: 0.1110
2025-07-30 01:29:31,550: t15.2023.10.01 val PER: 0.1380
2025-07-30 01:29:31,550: t15.2023.10.06 val PER: 0.0732
2025-07-30 01:29:31,550: t15.2023.10.08 val PER: 0.1827
2025-07-30 01:29:31,551: t15.2023.10.13 val PER: 0.1528
2025-07-30 01:29:31,551: t15.2023.10.15 val PER: 0.1252
2025-07-30 01:29:31,551: t15.2023.10.20 val PER: 0.1745
2025-07-30 01:29:31,551: t15.2023.10.22 val PER: 0.1169
2025-07-30 01:29:31,551: t15.2023.11.03 val PER: 0.1560
2025-07-30 01:29:31,551: t15.2023.11.04 val PER: 0.0205
2025-07-30 01:29:31,551: t15.2023.11.17 val PER: 0.0311
2025-07-30 01:29:31,551: t15.2023.11.19 val PER: 0.0180
2025-07-30 01:29:31,552: t15.2023.11.26 val PER: 0.0486
2025-07-30 01:29:31,552: t15.2023.12.03 val PER: 0.0536
2025-07-30 01:29:31,552: t15.2023.12.08 val PER: 0.0519
2025-07-30 01:29:31,552: t15.2023.12.10 val PER: 0.0447
2025-07-30 01:29:31,552: t15.2023.12.17 val PER: 0.0977
2025-07-30 01:29:31,552: t15.2023.12.29 val PER: 0.0734
2025-07-30 01:29:31,552: t15.2024.02.25 val PER: 0.0801
2025-07-30 01:29:31,552: t15.2024.03.08 val PER: 0.1821
2025-07-30 01:29:31,553: t15.2024.03.15 val PER: 0.1745
2025-07-30 01:29:31,553: t15.2024.03.17 val PER: 0.0907
2025-07-30 01:29:31,553: t15.2024.05.10 val PER: 0.1248
2025-07-30 01:29:31,553: t15.2024.06.14 val PER: 0.1215
2025-07-30 01:29:31,553: t15.2024.07.19 val PER: 0.1615
2025-07-30 01:29:31,553: t15.2024.07.21 val PER: 0.0586
2025-07-30 01:29:31,553: t15.2024.07.28 val PER: 0.0904
2025-07-30 01:29:31,553: t15.2025.01.10 val PER: 0.2466
2025-07-30 01:29:31,554: t15.2025.01.12 val PER: 0.0970
2025-07-30 01:29:31,554: t15.2025.03.14 val PER: 0.3284
2025-07-30 01:29:31,554: t15.2025.03.16 val PER: 0.1754
2025-07-30 01:29:31,554: t15.2025.03.30 val PER: 0.2391
2025-07-30 01:29:31,554: t15.2025.04.13 val PER: 0.2211
2025-07-30 01:30:01,055: Train batch 42200: loss: 0.17 grad norm: 8.87 time: 0.098
2025-07-30 01:30:31,160: Train batch 42400: loss: 0.18 grad norm: 8.73 time: 0.096
2025-07-30 01:31:01,545: Train batch 42600: loss: 0.12 grad norm: 5.49 time: 0.092
2025-07-30 01:31:32,261: Train batch 42800: loss: 0.08 grad norm: 10.13 time: 0.125
2025-07-30 01:32:02,740: Train batch 43000: loss: 0.04 grad norm: 2.47 time: 0.087
2025-07-30 01:32:33,043: Train batch 43200: loss: 0.16 grad norm: 9.60 time: 0.100
2025-07-30 01:33:03,439: Train batch 43400: loss: 0.18 grad norm: 13.29 time: 0.126
2025-07-30 01:33:33,896: Train batch 43600: loss: 0.18 grad norm: 6.24 time: 0.079
2025-07-30 01:34:04,118: Train batch 43800: loss: 0.32 grad norm: 15.58 time: 0.088
2025-07-30 01:34:34,862: Train batch 44000: loss: 0.16 grad norm: 6.49 time: 0.087
2025-07-30 01:34:34,862: Running test after training batch: 44000
2025-07-30 01:34:47,885: Val batch 44000: PER (avg): 0.1109 CTC Loss (avg): 23.2601 time: 13.023
2025-07-30 01:34:47,885: t15.2023.08.13 val PER: 0.0842
2025-07-30 01:34:47,886: t15.2023.08.18 val PER: 0.0721
2025-07-30 01:34:47,886: t15.2023.08.20 val PER: 0.0548
2025-07-30 01:34:47,886: t15.2023.08.25 val PER: 0.0768
2025-07-30 01:34:47,886: t15.2023.08.27 val PER: 0.1640
2025-07-30 01:34:47,886: t15.2023.09.01 val PER: 0.0552
2025-07-30 01:34:47,886: t15.2023.09.03 val PER: 0.1188
2025-07-30 01:34:47,886: t15.2023.09.24 val PER: 0.0740
2025-07-30 01:34:47,886: t15.2023.09.29 val PER: 0.1066
2025-07-30 01:34:47,886: t15.2023.10.01 val PER: 0.1301
2025-07-30 01:34:47,887: t15.2023.10.06 val PER: 0.0657
2025-07-30 01:34:47,887: t15.2023.10.08 val PER: 0.1840
2025-07-30 01:34:47,887: t15.2023.10.13 val PER: 0.1590
2025-07-30 01:34:47,887: t15.2023.10.15 val PER: 0.1193
2025-07-30 01:34:47,887: t15.2023.10.20 val PER: 0.1678
2025-07-30 01:34:47,887: t15.2023.10.22 val PER: 0.1080
2025-07-30 01:34:47,887: t15.2023.11.03 val PER: 0.1526
2025-07-30 01:34:47,887: t15.2023.11.04 val PER: 0.0171
2025-07-30 01:34:47,887: t15.2023.11.17 val PER: 0.0249
2025-07-30 01:34:47,887: t15.2023.11.19 val PER: 0.0259
2025-07-30 01:34:47,888: t15.2023.11.26 val PER: 0.0558
2025-07-30 01:34:47,888: t15.2023.12.03 val PER: 0.0536
2025-07-30 01:34:47,888: t15.2023.12.08 val PER: 0.0393
2025-07-30 01:34:47,888: t15.2023.12.10 val PER: 0.0368
2025-07-30 01:34:47,888: t15.2023.12.17 val PER: 0.0915
2025-07-30 01:34:47,888: t15.2023.12.29 val PER: 0.0782
2025-07-30 01:34:47,888: t15.2024.02.25 val PER: 0.0801
2025-07-30 01:34:47,888: t15.2024.03.08 val PER: 0.2063
2025-07-30 01:34:47,888: t15.2024.03.15 val PER: 0.1776
2025-07-30 01:34:47,888: t15.2024.03.17 val PER: 0.0823
2025-07-30 01:34:47,889: t15.2024.05.10 val PER: 0.1233
2025-07-30 01:34:47,889: t15.2024.06.14 val PER: 0.1041
2025-07-30 01:34:47,889: t15.2024.07.19 val PER: 0.1628
2025-07-30 01:34:47,889: t15.2024.07.21 val PER: 0.0545
2025-07-30 01:34:47,889: t15.2024.07.28 val PER: 0.0941
2025-07-30 01:34:47,889: t15.2025.01.10 val PER: 0.2562
2025-07-30 01:34:47,889: t15.2025.01.12 val PER: 0.0947
2025-07-30 01:34:47,889: t15.2025.03.14 val PER: 0.3180
2025-07-30 01:34:47,889: t15.2025.03.16 val PER: 0.1767
2025-07-30 01:34:47,889: t15.2025.03.30 val PER: 0.2287
2025-07-30 01:34:47,890: t15.2025.04.13 val PER: 0.1983
2025-07-30 01:34:47,890: New best test PER 0.1122 --> 0.1109
2025-07-30 01:34:47,890: Checkpointing model
2025-07-30 01:34:49,126: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-07-30 01:35:19,179: Train batch 44200: loss: 0.23 grad norm: 13.50 time: 0.120
2025-07-30 01:35:49,143: Train batch 44400: loss: 0.04 grad norm: 2.92 time: 0.087
2025-07-30 01:36:19,167: Train batch 44600: loss: 0.42 grad norm: 19.50 time: 0.095
2025-07-30 01:36:49,487: Train batch 44800: loss: 0.05 grad norm: 2.93 time: 0.088
2025-07-30 01:37:19,626: Train batch 45000: loss: 0.05 grad norm: 3.59 time: 0.086
2025-07-30 01:37:50,383: Train batch 45200: loss: 0.17 grad norm: 9.42 time: 0.072
2025-07-30 01:38:20,912: Train batch 45400: loss: 0.14 grad norm: 8.55 time: 0.081
2025-07-30 01:38:50,842: Train batch 45600: loss: 0.05 grad norm: 3.49 time: 0.094
2025-07-30 01:39:21,729: Train batch 45800: loss: 0.34 grad norm: 14.79 time: 0.112
2025-07-30 01:39:52,028: Train batch 46000: loss: 0.07 grad norm: 8.19 time: 0.084
2025-07-30 01:39:52,028: Running test after training batch: 46000
2025-07-30 01:40:05,852: Val batch 46000: PER (avg): 0.1093 CTC Loss (avg): 23.0919 time: 13.824
2025-07-30 01:40:05,852: t15.2023.08.13 val PER: 0.0728
2025-07-30 01:40:05,852: t15.2023.08.18 val PER: 0.0855
2025-07-30 01:40:05,853: t15.2023.08.20 val PER: 0.0572
2025-07-30 01:40:05,853: t15.2023.08.25 val PER: 0.0843
2025-07-30 01:40:05,853: t15.2023.08.27 val PER: 0.1447
2025-07-30 01:40:05,853: t15.2023.09.01 val PER: 0.0471
2025-07-30 01:40:05,853: t15.2023.09.03 val PER: 0.1176
2025-07-30 01:40:05,853: t15.2023.09.24 val PER: 0.0777
2025-07-30 01:40:05,853: t15.2023.09.29 val PER: 0.1008
2025-07-30 01:40:05,853: t15.2023.10.01 val PER: 0.1288
2025-07-30 01:40:05,853: t15.2023.10.06 val PER: 0.0667
2025-07-30 01:40:05,854: t15.2023.10.08 val PER: 0.1935
2025-07-30 01:40:05,854: t15.2023.10.13 val PER: 0.1583
2025-07-30 01:40:05,854: t15.2023.10.15 val PER: 0.1173
2025-07-30 01:40:05,854: t15.2023.10.20 val PER: 0.1812
2025-07-30 01:40:05,854: t15.2023.10.22 val PER: 0.1002
2025-07-30 01:40:05,854: t15.2023.11.03 val PER: 0.1493
2025-07-30 01:40:05,854: t15.2023.11.04 val PER: 0.0171
2025-07-30 01:40:05,854: t15.2023.11.17 val PER: 0.0187
2025-07-30 01:40:05,854: t15.2023.11.19 val PER: 0.0120
2025-07-30 01:40:05,854: t15.2023.11.26 val PER: 0.0449
2025-07-30 01:40:05,855: t15.2023.12.03 val PER: 0.0588
2025-07-30 01:40:05,855: t15.2023.12.08 val PER: 0.0453
2025-07-30 01:40:05,855: t15.2023.12.10 val PER: 0.0434
2025-07-30 01:40:05,855: t15.2023.12.17 val PER: 0.0915
2025-07-30 01:40:05,855: t15.2023.12.29 val PER: 0.0837
2025-07-30 01:40:05,855: t15.2024.02.25 val PER: 0.0787
2025-07-30 01:40:05,855: t15.2024.03.08 val PER: 0.1906
2025-07-30 01:40:05,855: t15.2024.03.15 val PER: 0.1776
2025-07-30 01:40:05,856: t15.2024.03.17 val PER: 0.0718
2025-07-30 01:40:05,856: t15.2024.05.10 val PER: 0.1144
2025-07-30 01:40:05,856: t15.2024.06.14 val PER: 0.1136
2025-07-30 01:40:05,856: t15.2024.07.19 val PER: 0.1523
2025-07-30 01:40:05,856: t15.2024.07.21 val PER: 0.0572
2025-07-30 01:40:05,856: t15.2024.07.28 val PER: 0.0868
2025-07-30 01:40:05,856: t15.2025.01.10 val PER: 0.2410
2025-07-30 01:40:05,857: t15.2025.01.12 val PER: 0.0939
2025-07-30 01:40:05,857: t15.2025.03.14 val PER: 0.3343
2025-07-30 01:40:05,857: t15.2025.03.16 val PER: 0.1662
2025-07-30 01:40:05,857: t15.2025.03.30 val PER: 0.2322
2025-07-30 01:40:05,857: t15.2025.04.13 val PER: 0.2083
2025-07-30 01:40:05,857: New best test PER 0.1109 --> 0.1093
2025-07-30 01:40:05,857: Checkpointing model
2025-07-30 01:40:07,013: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-07-30 01:40:37,467: Train batch 46200: loss: 0.28 grad norm: 13.14 time: 0.092
2025-07-30 01:41:08,369: Train batch 46400: loss: 0.10 grad norm: 5.39 time: 0.088
2025-07-30 01:41:38,981: Train batch 46600: loss: 0.25 grad norm: 7.53 time: 0.117
2025-07-30 01:42:08,983: Train batch 46800: loss: 0.12 grad norm: 5.52 time: 0.105
2025-07-30 01:42:39,038: Train batch 47000: loss: 0.07 grad norm: 6.45 time: 0.092
2025-07-30 01:43:09,264: Train batch 47200: loss: 0.05 grad norm: 3.70 time: 0.121
2025-07-30 01:43:39,252: Train batch 47400: loss: 0.09 grad norm: 21.59 time: 0.098
2025-07-30 01:44:09,751: Train batch 47600: loss: 0.05 grad norm: 2.07 time: 0.096
2025-07-30 01:44:40,147: Train batch 47800: loss: 0.04 grad norm: 7.77 time: 0.112
2025-07-30 01:45:10,834: Train batch 48000: loss: 0.08 grad norm: 5.20 time: 0.088
2025-07-30 01:45:10,834: Running test after training batch: 48000
2025-07-30 01:45:24,082: Val batch 48000: PER (avg): 0.1091 CTC Loss (avg): 23.4126 time: 13.248
2025-07-30 01:45:24,083: t15.2023.08.13 val PER: 0.0728
2025-07-30 01:45:24,083: t15.2023.08.18 val PER: 0.0696
2025-07-30 01:45:24,083: t15.2023.08.20 val PER: 0.0556
2025-07-30 01:45:24,083: t15.2023.08.25 val PER: 0.0964
2025-07-30 01:45:24,083: t15.2023.08.27 val PER: 0.1543
2025-07-30 01:45:24,083: t15.2023.09.01 val PER: 0.0536
2025-07-30 01:45:24,083: t15.2023.09.03 val PER: 0.1176
2025-07-30 01:45:24,083: t15.2023.09.24 val PER: 0.0813
2025-07-30 01:45:24,084: t15.2023.09.29 val PER: 0.1098
2025-07-30 01:45:24,084: t15.2023.10.01 val PER: 0.1354
2025-07-30 01:45:24,084: t15.2023.10.06 val PER: 0.0657
2025-07-30 01:45:24,084: t15.2023.10.08 val PER: 0.1827
2025-07-30 01:45:24,084: t15.2023.10.13 val PER: 0.1528
2025-07-30 01:45:24,084: t15.2023.10.15 val PER: 0.1167
2025-07-30 01:45:24,084: t15.2023.10.20 val PER: 0.1544
2025-07-30 01:45:24,084: t15.2023.10.22 val PER: 0.1069
2025-07-30 01:45:24,084: t15.2023.11.03 val PER: 0.1499
2025-07-30 01:45:24,084: t15.2023.11.04 val PER: 0.0205
2025-07-30 01:45:24,085: t15.2023.11.17 val PER: 0.0280
2025-07-30 01:45:24,085: t15.2023.11.19 val PER: 0.0240
2025-07-30 01:45:24,085: t15.2023.11.26 val PER: 0.0514
2025-07-30 01:45:24,085: t15.2023.12.03 val PER: 0.0567
2025-07-30 01:45:24,085: t15.2023.12.08 val PER: 0.0366
2025-07-30 01:45:24,085: t15.2023.12.10 val PER: 0.0394
2025-07-30 01:45:24,085: t15.2023.12.17 val PER: 0.0977
2025-07-30 01:45:24,085: t15.2023.12.29 val PER: 0.0721
2025-07-30 01:45:24,085: t15.2024.02.25 val PER: 0.0801
2025-07-30 01:45:24,085: t15.2024.03.08 val PER: 0.1764
2025-07-30 01:45:24,086: t15.2024.03.15 val PER: 0.1789
2025-07-30 01:45:24,086: t15.2024.03.17 val PER: 0.0823
2025-07-30 01:45:24,086: t15.2024.05.10 val PER: 0.1159
2025-07-30 01:45:24,086: t15.2024.06.14 val PER: 0.1104
2025-07-30 01:45:24,086: t15.2024.07.19 val PER: 0.1556
2025-07-30 01:45:24,086: t15.2024.07.21 val PER: 0.0572
2025-07-30 01:45:24,086: t15.2024.07.28 val PER: 0.0890
2025-07-30 01:45:24,086: t15.2025.01.10 val PER: 0.2534
2025-07-30 01:45:24,086: t15.2025.01.12 val PER: 0.0978
2025-07-30 01:45:24,086: t15.2025.03.14 val PER: 0.3328
2025-07-30 01:45:24,087: t15.2025.03.16 val PER: 0.1584
2025-07-30 01:45:24,087: t15.2025.03.30 val PER: 0.2230
2025-07-30 01:45:24,087: t15.2025.04.13 val PER: 0.1740
2025-07-30 01:45:24,087: New best test PER 0.1093 --> 0.1091
2025-07-30 01:45:24,087: Checkpointing model
2025-07-30 01:45:25,242: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-07-30 01:45:55,073: Train batch 48200: loss: 0.12 grad norm: 7.88 time: 0.132
2025-07-30 01:46:25,613: Train batch 48400: loss: 0.01 grad norm: 0.48 time: 0.090
2025-07-30 01:46:55,686: Train batch 48600: loss: 0.12 grad norm: 15.69 time: 0.085
2025-07-30 01:47:26,059: Train batch 48800: loss: 0.06 grad norm: 2.62 time: 0.099
2025-07-30 01:47:56,349: Train batch 49000: loss: 0.03 grad norm: 2.16 time: 0.112
2025-07-30 01:48:26,382: Train batch 49200: loss: 0.06 grad norm: 4.50 time: 0.108
2025-07-30 01:48:56,323: Train batch 49400: loss: 0.04 grad norm: 6.34 time: 0.066
2025-07-30 01:49:26,440: Train batch 49600: loss: 0.08 grad norm: 3.18 time: 0.082
2025-07-30 01:49:57,321: Train batch 49800: loss: 0.16 grad norm: 7.32 time: 0.082
2025-07-30 01:50:27,233: Train batch 50000: loss: 0.06 grad norm: 2.90 time: 0.081
2025-07-30 01:50:27,233: Running test after training batch: 50000
2025-07-30 01:50:40,557: Val batch 50000: PER (avg): 0.1068 CTC Loss (avg): 23.0795 time: 13.324
2025-07-30 01:50:40,557: t15.2023.08.13 val PER: 0.0780
2025-07-30 01:50:40,557: t15.2023.08.18 val PER: 0.0654
2025-07-30 01:50:40,558: t15.2023.08.20 val PER: 0.0564
2025-07-30 01:50:40,558: t15.2023.08.25 val PER: 0.0873
2025-07-30 01:50:40,558: t15.2023.08.27 val PER: 0.1704
2025-07-30 01:50:40,558: t15.2023.09.01 val PER: 0.0511
2025-07-30 01:50:40,558: t15.2023.09.03 val PER: 0.1081
2025-07-30 01:50:40,558: t15.2023.09.24 val PER: 0.0704
2025-07-30 01:50:40,558: t15.2023.09.29 val PER: 0.1015
2025-07-30 01:50:40,558: t15.2023.10.01 val PER: 0.1341
2025-07-30 01:50:40,558: t15.2023.10.06 val PER: 0.0667
2025-07-30 01:50:40,559: t15.2023.10.08 val PER: 0.1867
2025-07-30 01:50:40,559: t15.2023.10.13 val PER: 0.1474
2025-07-30 01:50:40,559: t15.2023.10.15 val PER: 0.1154
2025-07-30 01:50:40,559: t15.2023.10.20 val PER: 0.1678
2025-07-30 01:50:40,559: t15.2023.10.22 val PER: 0.0980
2025-07-30 01:50:40,559: t15.2023.11.03 val PER: 0.1465
2025-07-30 01:50:40,559: t15.2023.11.04 val PER: 0.0205
2025-07-30 01:50:40,559: t15.2023.11.17 val PER: 0.0264
2025-07-30 01:50:40,560: t15.2023.11.19 val PER: 0.0140
2025-07-30 01:50:40,560: t15.2023.11.26 val PER: 0.0449
2025-07-30 01:50:40,560: t15.2023.12.03 val PER: 0.0494
2025-07-30 01:50:40,560: t15.2023.12.08 val PER: 0.0406
2025-07-30 01:50:40,560: t15.2023.12.10 val PER: 0.0329
2025-07-30 01:50:40,560: t15.2023.12.17 val PER: 0.0884
2025-07-30 01:50:40,560: t15.2023.12.29 val PER: 0.0728
2025-07-30 01:50:40,560: t15.2024.02.25 val PER: 0.0758
2025-07-30 01:50:40,560: t15.2024.03.08 val PER: 0.1593
2025-07-30 01:50:40,561: t15.2024.03.15 val PER: 0.1776
2025-07-30 01:50:40,561: t15.2024.03.17 val PER: 0.0795
2025-07-30 01:50:40,561: t15.2024.05.10 val PER: 0.1114
2025-07-30 01:50:40,561: t15.2024.06.14 val PER: 0.1041
2025-07-30 01:50:40,561: t15.2024.07.19 val PER: 0.1569
2025-07-30 01:50:40,562: t15.2024.07.21 val PER: 0.0538
2025-07-30 01:50:40,562: t15.2024.07.28 val PER: 0.0875
2025-07-30 01:50:40,562: t15.2025.01.10 val PER: 0.2507
2025-07-30 01:50:40,562: t15.2025.01.12 val PER: 0.1024
2025-07-30 01:50:40,562: t15.2025.03.14 val PER: 0.3373
2025-07-30 01:50:40,562: t15.2025.03.16 val PER: 0.1558
2025-07-30 01:50:40,563: t15.2025.03.30 val PER: 0.2287
2025-07-30 01:50:40,563: t15.2025.04.13 val PER: 0.1769
2025-07-30 01:50:40,563: New best test PER 0.1091 --> 0.1068
2025-07-30 01:50:40,563: Checkpointing model
2025-07-30 01:50:41,712: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-07-30 01:51:11,731: Train batch 50200: loss: 0.11 grad norm: 8.12 time: 0.126
2025-07-30 01:51:41,678: Train batch 50400: loss: 0.07 grad norm: 3.76 time: 0.108
2025-07-30 01:52:12,228: Train batch 50600: loss: 0.06 grad norm: 2.01 time: 0.126
2025-07-30 01:52:42,260: Train batch 50800: loss: 0.06 grad norm: 2.62 time: 0.113
2025-07-30 01:53:12,550: Train batch 51000: loss: 0.06 grad norm: 2.91 time: 0.116
2025-07-30 01:53:43,018: Train batch 51200: loss: 0.19 grad norm: 26.34 time: 0.120
2025-07-30 01:54:13,430: Train batch 51400: loss: 0.06 grad norm: 4.58 time: 0.110
2025-07-30 01:54:43,402: Train batch 51600: loss: 0.06 grad norm: 2.96 time: 0.091
2025-07-30 01:55:13,568: Train batch 51800: loss: 0.13 grad norm: 4.48 time: 0.078
2025-07-30 01:55:44,149: Train batch 52000: loss: 0.17 grad norm: 11.68 time: 0.088
2025-07-30 01:55:44,150: Running test after training batch: 52000
2025-07-30 01:55:57,436: Val batch 52000: PER (avg): 0.1071 CTC Loss (avg): 23.3739 time: 13.286
2025-07-30 01:55:57,437: t15.2023.08.13 val PER: 0.0665
2025-07-30 01:55:57,437: t15.2023.08.18 val PER: 0.0704
2025-07-30 01:55:57,437: t15.2023.08.20 val PER: 0.0492
2025-07-30 01:55:57,437: t15.2023.08.25 val PER: 0.0904
2025-07-30 01:55:57,437: t15.2023.08.27 val PER: 0.1511
2025-07-30 01:55:57,437: t15.2023.09.01 val PER: 0.0503
2025-07-30 01:55:57,437: t15.2023.09.03 val PER: 0.1128
2025-07-30 01:55:57,437: t15.2023.09.24 val PER: 0.0752
2025-07-30 01:55:57,438: t15.2023.09.29 val PER: 0.1015
2025-07-30 01:55:57,438: t15.2023.10.01 val PER: 0.1453
2025-07-30 01:55:57,438: t15.2023.10.06 val PER: 0.0700
2025-07-30 01:55:57,438: t15.2023.10.08 val PER: 0.1746
2025-07-30 01:55:57,438: t15.2023.10.13 val PER: 0.1396
2025-07-30 01:55:57,438: t15.2023.10.15 val PER: 0.1193
2025-07-30 01:55:57,438: t15.2023.10.20 val PER: 0.1745
2025-07-30 01:55:57,438: t15.2023.10.22 val PER: 0.1102
2025-07-30 01:55:57,439: t15.2023.11.03 val PER: 0.1547
2025-07-30 01:55:57,439: t15.2023.11.04 val PER: 0.0137
2025-07-30 01:55:57,439: t15.2023.11.17 val PER: 0.0249
2025-07-30 01:55:57,439: t15.2023.11.19 val PER: 0.0180
2025-07-30 01:55:57,439: t15.2023.11.26 val PER: 0.0478
2025-07-30 01:55:57,439: t15.2023.12.03 val PER: 0.0452
2025-07-30 01:55:57,439: t15.2023.12.08 val PER: 0.0426
2025-07-30 01:55:57,439: t15.2023.12.10 val PER: 0.0263
2025-07-30 01:55:57,440: t15.2023.12.17 val PER: 0.0852
2025-07-30 01:55:57,440: t15.2023.12.29 val PER: 0.0734
2025-07-30 01:55:57,440: t15.2024.02.25 val PER: 0.0801
2025-07-30 01:55:57,440: t15.2024.03.08 val PER: 0.1835
2025-07-30 01:55:57,440: t15.2024.03.15 val PER: 0.1707
2025-07-30 01:55:57,440: t15.2024.03.17 val PER: 0.0802
2025-07-30 01:55:57,440: t15.2024.05.10 val PER: 0.1189
2025-07-30 01:55:57,440: t15.2024.06.14 val PER: 0.1057
2025-07-30 01:55:57,440: t15.2024.07.19 val PER: 0.1529
2025-07-30 01:55:57,441: t15.2024.07.21 val PER: 0.0545
2025-07-30 01:55:57,441: t15.2024.07.28 val PER: 0.0897
2025-07-30 01:55:57,441: t15.2025.01.10 val PER: 0.2466
2025-07-30 01:55:57,441: t15.2025.01.12 val PER: 0.0993
2025-07-30 01:55:57,441: t15.2025.03.14 val PER: 0.3388
2025-07-30 01:55:57,441: t15.2025.03.16 val PER: 0.1584
2025-07-30 01:55:57,441: t15.2025.03.30 val PER: 0.2184
2025-07-30 01:55:57,441: t15.2025.04.13 val PER: 0.1797
2025-07-30 01:56:26,952: Train batch 52200: loss: 0.04 grad norm: 1.75 time: 0.102
2025-07-30 01:56:57,586: Train batch 52400: loss: 0.04 grad norm: 2.94 time: 0.085
2025-07-30 01:57:27,556: Train batch 52600: loss: 0.03 grad norm: 2.04 time: 0.087
2025-07-30 01:57:57,975: Train batch 52800: loss: 0.14 grad norm: 6.24 time: 0.091
2025-07-30 01:58:28,067: Train batch 53000: loss: 0.02 grad norm: 2.42 time: 0.117
2025-07-30 01:58:58,574: Train batch 53200: loss: 0.09 grad norm: 3.09 time: 0.090
2025-07-30 01:59:28,482: Train batch 53400: loss: 0.14 grad norm: 3.25 time: 0.081
2025-07-30 01:59:59,060: Train batch 53600: loss: 0.10 grad norm: 6.45 time: 0.112
2025-07-30 02:00:29,150: Train batch 53800: loss: 0.03 grad norm: 1.66 time: 0.109
2025-07-30 02:00:59,679: Train batch 54000: loss: 0.09 grad norm: 8.76 time: 0.086
2025-07-30 02:00:59,679: Running test after training batch: 54000
2025-07-30 02:01:13,322: Val batch 54000: PER (avg): 0.1062 CTC Loss (avg): 23.2511 time: 13.642
2025-07-30 02:01:13,322: t15.2023.08.13 val PER: 0.0728
2025-07-30 02:01:13,322: t15.2023.08.18 val PER: 0.0671
2025-07-30 02:01:13,322: t15.2023.08.20 val PER: 0.0604
2025-07-30 02:01:13,322: t15.2023.08.25 val PER: 0.0843
2025-07-30 02:01:13,323: t15.2023.08.27 val PER: 0.1688
2025-07-30 02:01:13,323: t15.2023.09.01 val PER: 0.0487
2025-07-30 02:01:13,323: t15.2023.09.03 val PER: 0.1105
2025-07-30 02:01:13,323: t15.2023.09.24 val PER: 0.0716
2025-07-30 02:01:13,323: t15.2023.09.29 val PER: 0.0983
2025-07-30 02:01:13,323: t15.2023.10.01 val PER: 0.1288
2025-07-30 02:01:13,323: t15.2023.10.06 val PER: 0.0624
2025-07-30 02:01:13,323: t15.2023.10.08 val PER: 0.1759
2025-07-30 02:01:13,323: t15.2023.10.13 val PER: 0.1474
2025-07-30 02:01:13,323: t15.2023.10.15 val PER: 0.1246
2025-07-30 02:01:13,324: t15.2023.10.20 val PER: 0.1946
2025-07-30 02:01:13,324: t15.2023.10.22 val PER: 0.1024
2025-07-30 02:01:13,324: t15.2023.11.03 val PER: 0.1418
2025-07-30 02:01:13,324: t15.2023.11.04 val PER: 0.0137
2025-07-30 02:01:13,324: t15.2023.11.17 val PER: 0.0295
2025-07-30 02:01:13,324: t15.2023.11.19 val PER: 0.0160
2025-07-30 02:01:13,324: t15.2023.11.26 val PER: 0.0514
2025-07-30 02:01:13,325: t15.2023.12.03 val PER: 0.0557
2025-07-30 02:01:13,325: t15.2023.12.08 val PER: 0.0366
2025-07-30 02:01:13,325: t15.2023.12.10 val PER: 0.0302
2025-07-30 02:01:13,325: t15.2023.12.17 val PER: 0.0790
2025-07-30 02:01:13,325: t15.2023.12.29 val PER: 0.0769
2025-07-30 02:01:13,325: t15.2024.02.25 val PER: 0.0702
2025-07-30 02:01:13,325: t15.2024.03.08 val PER: 0.1750
2025-07-30 02:01:13,325: t15.2024.03.15 val PER: 0.1682
2025-07-30 02:01:13,326: t15.2024.03.17 val PER: 0.0739
2025-07-30 02:01:13,326: t15.2024.05.10 val PER: 0.1293
2025-07-30 02:01:13,326: t15.2024.06.14 val PER: 0.1120
2025-07-30 02:01:13,326: t15.2024.07.19 val PER: 0.1595
2025-07-30 02:01:13,326: t15.2024.07.21 val PER: 0.0600
2025-07-30 02:01:13,326: t15.2024.07.28 val PER: 0.0897
2025-07-30 02:01:13,326: t15.2025.01.10 val PER: 0.2576
2025-07-30 02:01:13,326: t15.2025.01.12 val PER: 0.0801
2025-07-30 02:01:13,326: t15.2025.03.14 val PER: 0.3180
2025-07-30 02:01:13,327: t15.2025.03.16 val PER: 0.1505
2025-07-30 02:01:13,327: t15.2025.03.30 val PER: 0.2322
2025-07-30 02:01:13,327: t15.2025.04.13 val PER: 0.1740
2025-07-30 02:01:13,327: New best test PER 0.1068 --> 0.1062
2025-07-30 02:01:13,327: Checkpointing model
2025-07-30 02:01:14,504: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-07-30 02:01:44,452: Train batch 54200: loss: 0.10 grad norm: 5.53 time: 0.091
2025-07-30 02:02:15,062: Train batch 54400: loss: 0.04 grad norm: 2.02 time: 0.131
2025-07-30 02:02:45,058: Train batch 54600: loss: 0.02 grad norm: 0.93 time: 0.076
2025-07-30 02:03:15,503: Train batch 54800: loss: 0.04 grad norm: 2.72 time: 0.068
2025-07-30 02:03:45,583: Train batch 55000: loss: 0.21 grad norm: 3.14 time: 0.083
2025-07-30 02:04:16,101: Train batch 55200: loss: 0.11 grad norm: 5.38 time: 0.087
2025-07-30 02:04:45,773: Train batch 55400: loss: 0.02 grad norm: 1.98 time: 0.101
2025-07-30 02:05:15,732: Train batch 55600: loss: 0.01 grad norm: 0.44 time: 0.100
2025-07-30 02:05:46,062: Train batch 55800: loss: 0.04 grad norm: 3.97 time: 0.075
2025-07-30 02:06:16,395: Train batch 56000: loss: 0.06 grad norm: 11.88 time: 0.127
2025-07-30 02:06:16,396: Running test after training batch: 56000
2025-07-30 02:06:30,331: Val batch 56000: PER (avg): 0.1065 CTC Loss (avg): 23.1918 time: 13.935
2025-07-30 02:06:30,331: t15.2023.08.13 val PER: 0.0738
2025-07-30 02:06:30,331: t15.2023.08.18 val PER: 0.0704
2025-07-30 02:06:30,331: t15.2023.08.20 val PER: 0.0548
2025-07-30 02:06:30,331: t15.2023.08.25 val PER: 0.0889
2025-07-30 02:06:30,332: t15.2023.08.27 val PER: 0.1624
2025-07-30 02:06:30,332: t15.2023.09.01 val PER: 0.0438
2025-07-30 02:06:30,332: t15.2023.09.03 val PER: 0.1057
2025-07-30 02:06:30,332: t15.2023.09.24 val PER: 0.0813
2025-07-30 02:06:30,332: t15.2023.09.29 val PER: 0.0970
2025-07-30 02:06:30,332: t15.2023.10.01 val PER: 0.1341
2025-07-30 02:06:30,332: t15.2023.10.06 val PER: 0.0624
2025-07-30 02:06:30,332: t15.2023.10.08 val PER: 0.1691
2025-07-30 02:06:30,332: t15.2023.10.13 val PER: 0.1427
2025-07-30 02:06:30,333: t15.2023.10.15 val PER: 0.1187
2025-07-30 02:06:30,333: t15.2023.10.20 val PER: 0.1644
2025-07-30 02:06:30,333: t15.2023.10.22 val PER: 0.1024
2025-07-30 02:06:30,333: t15.2023.11.03 val PER: 0.1493
2025-07-30 02:06:30,333: t15.2023.11.04 val PER: 0.0205
2025-07-30 02:06:30,333: t15.2023.11.17 val PER: 0.0311
2025-07-30 02:06:30,333: t15.2023.11.19 val PER: 0.0140
2025-07-30 02:06:30,333: t15.2023.11.26 val PER: 0.0507
2025-07-30 02:06:30,334: t15.2023.12.03 val PER: 0.0462
2025-07-30 02:06:30,334: t15.2023.12.08 val PER: 0.0426
2025-07-30 02:06:30,334: t15.2023.12.10 val PER: 0.0276
2025-07-30 02:06:30,334: t15.2023.12.17 val PER: 0.0790
2025-07-30 02:06:30,334: t15.2023.12.29 val PER: 0.0762
2025-07-30 02:06:30,334: t15.2024.02.25 val PER: 0.0772
2025-07-30 02:06:30,334: t15.2024.03.08 val PER: 0.1807
2025-07-30 02:06:30,334: t15.2024.03.15 val PER: 0.1670
2025-07-30 02:06:30,335: t15.2024.03.17 val PER: 0.0816
2025-07-30 02:06:30,335: t15.2024.05.10 val PER: 0.1218
2025-07-30 02:06:30,335: t15.2024.06.14 val PER: 0.1167
2025-07-30 02:06:30,335: t15.2024.07.19 val PER: 0.1582
2025-07-30 02:06:30,335: t15.2024.07.21 val PER: 0.0621
2025-07-30 02:06:30,335: t15.2024.07.28 val PER: 0.0897
2025-07-30 02:06:30,335: t15.2025.01.10 val PER: 0.2397
2025-07-30 02:06:30,335: t15.2025.01.12 val PER: 0.0947
2025-07-30 02:06:30,336: t15.2025.03.14 val PER: 0.3269
2025-07-30 02:06:30,336: t15.2025.03.16 val PER: 0.1649
2025-07-30 02:06:30,336: t15.2025.03.30 val PER: 0.2207
2025-07-30 02:06:30,336: t15.2025.04.13 val PER: 0.1740
2025-07-30 02:07:00,559: Train batch 56200: loss: 0.05 grad norm: 2.89 time: 0.088
2025-07-30 02:07:31,191: Train batch 56400: loss: 0.01 grad norm: 0.65 time: 0.094
2025-07-30 02:08:01,551: Train batch 56600: loss: 0.06 grad norm: 3.35 time: 0.079
2025-07-30 02:08:31,644: Train batch 56800: loss: 0.11 grad norm: 5.60 time: 0.096
2025-07-30 02:09:01,945: Train batch 57000: loss: 0.02 grad norm: 1.32 time: 0.093
2025-07-30 02:09:32,233: Train batch 57200: loss: 0.20 grad norm: 22.55 time: 0.133
2025-07-30 02:10:02,919: Train batch 57400: loss: 0.01 grad norm: 0.60 time: 0.097
2025-07-30 02:10:33,371: Train batch 57600: loss: 0.05 grad norm: 2.68 time: 0.129
2025-07-30 02:11:03,809: Train batch 57800: loss: 0.04 grad norm: 3.40 time: 0.085
2025-07-30 02:11:34,046: Train batch 58000: loss: 0.13 grad norm: 6.56 time: 0.077
2025-07-30 02:11:34,047: Running test after training batch: 58000
2025-07-30 02:11:48,342: Val batch 58000: PER (avg): 0.1074 CTC Loss (avg): 23.4023 time: 14.295
2025-07-30 02:11:48,343: t15.2023.08.13 val PER: 0.0884
2025-07-30 02:11:48,343: t15.2023.08.18 val PER: 0.0654
2025-07-30 02:11:48,343: t15.2023.08.20 val PER: 0.0588
2025-07-30 02:11:48,343: t15.2023.08.25 val PER: 0.0858
2025-07-30 02:11:48,343: t15.2023.08.27 val PER: 0.1576
2025-07-30 02:11:48,344: t15.2023.09.01 val PER: 0.0471
2025-07-30 02:11:48,344: t15.2023.09.03 val PER: 0.1128
2025-07-30 02:11:48,344: t15.2023.09.24 val PER: 0.0716
2025-07-30 02:11:48,344: t15.2023.09.29 val PER: 0.0989
2025-07-30 02:11:48,344: t15.2023.10.01 val PER: 0.1334
2025-07-30 02:11:48,344: t15.2023.10.06 val PER: 0.0571
2025-07-30 02:11:48,344: t15.2023.10.08 val PER: 0.1786
2025-07-30 02:11:48,345: t15.2023.10.13 val PER: 0.1420
2025-07-30 02:11:48,345: t15.2023.10.15 val PER: 0.1154
2025-07-30 02:11:48,345: t15.2023.10.20 val PER: 0.1644
2025-07-30 02:11:48,345: t15.2023.10.22 val PER: 0.1080
2025-07-30 02:11:48,345: t15.2023.11.03 val PER: 0.1479
2025-07-30 02:11:48,345: t15.2023.11.04 val PER: 0.0102
2025-07-30 02:11:48,346: t15.2023.11.17 val PER: 0.0249
2025-07-30 02:11:48,346: t15.2023.11.19 val PER: 0.0140
2025-07-30 02:11:48,346: t15.2023.11.26 val PER: 0.0529
2025-07-30 02:11:48,346: t15.2023.12.03 val PER: 0.0578
2025-07-30 02:11:48,346: t15.2023.12.08 val PER: 0.0326
2025-07-30 02:11:48,346: t15.2023.12.10 val PER: 0.0355
2025-07-30 02:11:48,346: t15.2023.12.17 val PER: 0.0852
2025-07-30 02:11:48,346: t15.2023.12.29 val PER: 0.0762
2025-07-30 02:11:48,347: t15.2024.02.25 val PER: 0.0744
2025-07-30 02:11:48,347: t15.2024.03.08 val PER: 0.1849
2025-07-30 02:11:48,347: t15.2024.03.15 val PER: 0.1726
2025-07-30 02:11:48,347: t15.2024.03.17 val PER: 0.0774
2025-07-30 02:11:48,347: t15.2024.05.10 val PER: 0.1278
2025-07-30 02:11:48,347: t15.2024.06.14 val PER: 0.1025
2025-07-30 02:11:48,347: t15.2024.07.19 val PER: 0.1503
2025-07-30 02:11:48,347: t15.2024.07.21 val PER: 0.0662
2025-07-30 02:11:48,347: t15.2024.07.28 val PER: 0.0882
2025-07-30 02:11:48,347: t15.2025.01.10 val PER: 0.2507
2025-07-30 02:11:48,347: t15.2025.01.12 val PER: 0.1001
2025-07-30 02:11:48,348: t15.2025.03.14 val PER: 0.3343
2025-07-30 02:11:48,348: t15.2025.03.16 val PER: 0.1662
2025-07-30 02:11:48,348: t15.2025.03.30 val PER: 0.2230
2025-07-30 02:11:48,348: t15.2025.04.13 val PER: 0.1840
2025-07-30 02:12:18,244: Train batch 58200: loss: 0.02 grad norm: 0.98 time: 0.082
2025-07-30 02:12:48,390: Train batch 58400: loss: 0.04 grad norm: 2.58 time: 0.097
2025-07-30 02:13:19,050: Train batch 58600: loss: 0.04 grad norm: 2.85 time: 0.079
2025-07-30 02:13:49,663: Train batch 58800: loss: 0.19 grad norm: 11.29 time: 0.094
2025-07-30 02:14:19,798: Train batch 59000: loss: 0.07 grad norm: 4.23 time: 0.086
2025-07-30 02:14:50,076: Train batch 59200: loss: 0.04 grad norm: 2.49 time: 0.082
2025-07-30 02:15:19,968: Train batch 59400: loss: 0.03 grad norm: 2.21 time: 0.109
2025-07-30 02:15:50,094: Train batch 59600: loss: 0.02 grad norm: 3.83 time: 0.110
2025-07-30 02:16:20,456: Train batch 59800: loss: 0.04 grad norm: 2.79 time: 0.086
2025-07-30 02:16:51,265: Train batch 60000: loss: 0.04 grad norm: 2.84 time: 0.116
2025-07-30 02:16:51,265: Running test after training batch: 60000
2025-07-30 02:17:04,289: Val batch 60000: PER (avg): 0.1041 CTC Loss (avg): 22.5784 time: 13.023
2025-07-30 02:17:04,289: t15.2023.08.13 val PER: 0.0790
2025-07-30 02:17:04,289: t15.2023.08.18 val PER: 0.0645
2025-07-30 02:17:04,290: t15.2023.08.20 val PER: 0.0500
2025-07-30 02:17:04,290: t15.2023.08.25 val PER: 0.0813
2025-07-30 02:17:04,290: t15.2023.08.27 val PER: 0.1608
2025-07-30 02:17:04,290: t15.2023.09.01 val PER: 0.0446
2025-07-30 02:17:04,290: t15.2023.09.03 val PER: 0.1093
2025-07-30 02:17:04,290: t15.2023.09.24 val PER: 0.0765
2025-07-30 02:17:04,290: t15.2023.09.29 val PER: 0.0970
2025-07-30 02:17:04,290: t15.2023.10.01 val PER: 0.1262
2025-07-30 02:17:04,290: t15.2023.10.06 val PER: 0.0646
2025-07-30 02:17:04,291: t15.2023.10.08 val PER: 0.1637
2025-07-30 02:17:04,291: t15.2023.10.13 val PER: 0.1373
2025-07-30 02:17:04,291: t15.2023.10.15 val PER: 0.1154
2025-07-30 02:17:04,291: t15.2023.10.20 val PER: 0.1678
2025-07-30 02:17:04,291: t15.2023.10.22 val PER: 0.0924
2025-07-30 02:17:04,291: t15.2023.11.03 val PER: 0.1526
2025-07-30 02:17:04,291: t15.2023.11.04 val PER: 0.0205
2025-07-30 02:17:04,291: t15.2023.11.17 val PER: 0.0280
2025-07-30 02:17:04,292: t15.2023.11.19 val PER: 0.0200
2025-07-30 02:17:04,292: t15.2023.11.26 val PER: 0.0471
2025-07-30 02:17:04,292: t15.2023.12.03 val PER: 0.0620
2025-07-30 02:17:04,292: t15.2023.12.08 val PER: 0.0393
2025-07-30 02:17:04,292: t15.2023.12.10 val PER: 0.0329
2025-07-30 02:17:04,292: t15.2023.12.17 val PER: 0.0821
2025-07-30 02:17:04,292: t15.2023.12.29 val PER: 0.0776
2025-07-30 02:17:04,292: t15.2024.02.25 val PER: 0.0772
2025-07-30 02:17:04,292: t15.2024.03.08 val PER: 0.1807
2025-07-30 02:17:04,293: t15.2024.03.15 val PER: 0.1582
2025-07-30 02:17:04,293: t15.2024.03.17 val PER: 0.0732
2025-07-30 02:17:04,293: t15.2024.05.10 val PER: 0.1159
2025-07-30 02:17:04,293: t15.2024.06.14 val PER: 0.1088
2025-07-30 02:17:04,293: t15.2024.07.19 val PER: 0.1463
2025-07-30 02:17:04,293: t15.2024.07.21 val PER: 0.0579
2025-07-30 02:17:04,293: t15.2024.07.28 val PER: 0.0912
2025-07-30 02:17:04,293: t15.2025.01.10 val PER: 0.2466
2025-07-30 02:17:04,294: t15.2025.01.12 val PER: 0.0924
2025-07-30 02:17:04,294: t15.2025.03.14 val PER: 0.3121
2025-07-30 02:17:04,294: t15.2025.03.16 val PER: 0.1545
2025-07-30 02:17:04,294: t15.2025.03.30 val PER: 0.2023
2025-07-30 02:17:04,294: t15.2025.04.13 val PER: 0.1969
2025-07-30 02:17:04,294: New best test PER 0.1062 --> 0.1041
2025-07-30 02:17:04,294: Checkpointing model
2025-07-30 02:17:05,452: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-07-30 02:17:35,418: Train batch 60200: loss: 0.04 grad norm: 1.66 time: 0.102
2025-07-30 02:18:05,326: Train batch 60400: loss: 0.06 grad norm: 3.36 time: 0.076
2025-07-30 02:18:35,611: Train batch 60600: loss: 0.01 grad norm: 0.77 time: 0.091
2025-07-30 02:19:05,965: Train batch 60800: loss: 0.26 grad norm: 5.85 time: 0.093
2025-07-30 02:19:36,778: Train batch 61000: loss: 0.01 grad norm: 0.69 time: 0.117
2025-07-30 02:20:06,374: Train batch 61200: loss: 0.02 grad norm: 1.52 time: 0.083
2025-07-30 02:20:36,855: Train batch 61400: loss: 0.02 grad norm: 0.88 time: 0.101
2025-07-30 02:21:07,183: Train batch 61600: loss: 0.02 grad norm: 1.10 time: 0.078
2025-07-30 02:21:37,533: Train batch 61800: loss: 0.03 grad norm: 1.69 time: 0.100
2025-07-30 02:22:07,842: Train batch 62000: loss: 0.12 grad norm: 8.76 time: 0.099
2025-07-30 02:22:07,842: Running test after training batch: 62000
2025-07-30 02:22:20,928: Val batch 62000: PER (avg): 0.1051 CTC Loss (avg): 22.7175 time: 13.086
2025-07-30 02:22:20,929: t15.2023.08.13 val PER: 0.0748
2025-07-30 02:22:20,929: t15.2023.08.18 val PER: 0.0629
2025-07-30 02:22:20,929: t15.2023.08.20 val PER: 0.0492
2025-07-30 02:22:20,929: t15.2023.08.25 val PER: 0.0873
2025-07-30 02:22:20,929: t15.2023.08.27 val PER: 0.1463
2025-07-30 02:22:20,929: t15.2023.09.01 val PER: 0.0406
2025-07-30 02:22:20,929: t15.2023.09.03 val PER: 0.1045
2025-07-30 02:22:20,929: t15.2023.09.24 val PER: 0.0813
2025-07-30 02:22:20,930: t15.2023.09.29 val PER: 0.0996
2025-07-30 02:22:20,930: t15.2023.10.01 val PER: 0.1295
2025-07-30 02:22:20,930: t15.2023.10.06 val PER: 0.0624
2025-07-30 02:22:20,930: t15.2023.10.08 val PER: 0.1678
2025-07-30 02:22:20,930: t15.2023.10.13 val PER: 0.1396
2025-07-30 02:22:20,930: t15.2023.10.15 val PER: 0.1134
2025-07-30 02:22:20,930: t15.2023.10.20 val PER: 0.1644
2025-07-30 02:22:20,930: t15.2023.10.22 val PER: 0.1036
2025-07-30 02:22:20,930: t15.2023.11.03 val PER: 0.1547
2025-07-30 02:22:20,930: t15.2023.11.04 val PER: 0.0137
2025-07-30 02:22:20,931: t15.2023.11.17 val PER: 0.0264
2025-07-30 02:22:20,931: t15.2023.11.19 val PER: 0.0140
2025-07-30 02:22:20,931: t15.2023.11.26 val PER: 0.0529
2025-07-30 02:22:20,931: t15.2023.12.03 val PER: 0.0599
2025-07-30 02:22:20,931: t15.2023.12.08 val PER: 0.0426
2025-07-30 02:22:20,931: t15.2023.12.10 val PER: 0.0381
2025-07-30 02:22:20,931: t15.2023.12.17 val PER: 0.0884
2025-07-30 02:22:20,931: t15.2023.12.29 val PER: 0.0686
2025-07-30 02:22:20,931: t15.2024.02.25 val PER: 0.0674
2025-07-30 02:22:20,932: t15.2024.03.08 val PER: 0.1807
2025-07-30 02:22:20,932: t15.2024.03.15 val PER: 0.1670
2025-07-30 02:22:20,932: t15.2024.03.17 val PER: 0.0739
2025-07-30 02:22:20,932: t15.2024.05.10 val PER: 0.1174
2025-07-30 02:22:20,932: t15.2024.06.14 val PER: 0.1025
2025-07-30 02:22:20,932: t15.2024.07.19 val PER: 0.1543
2025-07-30 02:22:20,932: t15.2024.07.21 val PER: 0.0545
2025-07-30 02:22:20,932: t15.2024.07.28 val PER: 0.0941
2025-07-30 02:22:20,932: t15.2025.01.10 val PER: 0.2479
2025-07-30 02:22:20,933: t15.2025.01.12 val PER: 0.0955
2025-07-30 02:22:20,933: t15.2025.03.14 val PER: 0.3166
2025-07-30 02:22:20,933: t15.2025.03.16 val PER: 0.1531
2025-07-30 02:22:20,933: t15.2025.03.30 val PER: 0.2184
2025-07-30 02:22:20,933: t15.2025.04.13 val PER: 0.1940
2025-07-30 02:22:51,085: Train batch 62200: loss: 0.03 grad norm: 2.37 time: 0.085
2025-07-30 02:23:21,698: Train batch 62400: loss: 0.03 grad norm: 1.29 time: 0.078
2025-07-30 02:23:52,390: Train batch 62600: loss: 0.04 grad norm: 3.54 time: 0.077
2025-07-30 02:24:23,280: Train batch 62800: loss: 0.05 grad norm: 2.37 time: 0.100
2025-07-30 02:24:54,303: Train batch 63000: loss: 0.02 grad norm: 2.61 time: 0.089
2025-07-30 02:25:24,930: Train batch 63200: loss: 0.10 grad norm: 4.99 time: 0.094
2025-07-30 02:25:55,296: Train batch 63400: loss: 0.02 grad norm: 1.97 time: 0.089
2025-07-30 02:26:26,010: Train batch 63600: loss: 0.18 grad norm: 6.68 time: 0.093
2025-07-30 02:26:56,778: Train batch 63800: loss: 0.24 grad norm: 3.99 time: 0.095
2025-07-30 02:27:27,390: Train batch 64000: loss: 0.01 grad norm: 0.66 time: 0.086
2025-07-30 02:27:27,390: Running test after training batch: 64000
2025-07-30 02:27:40,564: Val batch 64000: PER (avg): 0.1057 CTC Loss (avg): 23.2813 time: 13.173
2025-07-30 02:27:40,564: t15.2023.08.13 val PER: 0.0707
2025-07-30 02:27:40,564: t15.2023.08.18 val PER: 0.0671
2025-07-30 02:27:40,564: t15.2023.08.20 val PER: 0.0516
2025-07-30 02:27:40,564: t15.2023.08.25 val PER: 0.0828
2025-07-30 02:27:40,564: t15.2023.08.27 val PER: 0.1688
2025-07-30 02:27:40,565: t15.2023.09.01 val PER: 0.0463
2025-07-30 02:27:40,565: t15.2023.09.03 val PER: 0.1164
2025-07-30 02:27:40,565: t15.2023.09.24 val PER: 0.0728
2025-07-30 02:27:40,565: t15.2023.09.29 val PER: 0.0951
2025-07-30 02:27:40,565: t15.2023.10.01 val PER: 0.1215
2025-07-30 02:27:40,565: t15.2023.10.06 val PER: 0.0657
2025-07-30 02:27:40,565: t15.2023.10.08 val PER: 0.1800
2025-07-30 02:27:40,565: t15.2023.10.13 val PER: 0.1358
2025-07-30 02:27:40,565: t15.2023.10.15 val PER: 0.1213
2025-07-30 02:27:40,566: t15.2023.10.20 val PER: 0.1611
2025-07-30 02:27:40,566: t15.2023.10.22 val PER: 0.1036
2025-07-30 02:27:40,566: t15.2023.11.03 val PER: 0.1486
2025-07-30 02:27:40,566: t15.2023.11.04 val PER: 0.0137
2025-07-30 02:27:40,566: t15.2023.11.17 val PER: 0.0311
2025-07-30 02:27:40,566: t15.2023.11.19 val PER: 0.0140
2025-07-30 02:27:40,566: t15.2023.11.26 val PER: 0.0464
2025-07-30 02:27:40,566: t15.2023.12.03 val PER: 0.0483
2025-07-30 02:27:40,567: t15.2023.12.08 val PER: 0.0379
2025-07-30 02:27:40,567: t15.2023.12.10 val PER: 0.0263
2025-07-30 02:27:40,567: t15.2023.12.17 val PER: 0.0790
2025-07-30 02:27:40,567: t15.2023.12.29 val PER: 0.0721
2025-07-30 02:27:40,567: t15.2024.02.25 val PER: 0.0674
2025-07-30 02:27:40,567: t15.2024.03.08 val PER: 0.1849
2025-07-30 02:27:40,567: t15.2024.03.15 val PER: 0.1714
2025-07-30 02:27:40,567: t15.2024.03.17 val PER: 0.0760
2025-07-30 02:27:40,568: t15.2024.05.10 val PER: 0.1218
2025-07-30 02:27:40,568: t15.2024.06.14 val PER: 0.1009
2025-07-30 02:27:40,568: t15.2024.07.19 val PER: 0.1648
2025-07-30 02:27:40,568: t15.2024.07.21 val PER: 0.0572
2025-07-30 02:27:40,568: t15.2024.07.28 val PER: 0.0882
2025-07-30 02:27:40,568: t15.2025.01.10 val PER: 0.2548
2025-07-30 02:27:40,568: t15.2025.01.12 val PER: 0.0931
2025-07-30 02:27:40,568: t15.2025.03.14 val PER: 0.3373
2025-07-30 02:27:40,569: t15.2025.03.16 val PER: 0.1623
2025-07-30 02:27:40,569: t15.2025.03.30 val PER: 0.2172
2025-07-30 02:27:40,569: t15.2025.04.13 val PER: 0.1954
2025-07-30 02:28:10,527: Train batch 64200: loss: 0.04 grad norm: 2.61 time: 0.082
2025-07-30 02:28:40,729: Train batch 64400: loss: 0.01 grad norm: 1.24 time: 0.106
2025-07-30 02:29:10,996: Train batch 64600: loss: 0.01 grad norm: 0.30 time: 0.093
2025-07-30 02:29:41,466: Train batch 64800: loss: 0.02 grad norm: 2.31 time: 0.124
2025-07-30 02:30:11,472: Train batch 65000: loss: 0.09 grad norm: 6.19 time: 0.089
2025-07-30 02:30:42,044: Train batch 65200: loss: 0.02 grad norm: 1.67 time: 0.132
2025-07-30 02:31:12,712: Train batch 65400: loss: 0.02 grad norm: 1.38 time: 0.095
2025-07-30 02:31:42,588: Train batch 65600: loss: 0.01 grad norm: 0.87 time: 0.084
2025-07-30 02:32:13,090: Train batch 65800: loss: 0.12 grad norm: 6.87 time: 0.108
2025-07-30 02:32:43,174: Train batch 66000: loss: 0.02 grad norm: 1.12 time: 0.091
2025-07-30 02:32:43,175: Running test after training batch: 66000
2025-07-30 02:32:56,180: Val batch 66000: PER (avg): 0.1047 CTC Loss (avg): 22.8891 time: 13.005
2025-07-30 02:32:56,180: t15.2023.08.13 val PER: 0.0738
2025-07-30 02:32:56,181: t15.2023.08.18 val PER: 0.0696
2025-07-30 02:32:56,181: t15.2023.08.20 val PER: 0.0500
2025-07-30 02:32:56,181: t15.2023.08.25 val PER: 0.0828
2025-07-30 02:32:56,181: t15.2023.08.27 val PER: 0.1640
2025-07-30 02:32:56,181: t15.2023.09.01 val PER: 0.0495
2025-07-30 02:32:56,181: t15.2023.09.03 val PER: 0.1128
2025-07-30 02:32:56,181: t15.2023.09.24 val PER: 0.0765
2025-07-30 02:32:56,181: t15.2023.09.29 val PER: 0.0976
2025-07-30 02:32:56,182: t15.2023.10.01 val PER: 0.1328
2025-07-30 02:32:56,182: t15.2023.10.06 val PER: 0.0624
2025-07-30 02:32:56,182: t15.2023.10.08 val PER: 0.1759
2025-07-30 02:32:56,182: t15.2023.10.13 val PER: 0.1396
2025-07-30 02:32:56,182: t15.2023.10.15 val PER: 0.1167
2025-07-30 02:32:56,182: t15.2023.10.20 val PER: 0.1611
2025-07-30 02:32:56,182: t15.2023.10.22 val PER: 0.0991
2025-07-30 02:32:56,182: t15.2023.11.03 val PER: 0.1486
2025-07-30 02:32:56,182: t15.2023.11.04 val PER: 0.0102
2025-07-30 02:32:56,183: t15.2023.11.17 val PER: 0.0264
2025-07-30 02:32:56,183: t15.2023.11.19 val PER: 0.0180
2025-07-30 02:32:56,183: t15.2023.11.26 val PER: 0.0428
2025-07-30 02:32:56,183: t15.2023.12.03 val PER: 0.0462
2025-07-30 02:32:56,183: t15.2023.12.08 val PER: 0.0346
2025-07-30 02:32:56,183: t15.2023.12.10 val PER: 0.0315
2025-07-30 02:32:56,183: t15.2023.12.17 val PER: 0.0842
2025-07-30 02:32:56,183: t15.2023.12.29 val PER: 0.0686
2025-07-30 02:32:56,184: t15.2024.02.25 val PER: 0.0758
2025-07-30 02:32:56,184: t15.2024.03.08 val PER: 0.1622
2025-07-30 02:32:56,184: t15.2024.03.15 val PER: 0.1701
2025-07-30 02:32:56,184: t15.2024.03.17 val PER: 0.0697
2025-07-30 02:32:56,184: t15.2024.05.10 val PER: 0.1218
2025-07-30 02:32:56,184: t15.2024.06.14 val PER: 0.1088
2025-07-30 02:32:56,184: t15.2024.07.19 val PER: 0.1562
2025-07-30 02:32:56,184: t15.2024.07.21 val PER: 0.0572
2025-07-30 02:32:56,185: t15.2024.07.28 val PER: 0.0912
2025-07-30 02:32:56,185: t15.2025.01.10 val PER: 0.2452
2025-07-30 02:32:56,185: t15.2025.01.12 val PER: 0.0931
2025-07-30 02:32:56,185: t15.2025.03.14 val PER: 0.3284
2025-07-30 02:32:56,185: t15.2025.03.16 val PER: 0.1414
2025-07-30 02:32:56,185: t15.2025.03.30 val PER: 0.2241
2025-07-30 02:32:56,185: t15.2025.04.13 val PER: 0.1954
2025-07-30 02:33:25,793: Train batch 66200: loss: 0.01 grad norm: 0.64 time: 0.081
2025-07-30 02:33:56,452: Train batch 66400: loss: 0.01 grad norm: 0.28 time: 0.112
2025-07-30 02:34:26,341: Train batch 66600: loss: 0.02 grad norm: 1.50 time: 0.088
2025-07-30 02:34:56,401: Train batch 66800: loss: 0.03 grad norm: 1.43 time: 0.097
2025-07-30 02:35:27,110: Train batch 67000: loss: 0.15 grad norm: 2.05 time: 0.126
2025-07-30 02:35:57,695: Train batch 67200: loss: 0.18 grad norm: 7.36 time: 0.125
2025-07-30 02:36:27,795: Train batch 67400: loss: 0.01 grad norm: 0.71 time: 0.092
2025-07-30 02:36:58,226: Train batch 67600: loss: 0.02 grad norm: 1.80 time: 0.084
2025-07-30 02:37:28,320: Train batch 67800: loss: 0.02 grad norm: 1.34 time: 0.085
2025-07-30 02:37:58,878: Train batch 68000: loss: 0.03 grad norm: 1.50 time: 0.088
2025-07-30 02:37:58,878: Running test after training batch: 68000
2025-07-30 02:38:12,066: Val batch 68000: PER (avg): 0.1043 CTC Loss (avg): 22.7011 time: 13.188
2025-07-30 02:38:12,066: t15.2023.08.13 val PER: 0.0738
2025-07-30 02:38:12,066: t15.2023.08.18 val PER: 0.0629
2025-07-30 02:38:12,066: t15.2023.08.20 val PER: 0.0540
2025-07-30 02:38:12,066: t15.2023.08.25 val PER: 0.0904
2025-07-30 02:38:12,067: t15.2023.08.27 val PER: 0.1511
2025-07-30 02:38:12,067: t15.2023.09.01 val PER: 0.0471
2025-07-30 02:38:12,067: t15.2023.09.03 val PER: 0.1176
2025-07-30 02:38:12,067: t15.2023.09.24 val PER: 0.0667
2025-07-30 02:38:12,067: t15.2023.09.29 val PER: 0.0938
2025-07-30 02:38:12,067: t15.2023.10.01 val PER: 0.1235
2025-07-30 02:38:12,067: t15.2023.10.06 val PER: 0.0581
2025-07-30 02:38:12,067: t15.2023.10.08 val PER: 0.1651
2025-07-30 02:38:12,067: t15.2023.10.13 val PER: 0.1389
2025-07-30 02:38:12,068: t15.2023.10.15 val PER: 0.1173
2025-07-30 02:38:12,068: t15.2023.10.20 val PER: 0.1544
2025-07-30 02:38:12,068: t15.2023.10.22 val PER: 0.0991
2025-07-30 02:38:12,068: t15.2023.11.03 val PER: 0.1520
2025-07-30 02:38:12,068: t15.2023.11.04 val PER: 0.0102
2025-07-30 02:38:12,068: t15.2023.11.17 val PER: 0.0295
2025-07-30 02:38:12,068: t15.2023.11.19 val PER: 0.0180
2025-07-30 02:38:12,068: t15.2023.11.26 val PER: 0.0420
2025-07-30 02:38:12,069: t15.2023.12.03 val PER: 0.0452
2025-07-30 02:38:12,069: t15.2023.12.08 val PER: 0.0346
2025-07-30 02:38:12,069: t15.2023.12.10 val PER: 0.0355
2025-07-30 02:38:12,069: t15.2023.12.17 val PER: 0.0821
2025-07-30 02:38:12,069: t15.2023.12.29 val PER: 0.0734
2025-07-30 02:38:12,069: t15.2024.02.25 val PER: 0.0688
2025-07-30 02:38:12,069: t15.2024.03.08 val PER: 0.1636
2025-07-30 02:38:12,069: t15.2024.03.15 val PER: 0.1676
2025-07-30 02:38:12,070: t15.2024.03.17 val PER: 0.0746
2025-07-30 02:38:12,070: t15.2024.05.10 val PER: 0.1293
2025-07-30 02:38:12,070: t15.2024.06.14 val PER: 0.1057
2025-07-30 02:38:12,070: t15.2024.07.19 val PER: 0.1575
2025-07-30 02:38:12,070: t15.2024.07.21 val PER: 0.0531
2025-07-30 02:38:12,070: t15.2024.07.28 val PER: 0.0978
2025-07-30 02:38:12,070: t15.2025.01.10 val PER: 0.2521
2025-07-30 02:38:12,070: t15.2025.01.12 val PER: 0.0931
2025-07-30 02:38:12,070: t15.2025.03.14 val PER: 0.3328
2025-07-30 02:38:12,071: t15.2025.03.16 val PER: 0.1597
2025-07-30 02:38:12,071: t15.2025.03.30 val PER: 0.2172
2025-07-30 02:38:12,071: t15.2025.04.13 val PER: 0.1912
2025-07-30 02:38:42,145: Train batch 68200: loss: 0.02 grad norm: 1.22 time: 0.133
2025-07-30 02:39:12,059: Train batch 68400: loss: 0.01 grad norm: 0.96 time: 0.103
2025-07-30 02:39:42,229: Train batch 68600: loss: 0.02 grad norm: 2.78 time: 0.082
2025-07-30 02:40:12,458: Train batch 68800: loss: 0.14 grad norm: 12.36 time: 0.084
2025-07-30 02:40:42,653: Train batch 69000: loss: 0.01 grad norm: 0.71 time: 0.085
2025-07-30 02:41:12,877: Train batch 69200: loss: 0.01 grad norm: 0.66 time: 0.093
2025-07-30 02:41:43,010: Train batch 69400: loss: 0.04 grad norm: 3.04 time: 0.088
2025-07-30 02:42:13,314: Train batch 69600: loss: 0.09 grad norm: 7.60 time: 0.092
2025-07-30 02:42:43,157: Train batch 69800: loss: 0.02 grad norm: 1.38 time: 0.093
2025-07-30 02:43:13,498: Train batch 70000: loss: 0.04 grad norm: 2.77 time: 0.082
2025-07-30 02:43:13,499: Running test after training batch: 70000
2025-07-30 02:43:26,651: Val batch 70000: PER (avg): 0.1051 CTC Loss (avg): 22.7363 time: 13.152
2025-07-30 02:43:26,651: t15.2023.08.13 val PER: 0.0665
2025-07-30 02:43:26,651: t15.2023.08.18 val PER: 0.0645
2025-07-30 02:43:26,652: t15.2023.08.20 val PER: 0.0548
2025-07-30 02:43:26,652: t15.2023.08.25 val PER: 0.0904
2025-07-30 02:43:26,652: t15.2023.08.27 val PER: 0.1543
2025-07-30 02:43:26,652: t15.2023.09.01 val PER: 0.0503
2025-07-30 02:43:26,652: t15.2023.09.03 val PER: 0.1235
2025-07-30 02:43:26,652: t15.2023.09.24 val PER: 0.0631
2025-07-30 02:43:26,652: t15.2023.09.29 val PER: 0.0951
2025-07-30 02:43:26,652: t15.2023.10.01 val PER: 0.1229
2025-07-30 02:43:26,653: t15.2023.10.06 val PER: 0.0635
2025-07-30 02:43:26,653: t15.2023.10.08 val PER: 0.1827
2025-07-30 02:43:26,653: t15.2023.10.13 val PER: 0.1389
2025-07-30 02:43:26,653: t15.2023.10.15 val PER: 0.1140
2025-07-30 02:43:26,653: t15.2023.10.20 val PER: 0.1577
2025-07-30 02:43:26,653: t15.2023.10.22 val PER: 0.1036
2025-07-30 02:43:26,653: t15.2023.11.03 val PER: 0.1547
2025-07-30 02:43:26,653: t15.2023.11.04 val PER: 0.0102
2025-07-30 02:43:26,654: t15.2023.11.17 val PER: 0.0280
2025-07-30 02:43:26,654: t15.2023.11.19 val PER: 0.0160
2025-07-30 02:43:26,654: t15.2023.11.26 val PER: 0.0420
2025-07-30 02:43:26,654: t15.2023.12.03 val PER: 0.0441
2025-07-30 02:43:26,654: t15.2023.12.08 val PER: 0.0366
2025-07-30 02:43:26,654: t15.2023.12.10 val PER: 0.0342
2025-07-30 02:43:26,654: t15.2023.12.17 val PER: 0.0780
2025-07-30 02:43:26,654: t15.2023.12.29 val PER: 0.0652
2025-07-30 02:43:26,655: t15.2024.02.25 val PER: 0.0674
2025-07-30 02:43:26,655: t15.2024.03.08 val PER: 0.1693
2025-07-30 02:43:26,655: t15.2024.03.15 val PER: 0.1695
2025-07-30 02:43:26,655: t15.2024.03.17 val PER: 0.0781
2025-07-30 02:43:26,655: t15.2024.05.10 val PER: 0.1174
2025-07-30 02:43:26,655: t15.2024.06.14 val PER: 0.1009
2025-07-30 02:43:26,655: t15.2024.07.19 val PER: 0.1641
2025-07-30 02:43:26,655: t15.2024.07.21 val PER: 0.0552
2025-07-30 02:43:26,656: t15.2024.07.28 val PER: 0.0912
2025-07-30 02:43:26,656: t15.2025.01.10 val PER: 0.2479
2025-07-30 02:43:26,656: t15.2025.01.12 val PER: 0.0916
2025-07-30 02:43:26,656: t15.2025.03.14 val PER: 0.3447
2025-07-30 02:43:26,656: t15.2025.03.16 val PER: 0.1610
2025-07-30 02:43:26,656: t15.2025.03.30 val PER: 0.2253
2025-07-30 02:43:26,656: t15.2025.04.13 val PER: 0.2011
2025-07-30 02:43:56,516: Train batch 70200: loss: 0.01 grad norm: 0.63 time: 0.084
2025-07-30 02:44:26,482: Train batch 70400: loss: 0.02 grad norm: 2.64 time: 0.086
2025-07-30 02:44:56,953: Train batch 70600: loss: 0.01 grad norm: 0.48 time: 0.102
2025-07-30 02:45:26,941: Train batch 70800: loss: 0.02 grad norm: 3.67 time: 0.088
2025-07-30 02:45:57,624: Train batch 71000: loss: 0.02 grad norm: 3.65 time: 0.122
2025-07-30 02:46:27,455: Train batch 71200: loss: 0.06 grad norm: 3.31 time: 0.075
2025-07-30 02:46:57,985: Train batch 71400: loss: 0.02 grad norm: 0.86 time: 0.097
2025-07-30 02:47:28,486: Train batch 71600: loss: 0.01 grad norm: 1.01 time: 0.070
2025-07-30 02:47:58,049: Train batch 71800: loss: 0.02 grad norm: 1.07 time: 0.089
2025-07-30 02:48:28,045: Train batch 72000: loss: 0.03 grad norm: 2.03 time: 0.085
2025-07-30 02:48:28,046: Running test after training batch: 72000
2025-07-30 02:48:42,127: Val batch 72000: PER (avg): 0.1041 CTC Loss (avg): 22.6876 time: 14.081
2025-07-30 02:48:42,127: t15.2023.08.13 val PER: 0.0738
2025-07-30 02:48:42,127: t15.2023.08.18 val PER: 0.0604
2025-07-30 02:48:42,127: t15.2023.08.20 val PER: 0.0477
2025-07-30 02:48:42,127: t15.2023.08.25 val PER: 0.0813
2025-07-30 02:48:42,128: t15.2023.08.27 val PER: 0.1479
2025-07-30 02:48:42,128: t15.2023.09.01 val PER: 0.0463
2025-07-30 02:48:42,128: t15.2023.09.03 val PER: 0.1176
2025-07-30 02:48:42,128: t15.2023.09.24 val PER: 0.0716
2025-07-30 02:48:42,128: t15.2023.09.29 val PER: 0.0944
2025-07-30 02:48:42,128: t15.2023.10.01 val PER: 0.1275
2025-07-30 02:48:42,128: t15.2023.10.06 val PER: 0.0592
2025-07-30 02:48:42,128: t15.2023.10.08 val PER: 0.1719
2025-07-30 02:48:42,129: t15.2023.10.13 val PER: 0.1396
2025-07-30 02:48:42,129: t15.2023.10.15 val PER: 0.1140
2025-07-30 02:48:42,129: t15.2023.10.20 val PER: 0.1409
2025-07-30 02:48:42,129: t15.2023.10.22 val PER: 0.1013
2025-07-30 02:48:42,129: t15.2023.11.03 val PER: 0.1493
2025-07-30 02:48:42,129: t15.2023.11.04 val PER: 0.0102
2025-07-30 02:48:42,129: t15.2023.11.17 val PER: 0.0280
2025-07-30 02:48:42,130: t15.2023.11.19 val PER: 0.0200
2025-07-30 02:48:42,130: t15.2023.11.26 val PER: 0.0478
2025-07-30 02:48:42,130: t15.2023.12.03 val PER: 0.0431
2025-07-30 02:48:42,130: t15.2023.12.08 val PER: 0.0340
2025-07-30 02:48:42,130: t15.2023.12.10 val PER: 0.0329
2025-07-30 02:48:42,130: t15.2023.12.17 val PER: 0.0717
2025-07-30 02:48:42,130: t15.2023.12.29 val PER: 0.0741
2025-07-30 02:48:42,130: t15.2024.02.25 val PER: 0.0702
2025-07-30 02:48:42,130: t15.2024.03.08 val PER: 0.1693
2025-07-30 02:48:42,131: t15.2024.03.15 val PER: 0.1720
2025-07-30 02:48:42,131: t15.2024.03.17 val PER: 0.0767
2025-07-30 02:48:42,131: t15.2024.05.10 val PER: 0.1189
2025-07-30 02:48:42,131: t15.2024.06.14 val PER: 0.1025
2025-07-30 02:48:42,131: t15.2024.07.19 val PER: 0.1556
2025-07-30 02:48:42,131: t15.2024.07.21 val PER: 0.0579
2025-07-30 02:48:42,131: t15.2024.07.28 val PER: 0.0890
2025-07-30 02:48:42,131: t15.2025.01.10 val PER: 0.2507
2025-07-30 02:48:42,132: t15.2025.01.12 val PER: 0.0978
2025-07-30 02:48:42,132: t15.2025.03.14 val PER: 0.3254
2025-07-30 02:48:42,132: t15.2025.03.16 val PER: 0.1688
2025-07-30 02:48:42,132: t15.2025.03.30 val PER: 0.2253
2025-07-30 02:48:42,132: t15.2025.04.13 val PER: 0.1869
2025-07-30 02:49:11,913: Train batch 72200: loss: 0.01 grad norm: 0.83 time: 0.092
2025-07-30 02:49:41,943: Train batch 72400: loss: 0.01 grad norm: 0.68 time: 0.129
2025-07-30 02:50:12,222: Train batch 72600: loss: 0.02 grad norm: 1.22 time: 0.090
2025-07-30 02:50:42,427: Train batch 72800: loss: 0.01 grad norm: 1.49 time: 0.085
2025-07-30 02:51:13,517: Train batch 73000: loss: 0.02 grad norm: 3.11 time: 0.093
2025-07-30 02:51:43,912: Train batch 73200: loss: 0.03 grad norm: 2.36 time: 0.097
2025-07-30 02:52:14,442: Train batch 73400: loss: 0.10 grad norm: 4.73 time: 0.111
2025-07-30 02:52:44,765: Train batch 73600: loss: 0.03 grad norm: 2.04 time: 0.132
2025-07-30 02:53:15,169: Train batch 73800: loss: 0.01 grad norm: 0.54 time: 0.088
2025-07-30 02:53:45,093: Train batch 74000: loss: 0.03 grad norm: 1.92 time: 0.072
2025-07-30 02:53:45,093: Running test after training batch: 74000
2025-07-30 02:53:58,771: Val batch 74000: PER (avg): 0.1048 CTC Loss (avg): 23.1135 time: 13.677
2025-07-30 02:53:58,771: t15.2023.08.13 val PER: 0.0696
2025-07-30 02:53:58,771: t15.2023.08.18 val PER: 0.0645
2025-07-30 02:53:58,771: t15.2023.08.20 val PER: 0.0492
2025-07-30 02:53:58,772: t15.2023.08.25 val PER: 0.0798
2025-07-30 02:53:58,772: t15.2023.08.27 val PER: 0.1447
2025-07-30 02:53:58,772: t15.2023.09.01 val PER: 0.0487
2025-07-30 02:53:58,772: t15.2023.09.03 val PER: 0.1164
2025-07-30 02:53:58,772: t15.2023.09.24 val PER: 0.0740
2025-07-30 02:53:58,772: t15.2023.09.29 val PER: 0.0913
2025-07-30 02:53:58,772: t15.2023.10.01 val PER: 0.1301
2025-07-30 02:53:58,772: t15.2023.10.06 val PER: 0.0624
2025-07-30 02:53:58,772: t15.2023.10.08 val PER: 0.1800
2025-07-30 02:53:58,772: t15.2023.10.13 val PER: 0.1458
2025-07-30 02:53:58,773: t15.2023.10.15 val PER: 0.1160
2025-07-30 02:53:58,773: t15.2023.10.20 val PER: 0.1611
2025-07-30 02:53:58,773: t15.2023.10.22 val PER: 0.1024
2025-07-30 02:53:58,773: t15.2023.11.03 val PER: 0.1533
2025-07-30 02:53:58,773: t15.2023.11.04 val PER: 0.0137
2025-07-30 02:53:58,773: t15.2023.11.17 val PER: 0.0218
2025-07-30 02:53:58,773: t15.2023.11.19 val PER: 0.0200
2025-07-30 02:53:58,773: t15.2023.11.26 val PER: 0.0420
2025-07-30 02:53:58,773: t15.2023.12.03 val PER: 0.0473
2025-07-30 02:53:58,773: t15.2023.12.08 val PER: 0.0333
2025-07-30 02:53:58,774: t15.2023.12.10 val PER: 0.0342
2025-07-30 02:53:58,774: t15.2023.12.17 val PER: 0.0852
2025-07-30 02:53:58,774: t15.2023.12.29 val PER: 0.0728
2025-07-30 02:53:58,774: t15.2024.02.25 val PER: 0.0772
2025-07-30 02:53:58,774: t15.2024.03.08 val PER: 0.1849
2025-07-30 02:53:58,774: t15.2024.03.15 val PER: 0.1682
2025-07-30 02:53:58,774: t15.2024.03.17 val PER: 0.0753
2025-07-30 02:53:58,774: t15.2024.05.10 val PER: 0.1233
2025-07-30 02:53:58,774: t15.2024.06.14 val PER: 0.1136
2025-07-30 02:53:58,774: t15.2024.07.19 val PER: 0.1496
2025-07-30 02:53:58,775: t15.2024.07.21 val PER: 0.0510
2025-07-30 02:53:58,775: t15.2024.07.28 val PER: 0.0868
2025-07-30 02:53:58,775: t15.2025.01.10 val PER: 0.2438
2025-07-30 02:53:58,775: t15.2025.01.12 val PER: 0.0931
2025-07-30 02:53:58,775: t15.2025.03.14 val PER: 0.3195
2025-07-30 02:53:58,775: t15.2025.03.16 val PER: 0.1610
2025-07-30 02:53:58,775: t15.2025.03.30 val PER: 0.2264
2025-07-30 02:53:58,775: t15.2025.04.13 val PER: 0.2040
2025-07-30 02:54:29,225: Train batch 74200: loss: 0.01 grad norm: 0.55 time: 0.083
2025-07-30 02:54:59,114: Train batch 74400: loss: 0.01 grad norm: 0.57 time: 0.125
2025-07-30 02:55:28,796: Train batch 74600: loss: 0.04 grad norm: 4.70 time: 0.099
2025-07-30 02:55:58,765: Train batch 74800: loss: 0.03 grad norm: 2.09 time: 0.086
2025-07-30 02:56:29,953: Train batch 75000: loss: 0.02 grad norm: 2.36 time: 0.130
2025-07-30 02:56:59,972: Train batch 75200: loss: 0.01 grad norm: 0.89 time: 0.097
2025-07-30 02:57:30,237: Train batch 75400: loss: 0.08 grad norm: 4.33 time: 0.091
2025-07-30 02:58:00,401: Train batch 75600: loss: 0.01 grad norm: 0.70 time: 0.094
2025-07-30 02:58:30,470: Train batch 75800: loss: 0.01 grad norm: 0.83 time: 0.129
2025-07-30 02:59:00,758: Train batch 76000: loss: 0.02 grad norm: 1.71 time: 0.082
2025-07-30 02:59:00,758: Running test after training batch: 76000
2025-07-30 02:59:14,226: Val batch 76000: PER (avg): 0.1036 CTC Loss (avg): 22.8425 time: 13.468
2025-07-30 02:59:14,226: t15.2023.08.13 val PER: 0.0707
2025-07-30 02:59:14,226: t15.2023.08.18 val PER: 0.0587
2025-07-30 02:59:14,227: t15.2023.08.20 val PER: 0.0492
2025-07-30 02:59:14,227: t15.2023.08.25 val PER: 0.0783
2025-07-30 02:59:14,227: t15.2023.08.27 val PER: 0.1463
2025-07-30 02:59:14,227: t15.2023.09.01 val PER: 0.0471
2025-07-30 02:59:14,227: t15.2023.09.03 val PER: 0.1081
2025-07-30 02:59:14,227: t15.2023.09.24 val PER: 0.0716
2025-07-30 02:59:14,227: t15.2023.09.29 val PER: 0.0932
2025-07-30 02:59:14,227: t15.2023.10.01 val PER: 0.1262
2025-07-30 02:59:14,227: t15.2023.10.06 val PER: 0.0614
2025-07-30 02:59:14,228: t15.2023.10.08 val PER: 0.1719
2025-07-30 02:59:14,228: t15.2023.10.13 val PER: 0.1373
2025-07-30 02:59:14,228: t15.2023.10.15 val PER: 0.1094
2025-07-30 02:59:14,228: t15.2023.10.20 val PER: 0.1577
2025-07-30 02:59:14,228: t15.2023.10.22 val PER: 0.0958
2025-07-30 02:59:14,228: t15.2023.11.03 val PER: 0.1526
2025-07-30 02:59:14,228: t15.2023.11.04 val PER: 0.0068
2025-07-30 02:59:14,228: t15.2023.11.17 val PER: 0.0249
2025-07-30 02:59:14,228: t15.2023.11.19 val PER: 0.0160
2025-07-30 02:59:14,228: t15.2023.11.26 val PER: 0.0478
2025-07-30 02:59:14,229: t15.2023.12.03 val PER: 0.0494
2025-07-30 02:59:14,229: t15.2023.12.08 val PER: 0.0353
2025-07-30 02:59:14,229: t15.2023.12.10 val PER: 0.0276
2025-07-30 02:59:14,229: t15.2023.12.17 val PER: 0.0728
2025-07-30 02:59:14,229: t15.2023.12.29 val PER: 0.0721
2025-07-30 02:59:14,229: t15.2024.02.25 val PER: 0.0744
2025-07-30 02:59:14,229: t15.2024.03.08 val PER: 0.1821
2025-07-30 02:59:14,229: t15.2024.03.15 val PER: 0.1807
2025-07-30 02:59:14,229: t15.2024.03.17 val PER: 0.0753
2025-07-30 02:59:14,229: t15.2024.05.10 val PER: 0.1218
2025-07-30 02:59:14,230: t15.2024.06.14 val PER: 0.0994
2025-07-30 02:59:14,230: t15.2024.07.19 val PER: 0.1562
2025-07-30 02:59:14,230: t15.2024.07.21 val PER: 0.0538
2025-07-30 02:59:14,230: t15.2024.07.28 val PER: 0.0897
2025-07-30 02:59:14,230: t15.2025.01.10 val PER: 0.2410
2025-07-30 02:59:14,230: t15.2025.01.12 val PER: 0.0955
2025-07-30 02:59:14,230: t15.2025.03.14 val PER: 0.3240
2025-07-30 02:59:14,230: t15.2025.03.16 val PER: 0.1597
2025-07-30 02:59:14,231: t15.2025.03.30 val PER: 0.2241
2025-07-30 02:59:14,231: t15.2025.04.13 val PER: 0.1912
2025-07-30 02:59:14,231: New best test PER 0.1041 --> 0.1036
2025-07-30 02:59:14,231: Checkpointing model
2025-07-30 02:59:15,411: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-07-30 02:59:45,717: Train batch 76200: loss: 0.06 grad norm: 3.72 time: 0.091
2025-07-30 03:00:16,105: Train batch 76400: loss: 0.07 grad norm: 4.05 time: 0.111
2025-07-30 03:00:46,273: Train batch 76600: loss: 0.03 grad norm: 2.29 time: 0.076
2025-07-30 03:01:16,513: Train batch 76800: loss: 0.02 grad norm: 1.21 time: 0.107
2025-07-30 03:01:47,016: Train batch 77000: loss: 0.01 grad norm: 0.49 time: 0.088
2025-07-30 03:02:17,136: Train batch 77200: loss: 0.01 grad norm: 0.68 time: 0.094
2025-07-30 03:02:47,828: Train batch 77400: loss: 0.14 grad norm: 8.42 time: 0.072
2025-07-30 03:03:18,267: Train batch 77600: loss: 0.01 grad norm: 1.01 time: 0.089
2025-07-30 03:03:48,742: Train batch 77800: loss: 0.13 grad norm: 10.30 time: 0.079
2025-07-30 03:04:18,865: Train batch 78000: loss: 0.03 grad norm: 1.89 time: 0.099
2025-07-30 03:04:18,865: Running test after training batch: 78000
2025-07-30 03:04:32,711: Val batch 78000: PER (avg): 0.1031 CTC Loss (avg): 22.8140 time: 13.846
2025-07-30 03:04:32,711: t15.2023.08.13 val PER: 0.0696
2025-07-30 03:04:32,712: t15.2023.08.18 val PER: 0.0562
2025-07-30 03:04:32,712: t15.2023.08.20 val PER: 0.0524
2025-07-30 03:04:32,712: t15.2023.08.25 val PER: 0.0783
2025-07-30 03:04:32,712: t15.2023.08.27 val PER: 0.1431
2025-07-30 03:04:32,712: t15.2023.09.01 val PER: 0.0406
2025-07-30 03:04:32,712: t15.2023.09.03 val PER: 0.1128
2025-07-30 03:04:32,712: t15.2023.09.24 val PER: 0.0667
2025-07-30 03:04:32,712: t15.2023.09.29 val PER: 0.0919
2025-07-30 03:04:32,713: t15.2023.10.01 val PER: 0.1295
2025-07-30 03:04:32,713: t15.2023.10.06 val PER: 0.0560
2025-07-30 03:04:32,713: t15.2023.10.08 val PER: 0.1705
2025-07-30 03:04:32,713: t15.2023.10.13 val PER: 0.1435
2025-07-30 03:04:32,713: t15.2023.10.15 val PER: 0.1121
2025-07-30 03:04:32,713: t15.2023.10.20 val PER: 0.1644
2025-07-30 03:04:32,713: t15.2023.10.22 val PER: 0.0958
2025-07-30 03:04:32,713: t15.2023.11.03 val PER: 0.1513
2025-07-30 03:04:32,713: t15.2023.11.04 val PER: 0.0102
2025-07-30 03:04:32,714: t15.2023.11.17 val PER: 0.0233
2025-07-30 03:04:32,714: t15.2023.11.19 val PER: 0.0180
2025-07-30 03:04:32,714: t15.2023.11.26 val PER: 0.0442
2025-07-30 03:04:32,714: t15.2023.12.03 val PER: 0.0473
2025-07-30 03:04:32,714: t15.2023.12.08 val PER: 0.0353
2025-07-30 03:04:32,714: t15.2023.12.10 val PER: 0.0237
2025-07-30 03:04:32,714: t15.2023.12.17 val PER: 0.0738
2025-07-30 03:04:32,714: t15.2023.12.29 val PER: 0.0803
2025-07-30 03:04:32,714: t15.2024.02.25 val PER: 0.0744
2025-07-30 03:04:32,715: t15.2024.03.08 val PER: 0.1778
2025-07-30 03:04:32,715: t15.2024.03.15 val PER: 0.1682
2025-07-30 03:04:32,715: t15.2024.03.17 val PER: 0.0746
2025-07-30 03:04:32,715: t15.2024.05.10 val PER: 0.1233
2025-07-30 03:04:32,715: t15.2024.06.14 val PER: 0.1057
2025-07-30 03:04:32,715: t15.2024.07.19 val PER: 0.1529
2025-07-30 03:04:32,715: t15.2024.07.21 val PER: 0.0524
2025-07-30 03:04:32,715: t15.2024.07.28 val PER: 0.0941
2025-07-30 03:04:32,716: t15.2025.01.10 val PER: 0.2328
2025-07-30 03:04:32,716: t15.2025.01.12 val PER: 0.0931
2025-07-30 03:04:32,716: t15.2025.03.14 val PER: 0.3225
2025-07-30 03:04:32,716: t15.2025.03.16 val PER: 0.1597
2025-07-30 03:04:32,716: t15.2025.03.30 val PER: 0.2241
2025-07-30 03:04:32,716: t15.2025.04.13 val PER: 0.1983
2025-07-30 03:04:32,716: New best test PER 0.1036 --> 0.1031
2025-07-30 03:04:32,716: Checkpointing model
2025-07-30 03:04:33,894: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-07-30 03:05:04,754: Train batch 78200: loss: 0.01 grad norm: 1.16 time: 0.075
2025-07-30 03:05:34,316: Train batch 78400: loss: 0.03 grad norm: 2.12 time: 0.077
2025-07-30 03:06:04,475: Train batch 78600: loss: 0.03 grad norm: 1.89 time: 0.085
2025-07-30 03:06:34,981: Train batch 78800: loss: 0.01 grad norm: 3.06 time: 0.094
2025-07-30 03:07:05,519: Train batch 79000: loss: 0.08 grad norm: 3.95 time: 0.124
2025-07-30 03:07:35,524: Train batch 79200: loss: 0.03 grad norm: 2.94 time: 0.107
2025-07-30 03:08:06,023: Train batch 79400: loss: 0.02 grad norm: 1.83 time: 0.086
2025-07-30 03:08:36,706: Train batch 79600: loss: 0.02 grad norm: 0.89 time: 0.077
2025-07-30 03:09:07,315: Train batch 79800: loss: 0.02 grad norm: 1.21 time: 0.083
2025-07-30 03:09:37,375: Train batch 80000: loss: 0.17 grad norm: 5.86 time: 0.077
2025-07-30 03:09:37,375: Running test after training batch: 80000
2025-07-30 03:09:50,386: Val batch 80000: PER (avg): 0.1033 CTC Loss (avg): 22.8260 time: 13.011
2025-07-30 03:09:50,387: t15.2023.08.13 val PER: 0.0696
2025-07-30 03:09:50,387: t15.2023.08.18 val PER: 0.0620
2025-07-30 03:09:50,387: t15.2023.08.20 val PER: 0.0500
2025-07-30 03:09:50,387: t15.2023.08.25 val PER: 0.0843
2025-07-30 03:09:50,387: t15.2023.08.27 val PER: 0.1495
2025-07-30 03:09:50,387: t15.2023.09.01 val PER: 0.0463
2025-07-30 03:09:50,387: t15.2023.09.03 val PER: 0.1081
2025-07-30 03:09:50,387: t15.2023.09.24 val PER: 0.0728
2025-07-30 03:09:50,387: t15.2023.09.29 val PER: 0.0938
2025-07-30 03:09:50,388: t15.2023.10.01 val PER: 0.1328
2025-07-30 03:09:50,388: t15.2023.10.06 val PER: 0.0624
2025-07-30 03:09:50,388: t15.2023.10.08 val PER: 0.1773
2025-07-30 03:09:50,388: t15.2023.10.13 val PER: 0.1420
2025-07-30 03:09:50,388: t15.2023.10.15 val PER: 0.1127
2025-07-30 03:09:50,388: t15.2023.10.20 val PER: 0.1477
2025-07-30 03:09:50,388: t15.2023.10.22 val PER: 0.0958
2025-07-30 03:09:50,388: t15.2023.11.03 val PER: 0.1486
2025-07-30 03:09:50,388: t15.2023.11.04 val PER: 0.0205
2025-07-30 03:09:50,388: t15.2023.11.17 val PER: 0.0295
2025-07-30 03:09:50,389: t15.2023.11.19 val PER: 0.0180
2025-07-30 03:09:50,389: t15.2023.11.26 val PER: 0.0413
2025-07-30 03:09:50,389: t15.2023.12.03 val PER: 0.0452
2025-07-30 03:09:50,389: t15.2023.12.08 val PER: 0.0360
2025-07-30 03:09:50,389: t15.2023.12.10 val PER: 0.0276
2025-07-30 03:09:50,389: t15.2023.12.17 val PER: 0.0759
2025-07-30 03:09:50,389: t15.2023.12.29 val PER: 0.0707
2025-07-30 03:09:50,389: t15.2024.02.25 val PER: 0.0716
2025-07-30 03:09:50,389: t15.2024.03.08 val PER: 0.1863
2025-07-30 03:09:50,389: t15.2024.03.15 val PER: 0.1801
2025-07-30 03:09:50,390: t15.2024.03.17 val PER: 0.0711
2025-07-30 03:09:50,390: t15.2024.05.10 val PER: 0.1218
2025-07-30 03:09:50,390: t15.2024.06.14 val PER: 0.0962
2025-07-30 03:09:50,390: t15.2024.07.19 val PER: 0.1536
2025-07-30 03:09:50,390: t15.2024.07.21 val PER: 0.0510
2025-07-30 03:09:50,390: t15.2024.07.28 val PER: 0.0868
2025-07-30 03:09:50,390: t15.2025.01.10 val PER: 0.2397
2025-07-30 03:09:50,390: t15.2025.01.12 val PER: 0.0931
2025-07-30 03:09:50,391: t15.2025.03.14 val PER: 0.3269
2025-07-30 03:09:50,391: t15.2025.03.16 val PER: 0.1440
2025-07-30 03:09:50,391: t15.2025.03.30 val PER: 0.2195
2025-07-30 03:09:50,391: t15.2025.04.13 val PER: 0.1883
2025-07-30 03:10:20,023: Train batch 80200: loss: 0.01 grad norm: 4.69 time: 0.085
2025-07-30 03:10:49,792: Train batch 80400: loss: 0.01 grad norm: 1.14 time: 0.083
2025-07-30 03:11:20,253: Train batch 80600: loss: 0.01 grad norm: 0.49 time: 0.094
2025-07-30 03:11:50,578: Train batch 80800: loss: 0.02 grad norm: 1.83 time: 0.080
2025-07-30 03:12:21,501: Train batch 81000: loss: 0.02 grad norm: 2.30 time: 0.119
2025-07-30 03:12:51,440: Train batch 81200: loss: 0.02 grad norm: 0.79 time: 0.103
2025-07-30 03:13:21,274: Train batch 81400: loss: 0.03 grad norm: 2.20 time: 0.083
2025-07-30 03:13:51,899: Train batch 81600: loss: 0.01 grad norm: 0.68 time: 0.087
2025-07-30 03:14:22,547: Train batch 81800: loss: 0.00 grad norm: 0.21 time: 0.079
2025-07-30 03:14:52,827: Train batch 82000: loss: 0.02 grad norm: 0.74 time: 0.095
2025-07-30 03:14:52,827: Running test after training batch: 82000
2025-07-30 03:15:06,430: Val batch 82000: PER (avg): 0.1023 CTC Loss (avg): 22.9795 time: 13.603
2025-07-30 03:15:06,430: t15.2023.08.13 val PER: 0.0707
2025-07-30 03:15:06,430: t15.2023.08.18 val PER: 0.0587
2025-07-30 03:15:06,431: t15.2023.08.20 val PER: 0.0485
2025-07-30 03:15:06,431: t15.2023.08.25 val PER: 0.0828
2025-07-30 03:15:06,431: t15.2023.08.27 val PER: 0.1592
2025-07-30 03:15:06,431: t15.2023.09.01 val PER: 0.0463
2025-07-30 03:15:06,431: t15.2023.09.03 val PER: 0.1069
2025-07-30 03:15:06,431: t15.2023.09.24 val PER: 0.0716
2025-07-30 03:15:06,431: t15.2023.09.29 val PER: 0.0964
2025-07-30 03:15:06,431: t15.2023.10.01 val PER: 0.1288
2025-07-30 03:15:06,431: t15.2023.10.06 val PER: 0.0614
2025-07-30 03:15:06,432: t15.2023.10.08 val PER: 0.1705
2025-07-30 03:15:06,432: t15.2023.10.13 val PER: 0.1412
2025-07-30 03:15:06,432: t15.2023.10.15 val PER: 0.1094
2025-07-30 03:15:06,432: t15.2023.10.20 val PER: 0.1544
2025-07-30 03:15:06,432: t15.2023.10.22 val PER: 0.0991
2025-07-30 03:15:06,432: t15.2023.11.03 val PER: 0.1499
2025-07-30 03:15:06,432: t15.2023.11.04 val PER: 0.0171
2025-07-30 03:15:06,432: t15.2023.11.17 val PER: 0.0280
2025-07-30 03:15:06,432: t15.2023.11.19 val PER: 0.0160
2025-07-30 03:15:06,432: t15.2023.11.26 val PER: 0.0449
2025-07-30 03:15:06,433: t15.2023.12.03 val PER: 0.0462
2025-07-30 03:15:06,433: t15.2023.12.08 val PER: 0.0353
2025-07-30 03:15:06,433: t15.2023.12.10 val PER: 0.0223
2025-07-30 03:15:06,433: t15.2023.12.17 val PER: 0.0676
2025-07-30 03:15:06,433: t15.2023.12.29 val PER: 0.0666
2025-07-30 03:15:06,433: t15.2024.02.25 val PER: 0.0787
2025-07-30 03:15:06,433: t15.2024.03.08 val PER: 0.1792
2025-07-30 03:15:06,433: t15.2024.03.15 val PER: 0.1745
2025-07-30 03:15:06,433: t15.2024.03.17 val PER: 0.0725
2025-07-30 03:15:06,434: t15.2024.05.10 val PER: 0.1100
2025-07-30 03:15:06,434: t15.2024.06.14 val PER: 0.1041
2025-07-30 03:15:06,434: t15.2024.07.19 val PER: 0.1523
2025-07-30 03:15:06,434: t15.2024.07.21 val PER: 0.0545
2025-07-30 03:15:06,434: t15.2024.07.28 val PER: 0.0904
2025-07-30 03:15:06,434: t15.2025.01.10 val PER: 0.2383
2025-07-30 03:15:06,435: t15.2025.01.12 val PER: 0.0808
2025-07-30 03:15:06,435: t15.2025.03.14 val PER: 0.3299
2025-07-30 03:15:06,435: t15.2025.03.16 val PER: 0.1466
2025-07-30 03:15:06,435: t15.2025.03.30 val PER: 0.2172
2025-07-30 03:15:06,435: t15.2025.04.13 val PER: 0.1883
2025-07-30 03:15:06,435: New best test PER 0.1031 --> 0.1023
2025-07-30 03:15:06,435: Checkpointing model
2025-07-30 03:15:07,625: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-07-30 03:15:37,744: Train batch 82200: loss: 0.00 grad norm: 0.09 time: 0.086
2025-07-30 03:16:08,252: Train batch 82400: loss: 0.03 grad norm: 2.30 time: 0.123
2025-07-30 03:16:38,601: Train batch 82600: loss: 0.03 grad norm: 2.37 time: 0.082
2025-07-30 03:17:09,161: Train batch 82800: loss: 0.01 grad norm: 1.58 time: 0.118
2025-07-30 03:17:39,521: Train batch 83000: loss: 0.05 grad norm: 6.08 time: 0.122
2025-07-30 03:18:09,977: Train batch 83200: loss: 0.02 grad norm: 1.34 time: 0.096
2025-07-30 03:18:40,577: Train batch 83400: loss: 0.02 grad norm: 0.69 time: 0.109
2025-07-30 03:19:10,687: Train batch 83600: loss: 0.01 grad norm: 0.93 time: 0.089
2025-07-30 03:19:41,647: Train batch 83800: loss: 0.09 grad norm: 17.55 time: 0.116
2025-07-30 03:20:12,097: Train batch 84000: loss: 0.01 grad norm: 1.26 time: 0.079
2025-07-30 03:20:12,097: Running test after training batch: 84000
2025-07-30 03:20:25,567: Val batch 84000: PER (avg): 0.1034 CTC Loss (avg): 22.7766 time: 13.470
2025-07-30 03:20:25,568: t15.2023.08.13 val PER: 0.0728
2025-07-30 03:20:25,568: t15.2023.08.18 val PER: 0.0604
2025-07-30 03:20:25,568: t15.2023.08.20 val PER: 0.0492
2025-07-30 03:20:25,568: t15.2023.08.25 val PER: 0.0873
2025-07-30 03:20:25,568: t15.2023.08.27 val PER: 0.1447
2025-07-30 03:20:25,568: t15.2023.09.01 val PER: 0.0446
2025-07-30 03:20:25,568: t15.2023.09.03 val PER: 0.1140
2025-07-30 03:20:25,568: t15.2023.09.24 val PER: 0.0643
2025-07-30 03:20:25,569: t15.2023.09.29 val PER: 0.0957
2025-07-30 03:20:25,569: t15.2023.10.01 val PER: 0.1301
2025-07-30 03:20:25,569: t15.2023.10.06 val PER: 0.0571
2025-07-30 03:20:25,569: t15.2023.10.08 val PER: 0.1773
2025-07-30 03:20:25,569: t15.2023.10.13 val PER: 0.1435
2025-07-30 03:20:25,569: t15.2023.10.15 val PER: 0.1074
2025-07-30 03:20:25,569: t15.2023.10.20 val PER: 0.1611
2025-07-30 03:20:25,569: t15.2023.10.22 val PER: 0.1036
2025-07-30 03:20:25,570: t15.2023.11.03 val PER: 0.1493
2025-07-30 03:20:25,570: t15.2023.11.04 val PER: 0.0137
2025-07-30 03:20:25,570: t15.2023.11.17 val PER: 0.0280
2025-07-30 03:20:25,570: t15.2023.11.19 val PER: 0.0220
2025-07-30 03:20:25,570: t15.2023.11.26 val PER: 0.0449
2025-07-30 03:20:25,570: t15.2023.12.03 val PER: 0.0452
2025-07-30 03:20:25,570: t15.2023.12.08 val PER: 0.0340
2025-07-30 03:20:25,570: t15.2023.12.10 val PER: 0.0250
2025-07-30 03:20:25,570: t15.2023.12.17 val PER: 0.0738
2025-07-30 03:20:25,571: t15.2023.12.29 val PER: 0.0686
2025-07-30 03:20:25,571: t15.2024.02.25 val PER: 0.0772
2025-07-30 03:20:25,571: t15.2024.03.08 val PER: 0.1778
2025-07-30 03:20:25,571: t15.2024.03.15 val PER: 0.1826
2025-07-30 03:20:25,571: t15.2024.03.17 val PER: 0.0732
2025-07-30 03:20:25,571: t15.2024.05.10 val PER: 0.1100
2025-07-30 03:20:25,571: t15.2024.06.14 val PER: 0.1073
2025-07-30 03:20:25,571: t15.2024.07.19 val PER: 0.1575
2025-07-30 03:20:25,572: t15.2024.07.21 val PER: 0.0524
2025-07-30 03:20:25,572: t15.2024.07.28 val PER: 0.0853
2025-07-30 03:20:25,572: t15.2025.01.10 val PER: 0.2424
2025-07-30 03:20:25,572: t15.2025.01.12 val PER: 0.0908
2025-07-30 03:20:25,572: t15.2025.03.14 val PER: 0.3240
2025-07-30 03:20:25,572: t15.2025.03.16 val PER: 0.1545
2025-07-30 03:20:25,572: t15.2025.03.30 val PER: 0.2172
2025-07-30 03:20:25,572: t15.2025.04.13 val PER: 0.1897
2025-07-30 03:20:55,075: Train batch 84200: loss: 0.01 grad norm: 4.02 time: 0.083
2025-07-30 03:21:24,957: Train batch 84400: loss: 0.01 grad norm: 0.96 time: 0.089
2025-07-30 03:21:55,471: Train batch 84600: loss: 0.01 grad norm: 3.27 time: 0.092
2025-07-30 03:22:25,361: Train batch 84800: loss: 0.03 grad norm: 2.61 time: 0.078
2025-07-30 03:22:55,307: Train batch 85000: loss: 0.01 grad norm: 0.35 time: 0.090
2025-07-30 03:23:25,185: Train batch 85200: loss: 0.08 grad norm: 13.86 time: 0.092
2025-07-30 03:23:56,084: Train batch 85400: loss: 0.03 grad norm: 3.89 time: 0.122
2025-07-30 03:24:26,363: Train batch 85600: loss: 0.03 grad norm: 2.32 time: 0.115
2025-07-30 03:24:56,687: Train batch 85800: loss: 0.01 grad norm: 1.04 time: 0.092
2025-07-30 03:25:26,784: Train batch 86000: loss: 0.01 grad norm: 0.93 time: 0.074
2025-07-30 03:25:26,784: Running test after training batch: 86000
2025-07-30 03:25:40,025: Val batch 86000: PER (avg): 0.1035 CTC Loss (avg): 22.7672 time: 13.240
2025-07-30 03:25:40,025: t15.2023.08.13 val PER: 0.0759
2025-07-30 03:25:40,025: t15.2023.08.18 val PER: 0.0595
2025-07-30 03:25:40,025: t15.2023.08.20 val PER: 0.0492
2025-07-30 03:25:40,025: t15.2023.08.25 val PER: 0.0858
2025-07-30 03:25:40,026: t15.2023.08.27 val PER: 0.1511
2025-07-30 03:25:40,026: t15.2023.09.01 val PER: 0.0463
2025-07-30 03:25:40,026: t15.2023.09.03 val PER: 0.1176
2025-07-30 03:25:40,026: t15.2023.09.24 val PER: 0.0704
2025-07-30 03:25:40,026: t15.2023.09.29 val PER: 0.0964
2025-07-30 03:25:40,026: t15.2023.10.01 val PER: 0.1262
2025-07-30 03:25:40,026: t15.2023.10.06 val PER: 0.0646
2025-07-30 03:25:40,026: t15.2023.10.08 val PER: 0.1773
2025-07-30 03:25:40,026: t15.2023.10.13 val PER: 0.1350
2025-07-30 03:25:40,026: t15.2023.10.15 val PER: 0.1154
2025-07-30 03:25:40,027: t15.2023.10.20 val PER: 0.1678
2025-07-30 03:25:40,027: t15.2023.10.22 val PER: 0.0958
2025-07-30 03:25:40,027: t15.2023.11.03 val PER: 0.1493
2025-07-30 03:25:40,027: t15.2023.11.04 val PER: 0.0171
2025-07-30 03:25:40,027: t15.2023.11.17 val PER: 0.0233
2025-07-30 03:25:40,027: t15.2023.11.19 val PER: 0.0200
2025-07-30 03:25:40,027: t15.2023.11.26 val PER: 0.0493
2025-07-30 03:25:40,027: t15.2023.12.03 val PER: 0.0452
2025-07-30 03:25:40,027: t15.2023.12.08 val PER: 0.0366
2025-07-30 03:25:40,027: t15.2023.12.10 val PER: 0.0237
2025-07-30 03:25:40,028: t15.2023.12.17 val PER: 0.0686
2025-07-30 03:25:40,028: t15.2023.12.29 val PER: 0.0679
2025-07-30 03:25:40,028: t15.2024.02.25 val PER: 0.0744
2025-07-30 03:25:40,028: t15.2024.03.08 val PER: 0.1821
2025-07-30 03:25:40,028: t15.2024.03.15 val PER: 0.1857
2025-07-30 03:25:40,028: t15.2024.03.17 val PER: 0.0704
2025-07-30 03:25:40,028: t15.2024.05.10 val PER: 0.1144
2025-07-30 03:25:40,028: t15.2024.06.14 val PER: 0.1025
2025-07-30 03:25:40,028: t15.2024.07.19 val PER: 0.1569
2025-07-30 03:25:40,029: t15.2024.07.21 val PER: 0.0552
2025-07-30 03:25:40,029: t15.2024.07.28 val PER: 0.0846
2025-07-30 03:25:40,029: t15.2025.01.10 val PER: 0.2355
2025-07-30 03:25:40,029: t15.2025.01.12 val PER: 0.0955
2025-07-30 03:25:40,029: t15.2025.03.14 val PER: 0.3195
2025-07-30 03:25:40,029: t15.2025.03.16 val PER: 0.1427
2025-07-30 03:25:40,029: t15.2025.03.30 val PER: 0.2218
2025-07-30 03:25:40,029: t15.2025.04.13 val PER: 0.1869
2025-07-30 03:26:09,851: Train batch 86200: loss: 0.01 grad norm: 1.04 time: 0.092
2025-07-30 03:26:40,008: Train batch 86400: loss: 0.05 grad norm: 3.75 time: 0.081
2025-07-30 03:27:10,401: Train batch 86600: loss: 0.04 grad norm: 3.97 time: 0.109
2025-07-30 03:27:40,646: Train batch 86800: loss: 0.00 grad norm: 0.13 time: 0.105
2025-07-30 03:28:10,965: Train batch 87000: loss: 0.07 grad norm: 4.56 time: 0.077
2025-07-30 03:28:41,273: Train batch 87200: loss: 0.02 grad norm: 1.42 time: 0.082
2025-07-30 03:29:11,026: Train batch 87400: loss: 0.01 grad norm: 0.31 time: 0.078
2025-07-30 03:29:41,505: Train batch 87600: loss: 0.06 grad norm: 3.25 time: 0.125
2025-07-30 03:30:11,878: Train batch 87800: loss: 0.01 grad norm: 1.15 time: 0.099
2025-07-30 03:30:41,808: Train batch 88000: loss: 0.00 grad norm: 0.13 time: 0.076
2025-07-30 03:30:41,808: Running test after training batch: 88000
2025-07-30 03:30:56,129: Val batch 88000: PER (avg): 0.1016 CTC Loss (avg): 22.7090 time: 14.321
2025-07-30 03:30:56,130: t15.2023.08.13 val PER: 0.0728
2025-07-30 03:30:56,130: t15.2023.08.18 val PER: 0.0612
2025-07-30 03:30:56,130: t15.2023.08.20 val PER: 0.0508
2025-07-30 03:30:56,130: t15.2023.08.25 val PER: 0.0768
2025-07-30 03:30:56,130: t15.2023.08.27 val PER: 0.1447
2025-07-30 03:30:56,130: t15.2023.09.01 val PER: 0.0471
2025-07-30 03:30:56,130: t15.2023.09.03 val PER: 0.1093
2025-07-30 03:30:56,130: t15.2023.09.24 val PER: 0.0728
2025-07-30 03:30:56,131: t15.2023.09.29 val PER: 0.0919
2025-07-30 03:30:56,131: t15.2023.10.01 val PER: 0.1268
2025-07-30 03:30:56,131: t15.2023.10.06 val PER: 0.0581
2025-07-30 03:30:56,131: t15.2023.10.08 val PER: 0.1678
2025-07-30 03:30:56,131: t15.2023.10.13 val PER: 0.1334
2025-07-30 03:30:56,131: t15.2023.10.15 val PER: 0.1114
2025-07-30 03:30:56,131: t15.2023.10.20 val PER: 0.1678
2025-07-30 03:30:56,131: t15.2023.10.22 val PER: 0.0935
2025-07-30 03:30:56,131: t15.2023.11.03 val PER: 0.1472
2025-07-30 03:30:56,132: t15.2023.11.04 val PER: 0.0137
2025-07-30 03:30:56,132: t15.2023.11.17 val PER: 0.0280
2025-07-30 03:30:56,132: t15.2023.11.19 val PER: 0.0160
2025-07-30 03:30:56,132: t15.2023.11.26 val PER: 0.0435
2025-07-30 03:30:56,132: t15.2023.12.03 val PER: 0.0452
2025-07-30 03:30:56,132: t15.2023.12.08 val PER: 0.0326
2025-07-30 03:30:56,132: t15.2023.12.10 val PER: 0.0210
2025-07-30 03:30:56,132: t15.2023.12.17 val PER: 0.0686
2025-07-30 03:30:56,133: t15.2023.12.29 val PER: 0.0686
2025-07-30 03:30:56,133: t15.2024.02.25 val PER: 0.0744
2025-07-30 03:30:56,133: t15.2024.03.08 val PER: 0.1764
2025-07-30 03:30:56,133: t15.2024.03.15 val PER: 0.1814
2025-07-30 03:30:56,133: t15.2024.03.17 val PER: 0.0697
2025-07-30 03:30:56,133: t15.2024.05.10 val PER: 0.1144
2025-07-30 03:30:56,133: t15.2024.06.14 val PER: 0.1009
2025-07-30 03:30:56,133: t15.2024.07.19 val PER: 0.1536
2025-07-30 03:30:56,133: t15.2024.07.21 val PER: 0.0503
2025-07-30 03:30:56,134: t15.2024.07.28 val PER: 0.0846
2025-07-30 03:30:56,134: t15.2025.01.10 val PER: 0.2369
2025-07-30 03:30:56,134: t15.2025.01.12 val PER: 0.0870
2025-07-30 03:30:56,134: t15.2025.03.14 val PER: 0.3284
2025-07-30 03:30:56,134: t15.2025.03.16 val PER: 0.1505
2025-07-30 03:30:56,134: t15.2025.03.30 val PER: 0.2230
2025-07-30 03:30:56,134: t15.2025.04.13 val PER: 0.1869
2025-07-30 03:30:56,134: New best test PER 0.1023 --> 0.1016
2025-07-30 03:30:56,134: Checkpointing model
2025-07-30 03:30:57,315: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-07-30 03:31:26,970: Train batch 88200: loss: 0.02 grad norm: 3.34 time: 0.080
2025-07-30 03:31:57,766: Train batch 88400: loss: 0.07 grad norm: 4.66 time: 0.096
2025-07-30 03:32:27,888: Train batch 88600: loss: 0.04 grad norm: 3.07 time: 0.112
2025-07-30 03:32:58,034: Train batch 88800: loss: 0.01 grad norm: 0.47 time: 0.076
2025-07-30 03:33:28,750: Train batch 89000: loss: 0.01 grad norm: 0.46 time: 0.091
2025-07-30 03:33:59,107: Train batch 89200: loss: 0.01 grad norm: 0.77 time: 0.102
2025-07-30 03:34:28,826: Train batch 89400: loss: 0.04 grad norm: 4.01 time: 0.092
2025-07-30 03:34:58,887: Train batch 89600: loss: 0.01 grad norm: 5.06 time: 0.089
2025-07-30 03:35:28,967: Train batch 89800: loss: 0.01 grad norm: 0.67 time: 0.088
2025-07-30 03:35:59,167: Train batch 90000: loss: 0.02 grad norm: 2.39 time: 0.090
2025-07-30 03:35:59,167: Running test after training batch: 90000
2025-07-30 03:36:12,782: Val batch 90000: PER (avg): 0.1035 CTC Loss (avg): 22.6082 time: 13.615
2025-07-30 03:36:12,783: t15.2023.08.13 val PER: 0.0717
2025-07-30 03:36:12,783: t15.2023.08.18 val PER: 0.0629
2025-07-30 03:36:12,783: t15.2023.08.20 val PER: 0.0516
2025-07-30 03:36:12,783: t15.2023.08.25 val PER: 0.0783
2025-07-30 03:36:12,783: t15.2023.08.27 val PER: 0.1479
2025-07-30 03:36:12,783: t15.2023.09.01 val PER: 0.0487
2025-07-30 03:36:12,783: t15.2023.09.03 val PER: 0.1188
2025-07-30 03:36:12,783: t15.2023.09.24 val PER: 0.0655
2025-07-30 03:36:12,784: t15.2023.09.29 val PER: 0.0938
2025-07-30 03:36:12,784: t15.2023.10.01 val PER: 0.1328
2025-07-30 03:36:12,784: t15.2023.10.06 val PER: 0.0635
2025-07-30 03:36:12,784: t15.2023.10.08 val PER: 0.1705
2025-07-30 03:36:12,784: t15.2023.10.13 val PER: 0.1412
2025-07-30 03:36:12,784: t15.2023.10.15 val PER: 0.1094
2025-07-30 03:36:12,784: t15.2023.10.20 val PER: 0.1611
2025-07-30 03:36:12,784: t15.2023.10.22 val PER: 0.0935
2025-07-30 03:36:12,784: t15.2023.11.03 val PER: 0.1526
2025-07-30 03:36:12,785: t15.2023.11.04 val PER: 0.0137
2025-07-30 03:36:12,785: t15.2023.11.17 val PER: 0.0264
2025-07-30 03:36:12,785: t15.2023.11.19 val PER: 0.0140
2025-07-30 03:36:12,785: t15.2023.11.26 val PER: 0.0442
2025-07-30 03:36:12,785: t15.2023.12.03 val PER: 0.0494
2025-07-30 03:36:12,785: t15.2023.12.08 val PER: 0.0333
2025-07-30 03:36:12,785: t15.2023.12.10 val PER: 0.0250
2025-07-30 03:36:12,785: t15.2023.12.17 val PER: 0.0728
2025-07-30 03:36:12,786: t15.2023.12.29 val PER: 0.0679
2025-07-30 03:36:12,786: t15.2024.02.25 val PER: 0.0730
2025-07-30 03:36:12,786: t15.2024.03.08 val PER: 0.1878
2025-07-30 03:36:12,786: t15.2024.03.15 val PER: 0.1789
2025-07-30 03:36:12,786: t15.2024.03.17 val PER: 0.0704
2025-07-30 03:36:12,786: t15.2024.05.10 val PER: 0.1129
2025-07-30 03:36:12,786: t15.2024.06.14 val PER: 0.1025
2025-07-30 03:36:12,786: t15.2024.07.19 val PER: 0.1562
2025-07-30 03:36:12,787: t15.2024.07.21 val PER: 0.0531
2025-07-30 03:36:12,787: t15.2024.07.28 val PER: 0.0890
2025-07-30 03:36:12,787: t15.2025.01.10 val PER: 0.2424
2025-07-30 03:36:12,787: t15.2025.01.12 val PER: 0.0885
2025-07-30 03:36:12,787: t15.2025.03.14 val PER: 0.3254
2025-07-30 03:36:12,787: t15.2025.03.16 val PER: 0.1649
2025-07-30 03:36:12,787: t15.2025.03.30 val PER: 0.2184
2025-07-30 03:36:12,787: t15.2025.04.13 val PER: 0.1883
2025-07-30 03:36:42,614: Train batch 90200: loss: 0.01 grad norm: 0.37 time: 0.087
2025-07-30 03:37:12,854: Train batch 90400: loss: 0.01 grad norm: 1.17 time: 0.108
2025-07-30 03:37:43,098: Train batch 90600: loss: 0.02 grad norm: 1.88 time: 0.094
2025-07-30 03:38:13,048: Train batch 90800: loss: 0.03 grad norm: 1.89 time: 0.073
2025-07-30 03:38:43,072: Train batch 91000: loss: 0.01 grad norm: 0.56 time: 0.086
2025-07-30 03:39:13,343: Train batch 91200: loss: 0.06 grad norm: 4.91 time: 0.091
2025-07-30 03:39:43,722: Train batch 91400: loss: 0.03 grad norm: 2.22 time: 0.084
2025-07-30 03:40:13,844: Train batch 91600: loss: 0.05 grad norm: 3.80 time: 0.087
2025-07-30 03:40:43,877: Train batch 91800: loss: 0.01 grad norm: 1.03 time: 0.130
2025-07-30 03:41:13,414: Train batch 92000: loss: 0.07 grad norm: 3.50 time: 0.078
2025-07-30 03:41:13,414: Running test after training batch: 92000
2025-07-30 03:41:27,180: Val batch 92000: PER (avg): 0.1024 CTC Loss (avg): 22.7290 time: 13.766
2025-07-30 03:41:27,181: t15.2023.08.13 val PER: 0.0696
2025-07-30 03:41:27,181: t15.2023.08.18 val PER: 0.0604
2025-07-30 03:41:27,181: t15.2023.08.20 val PER: 0.0469
2025-07-30 03:41:27,181: t15.2023.08.25 val PER: 0.0783
2025-07-30 03:41:27,181: t15.2023.08.27 val PER: 0.1576
2025-07-30 03:41:27,181: t15.2023.09.01 val PER: 0.0455
2025-07-30 03:41:27,181: t15.2023.09.03 val PER: 0.1164
2025-07-30 03:41:27,181: t15.2023.09.24 val PER: 0.0728
2025-07-30 03:41:27,182: t15.2023.09.29 val PER: 0.0900
2025-07-30 03:41:27,182: t15.2023.10.01 val PER: 0.1268
2025-07-30 03:41:27,182: t15.2023.10.06 val PER: 0.0592
2025-07-30 03:41:27,182: t15.2023.10.08 val PER: 0.1691
2025-07-30 03:41:27,182: t15.2023.10.13 val PER: 0.1365
2025-07-30 03:41:27,182: t15.2023.10.15 val PER: 0.1101
2025-07-30 03:41:27,182: t15.2023.10.20 val PER: 0.1544
2025-07-30 03:41:27,182: t15.2023.10.22 val PER: 0.0947
2025-07-30 03:41:27,182: t15.2023.11.03 val PER: 0.1526
2025-07-30 03:41:27,182: t15.2023.11.04 val PER: 0.0171
2025-07-30 03:41:27,183: t15.2023.11.17 val PER: 0.0249
2025-07-30 03:41:27,183: t15.2023.11.19 val PER: 0.0160
2025-07-30 03:41:27,183: t15.2023.11.26 val PER: 0.0471
2025-07-30 03:41:27,183: t15.2023.12.03 val PER: 0.0452
2025-07-30 03:41:27,183: t15.2023.12.08 val PER: 0.0346
2025-07-30 03:41:27,183: t15.2023.12.10 val PER: 0.0250
2025-07-30 03:41:27,183: t15.2023.12.17 val PER: 0.0748
2025-07-30 03:41:27,183: t15.2023.12.29 val PER: 0.0700
2025-07-30 03:41:27,184: t15.2024.02.25 val PER: 0.0744
2025-07-30 03:41:27,184: t15.2024.03.08 val PER: 0.1821
2025-07-30 03:41:27,184: t15.2024.03.15 val PER: 0.1757
2025-07-30 03:41:27,184: t15.2024.03.17 val PER: 0.0711
2025-07-30 03:41:27,184: t15.2024.05.10 val PER: 0.1189
2025-07-30 03:41:27,184: t15.2024.06.14 val PER: 0.1025
2025-07-30 03:41:27,184: t15.2024.07.19 val PER: 0.1556
2025-07-30 03:41:27,184: t15.2024.07.21 val PER: 0.0510
2025-07-30 03:41:27,184: t15.2024.07.28 val PER: 0.0882
2025-07-30 03:41:27,185: t15.2025.01.10 val PER: 0.2397
2025-07-30 03:41:27,185: t15.2025.01.12 val PER: 0.0901
2025-07-30 03:41:27,185: t15.2025.03.14 val PER: 0.3166
2025-07-30 03:41:27,185: t15.2025.03.16 val PER: 0.1545
2025-07-30 03:41:27,185: t15.2025.03.30 val PER: 0.2218
2025-07-30 03:41:27,185: t15.2025.04.13 val PER: 0.1812
2025-07-30 03:41:57,063: Train batch 92200: loss: 0.01 grad norm: 0.26 time: 0.126
2025-07-30 03:42:27,369: Train batch 92400: loss: 0.01 grad norm: 0.31 time: 0.096
2025-07-30 03:42:57,780: Train batch 92600: loss: 0.03 grad norm: 1.55 time: 0.100
2025-07-30 03:43:28,387: Train batch 92800: loss: 0.01 grad norm: 0.36 time: 0.106
2025-07-30 03:43:59,001: Train batch 93000: loss: 0.01 grad norm: 0.33 time: 0.100
2025-07-30 03:44:29,744: Train batch 93200: loss: 0.02 grad norm: 2.75 time: 0.113
2025-07-30 03:45:00,377: Train batch 93400: loss: 0.01 grad norm: 0.39 time: 0.128
2025-07-30 03:45:30,032: Train batch 93600: loss: 0.01 grad norm: 0.32 time: 0.075
2025-07-30 03:46:00,203: Train batch 93800: loss: 0.02 grad norm: 4.39 time: 0.089
2025-07-30 03:46:30,135: Train batch 94000: loss: 0.01 grad norm: 0.93 time: 0.092
2025-07-30 03:46:30,135: Running test after training batch: 94000
2025-07-30 03:46:43,287: Val batch 94000: PER (avg): 0.1020 CTC Loss (avg): 22.5971 time: 13.152
2025-07-30 03:46:43,288: t15.2023.08.13 val PER: 0.0686
2025-07-30 03:46:43,288: t15.2023.08.18 val PER: 0.0570
2025-07-30 03:46:43,288: t15.2023.08.20 val PER: 0.0508
2025-07-30 03:46:43,288: t15.2023.08.25 val PER: 0.0798
2025-07-30 03:46:43,288: t15.2023.08.27 val PER: 0.1511
2025-07-30 03:46:43,288: t15.2023.09.01 val PER: 0.0471
2025-07-30 03:46:43,288: t15.2023.09.03 val PER: 0.1152
2025-07-30 03:46:43,288: t15.2023.09.24 val PER: 0.0716
2025-07-30 03:46:43,289: t15.2023.09.29 val PER: 0.0900
2025-07-30 03:46:43,289: t15.2023.10.01 val PER: 0.1268
2025-07-30 03:46:43,289: t15.2023.10.06 val PER: 0.0603
2025-07-30 03:46:43,289: t15.2023.10.08 val PER: 0.1691
2025-07-30 03:46:43,289: t15.2023.10.13 val PER: 0.1358
2025-07-30 03:46:43,289: t15.2023.10.15 val PER: 0.1101
2025-07-30 03:46:43,289: t15.2023.10.20 val PER: 0.1544
2025-07-30 03:46:43,289: t15.2023.10.22 val PER: 0.0958
2025-07-30 03:46:43,289: t15.2023.11.03 val PER: 0.1513
2025-07-30 03:46:43,289: t15.2023.11.04 val PER: 0.0171
2025-07-30 03:46:43,290: t15.2023.11.17 val PER: 0.0233
2025-07-30 03:46:43,290: t15.2023.11.19 val PER: 0.0160
2025-07-30 03:46:43,290: t15.2023.11.26 val PER: 0.0449
2025-07-30 03:46:43,290: t15.2023.12.03 val PER: 0.0462
2025-07-30 03:46:43,290: t15.2023.12.08 val PER: 0.0346
2025-07-30 03:46:43,290: t15.2023.12.10 val PER: 0.0263
2025-07-30 03:46:43,290: t15.2023.12.17 val PER: 0.0686
2025-07-30 03:46:43,290: t15.2023.12.29 val PER: 0.0659
2025-07-30 03:46:43,291: t15.2024.02.25 val PER: 0.0758
2025-07-30 03:46:43,291: t15.2024.03.08 val PER: 0.1792
2025-07-30 03:46:43,291: t15.2024.03.15 val PER: 0.1782
2025-07-30 03:46:43,291: t15.2024.03.17 val PER: 0.0718
2025-07-30 03:46:43,291: t15.2024.05.10 val PER: 0.1248
2025-07-30 03:46:43,291: t15.2024.06.14 val PER: 0.0978
2025-07-30 03:46:43,291: t15.2024.07.19 val PER: 0.1510
2025-07-30 03:46:43,291: t15.2024.07.21 val PER: 0.0476
2025-07-30 03:46:43,292: t15.2024.07.28 val PER: 0.0912
2025-07-30 03:46:43,292: t15.2025.01.10 val PER: 0.2424
2025-07-30 03:46:43,292: t15.2025.01.12 val PER: 0.0893
2025-07-30 03:46:43,292: t15.2025.03.14 val PER: 0.3180
2025-07-30 03:46:43,292: t15.2025.03.16 val PER: 0.1571
2025-07-30 03:46:43,292: t15.2025.03.30 val PER: 0.2195
2025-07-30 03:46:43,292: t15.2025.04.13 val PER: 0.1869
2025-07-30 03:47:12,872: Train batch 94200: loss: 0.02 grad norm: 0.88 time: 0.090
2025-07-30 03:47:42,809: Train batch 94400: loss: 0.01 grad norm: 0.57 time: 0.077
2025-07-30 03:48:13,686: Train batch 94600: loss: 0.03 grad norm: 3.45 time: 0.127
2025-07-30 03:48:44,181: Train batch 94800: loss: 0.02 grad norm: 1.44 time: 0.078
2025-07-30 03:49:14,702: Train batch 95000: loss: 0.01 grad norm: 0.51 time: 0.089
2025-07-30 03:49:44,803: Train batch 95200: loss: 0.01 grad norm: 0.56 time: 0.112
2025-07-30 03:50:15,341: Train batch 95400: loss: 0.01 grad norm: 0.60 time: 0.114
2025-07-30 03:50:45,529: Train batch 95600: loss: 0.01 grad norm: 0.45 time: 0.116
2025-07-30 03:51:15,942: Train batch 95800: loss: 0.02 grad norm: 1.59 time: 0.105
2025-07-30 03:51:45,928: Train batch 96000: loss: 0.02 grad norm: 0.69 time: 0.087
2025-07-30 03:51:45,928: Running test after training batch: 96000
2025-07-30 03:51:59,414: Val batch 96000: PER (avg): 0.1021 CTC Loss (avg): 22.7900 time: 13.486
2025-07-30 03:51:59,414: t15.2023.08.13 val PER: 0.0676
2025-07-30 03:51:59,414: t15.2023.08.18 val PER: 0.0587
2025-07-30 03:51:59,415: t15.2023.08.20 val PER: 0.0516
2025-07-30 03:51:59,415: t15.2023.08.25 val PER: 0.0828
2025-07-30 03:51:59,415: t15.2023.08.27 val PER: 0.1511
2025-07-30 03:51:59,415: t15.2023.09.01 val PER: 0.0487
2025-07-30 03:51:59,415: t15.2023.09.03 val PER: 0.1116
2025-07-30 03:51:59,415: t15.2023.09.24 val PER: 0.0752
2025-07-30 03:51:59,415: t15.2023.09.29 val PER: 0.0925
2025-07-30 03:51:59,415: t15.2023.10.01 val PER: 0.1268
2025-07-30 03:51:59,415: t15.2023.10.06 val PER: 0.0603
2025-07-30 03:51:59,415: t15.2023.10.08 val PER: 0.1705
2025-07-30 03:51:59,416: t15.2023.10.13 val PER: 0.1373
2025-07-30 03:51:59,416: t15.2023.10.15 val PER: 0.1154
2025-07-30 03:51:59,416: t15.2023.10.20 val PER: 0.1577
2025-07-30 03:51:59,416: t15.2023.10.22 val PER: 0.0969
2025-07-30 03:51:59,416: t15.2023.11.03 val PER: 0.1499
2025-07-30 03:51:59,416: t15.2023.11.04 val PER: 0.0171
2025-07-30 03:51:59,416: t15.2023.11.17 val PER: 0.0249
2025-07-30 03:51:59,416: t15.2023.11.19 val PER: 0.0140
2025-07-30 03:51:59,416: t15.2023.11.26 val PER: 0.0471
2025-07-30 03:51:59,417: t15.2023.12.03 val PER: 0.0483
2025-07-30 03:51:59,417: t15.2023.12.08 val PER: 0.0346
2025-07-30 03:51:59,417: t15.2023.12.10 val PER: 0.0223
2025-07-30 03:51:59,417: t15.2023.12.17 val PER: 0.0665
2025-07-30 03:51:59,417: t15.2023.12.29 val PER: 0.0666
2025-07-30 03:51:59,417: t15.2024.02.25 val PER: 0.0787
2025-07-30 03:51:59,417: t15.2024.03.08 val PER: 0.1892
2025-07-30 03:51:59,417: t15.2024.03.15 val PER: 0.1751
2025-07-30 03:51:59,417: t15.2024.03.17 val PER: 0.0704
2025-07-30 03:51:59,418: t15.2024.05.10 val PER: 0.1159
2025-07-30 03:51:59,418: t15.2024.06.14 val PER: 0.1041
2025-07-30 03:51:59,418: t15.2024.07.19 val PER: 0.1543
2025-07-30 03:51:59,418: t15.2024.07.21 val PER: 0.0490
2025-07-30 03:51:59,418: t15.2024.07.28 val PER: 0.0875
2025-07-30 03:51:59,418: t15.2025.01.10 val PER: 0.2410
2025-07-30 03:51:59,418: t15.2025.01.12 val PER: 0.0831
2025-07-30 03:51:59,418: t15.2025.03.14 val PER: 0.3210
2025-07-30 03:51:59,419: t15.2025.03.16 val PER: 0.1492
2025-07-30 03:51:59,419: t15.2025.03.30 val PER: 0.2184
2025-07-30 03:51:59,419: t15.2025.04.13 val PER: 0.1769
2025-07-30 03:52:29,239: Train batch 96200: loss: 0.03 grad norm: 2.12 time: 0.072
2025-07-30 03:52:59,212: Train batch 96400: loss: 0.00 grad norm: 0.19 time: 0.108
2025-07-30 03:53:29,714: Train batch 96600: loss: 0.02 grad norm: 0.89 time: 0.118
2025-07-30 03:53:59,896: Train batch 96800: loss: 0.01 grad norm: 0.77 time: 0.116
2025-07-30 03:54:29,995: Train batch 97000: loss: 0.01 grad norm: 0.63 time: 0.070
2025-07-30 03:55:00,558: Train batch 97200: loss: 0.01 grad norm: 0.36 time: 0.095
2025-07-30 03:55:30,748: Train batch 97400: loss: 0.01 grad norm: 0.52 time: 0.113
2025-07-30 03:56:00,852: Train batch 97600: loss: 0.00 grad norm: 0.43 time: 0.098
2025-07-30 03:56:31,186: Train batch 97800: loss: 0.00 grad norm: 0.40 time: 0.096
2025-07-30 03:57:01,825: Train batch 98000: loss: 0.01 grad norm: 0.76 time: 0.100
2025-07-30 03:57:01,826: Running test after training batch: 98000
2025-07-30 03:57:15,798: Val batch 98000: PER (avg): 0.1017 CTC Loss (avg): 22.6471 time: 13.973
2025-07-30 03:57:15,799: t15.2023.08.13 val PER: 0.0707
2025-07-30 03:57:15,799: t15.2023.08.18 val PER: 0.0578
2025-07-30 03:57:15,799: t15.2023.08.20 val PER: 0.0492
2025-07-30 03:57:15,799: t15.2023.08.25 val PER: 0.0798
2025-07-30 03:57:15,799: t15.2023.08.27 val PER: 0.1431
2025-07-30 03:57:15,799: t15.2023.09.01 val PER: 0.0463
2025-07-30 03:57:15,799: t15.2023.09.03 val PER: 0.1140
2025-07-30 03:57:15,799: t15.2023.09.24 val PER: 0.0692
2025-07-30 03:57:15,800: t15.2023.09.29 val PER: 0.0944
2025-07-30 03:57:15,800: t15.2023.10.01 val PER: 0.1295
2025-07-30 03:57:15,800: t15.2023.10.06 val PER: 0.0571
2025-07-30 03:57:15,800: t15.2023.10.08 val PER: 0.1719
2025-07-30 03:57:15,800: t15.2023.10.13 val PER: 0.1396
2025-07-30 03:57:15,800: t15.2023.10.15 val PER: 0.1121
2025-07-30 03:57:15,800: t15.2023.10.20 val PER: 0.1544
2025-07-30 03:57:15,800: t15.2023.10.22 val PER: 0.0980
2025-07-30 03:57:15,800: t15.2023.11.03 val PER: 0.1533
2025-07-30 03:57:15,801: t15.2023.11.04 val PER: 0.0205
2025-07-30 03:57:15,801: t15.2023.11.17 val PER: 0.0264
2025-07-30 03:57:15,801: t15.2023.11.19 val PER: 0.0140
2025-07-30 03:57:15,801: t15.2023.11.26 val PER: 0.0420
2025-07-30 03:57:15,801: t15.2023.12.03 val PER: 0.0483
2025-07-30 03:57:15,801: t15.2023.12.08 val PER: 0.0320
2025-07-30 03:57:15,801: t15.2023.12.10 val PER: 0.0237
2025-07-30 03:57:15,801: t15.2023.12.17 val PER: 0.0676
2025-07-30 03:57:15,802: t15.2023.12.29 val PER: 0.0721
2025-07-30 03:57:15,802: t15.2024.02.25 val PER: 0.0744
2025-07-30 03:57:15,802: t15.2024.03.08 val PER: 0.1778
2025-07-30 03:57:15,802: t15.2024.03.15 val PER: 0.1764
2025-07-30 03:57:15,802: t15.2024.03.17 val PER: 0.0718
2025-07-30 03:57:15,802: t15.2024.05.10 val PER: 0.1174
2025-07-30 03:57:15,802: t15.2024.06.14 val PER: 0.0994
2025-07-30 03:57:15,802: t15.2024.07.19 val PER: 0.1490
2025-07-30 03:57:15,802: t15.2024.07.21 val PER: 0.0510
2025-07-30 03:57:15,803: t15.2024.07.28 val PER: 0.0897
2025-07-30 03:57:15,803: t15.2025.01.10 val PER: 0.2397
2025-07-30 03:57:15,803: t15.2025.01.12 val PER: 0.0862
2025-07-30 03:57:15,803: t15.2025.03.14 val PER: 0.3136
2025-07-30 03:57:15,803: t15.2025.03.16 val PER: 0.1453
2025-07-30 03:57:15,803: t15.2025.03.30 val PER: 0.2161
2025-07-30 03:57:15,803: t15.2025.04.13 val PER: 0.1883
2025-07-30 03:57:45,649: Train batch 98200: loss: 0.01 grad norm: 0.59 time: 0.081
2025-07-30 03:58:16,375: Train batch 98400: loss: 0.01 grad norm: 0.46 time: 0.125
2025-07-30 03:58:46,715: Train batch 98600: loss: 0.01 grad norm: 0.24 time: 0.092
2025-07-30 03:59:17,039: Train batch 98800: loss: 0.01 grad norm: 0.52 time: 0.082
2025-07-30 03:59:46,831: Train batch 99000: loss: 0.01 grad norm: 1.64 time: 0.093
2025-07-30 04:00:17,232: Train batch 99200: loss: 0.03 grad norm: 1.63 time: 0.072
2025-07-30 04:00:47,162: Train batch 99400: loss: 0.01 grad norm: 0.65 time: 0.091
2025-07-30 04:01:17,113: Train batch 99600: loss: 0.03 grad norm: 2.90 time: 0.087
2025-07-30 04:01:46,915: Train batch 99800: loss: 0.01 grad norm: 1.07 time: 0.074
2025-07-30 04:02:17,401: Train batch 100000: loss: 0.01 grad norm: 4.57 time: 0.112
2025-07-30 04:02:17,402: Running test after training batch: 100000
2025-07-30 04:02:31,429: Val batch 100000: PER (avg): 0.1020 CTC Loss (avg): 22.5776 time: 14.027
2025-07-30 04:02:31,429: t15.2023.08.13 val PER: 0.0707
2025-07-30 04:02:31,429: t15.2023.08.18 val PER: 0.0595
2025-07-30 04:02:31,429: t15.2023.08.20 val PER: 0.0516
2025-07-30 04:02:31,430: t15.2023.08.25 val PER: 0.0828
2025-07-30 04:02:31,430: t15.2023.08.27 val PER: 0.1463
2025-07-30 04:02:31,430: t15.2023.09.01 val PER: 0.0471
2025-07-30 04:02:31,430: t15.2023.09.03 val PER: 0.1116
2025-07-30 04:02:31,430: t15.2023.09.24 val PER: 0.0740
2025-07-30 04:02:31,430: t15.2023.09.29 val PER: 0.0906
2025-07-30 04:02:31,430: t15.2023.10.01 val PER: 0.1301
2025-07-30 04:02:31,430: t15.2023.10.06 val PER: 0.0614
2025-07-30 04:02:31,430: t15.2023.10.08 val PER: 0.1664
2025-07-30 04:02:31,431: t15.2023.10.13 val PER: 0.1404
2025-07-30 04:02:31,431: t15.2023.10.15 val PER: 0.1101
2025-07-30 04:02:31,431: t15.2023.10.20 val PER: 0.1644
2025-07-30 04:02:31,431: t15.2023.10.22 val PER: 0.0913
2025-07-30 04:02:31,431: t15.2023.11.03 val PER: 0.1506
2025-07-30 04:02:31,431: t15.2023.11.04 val PER: 0.0171
2025-07-30 04:02:31,431: t15.2023.11.17 val PER: 0.0249
2025-07-30 04:02:31,431: t15.2023.11.19 val PER: 0.0140
2025-07-30 04:02:31,432: t15.2023.11.26 val PER: 0.0449
2025-07-30 04:02:31,432: t15.2023.12.03 val PER: 0.0473
2025-07-30 04:02:31,432: t15.2023.12.08 val PER: 0.0340
2025-07-30 04:02:31,432: t15.2023.12.10 val PER: 0.0250
2025-07-30 04:02:31,432: t15.2023.12.17 val PER: 0.0738
2025-07-30 04:02:31,432: t15.2023.12.29 val PER: 0.0721
2025-07-30 04:02:31,432: t15.2024.02.25 val PER: 0.0744
2025-07-30 04:02:31,432: t15.2024.03.08 val PER: 0.1721
2025-07-30 04:02:31,432: t15.2024.03.15 val PER: 0.1764
2025-07-30 04:02:31,433: t15.2024.03.17 val PER: 0.0725
2025-07-30 04:02:31,433: t15.2024.05.10 val PER: 0.1174
2025-07-30 04:02:31,433: t15.2024.06.14 val PER: 0.0978
2025-07-30 04:02:31,433: t15.2024.07.19 val PER: 0.1496
2025-07-30 04:02:31,433: t15.2024.07.21 val PER: 0.0538
2025-07-30 04:02:31,433: t15.2024.07.28 val PER: 0.0897
2025-07-30 04:02:31,433: t15.2025.01.10 val PER: 0.2383
2025-07-30 04:02:31,433: t15.2025.01.12 val PER: 0.0855
2025-07-30 04:02:31,434: t15.2025.03.14 val PER: 0.3195
2025-07-30 04:02:31,434: t15.2025.03.16 val PER: 0.1479
2025-07-30 04:02:31,434: t15.2025.03.30 val PER: 0.2184
2025-07-30 04:02:31,434: t15.2025.04.13 val PER: 0.1826
2025-07-30 04:03:01,140: Train batch 100200: loss: 0.11 grad norm: 8.73 time: 0.075
2025-07-30 04:03:31,238: Train batch 100400: loss: 0.02 grad norm: 1.69 time: 0.107
2025-07-30 04:04:01,176: Train batch 100600: loss: 0.01 grad norm: 0.40 time: 0.079
2025-07-30 04:04:31,657: Train batch 100800: loss: 0.05 grad norm: 9.34 time: 0.127
2025-07-30 04:05:01,818: Train batch 101000: loss: 0.06 grad norm: 3.99 time: 0.089
2025-07-30 04:05:32,538: Train batch 101200: loss: 0.02 grad norm: 0.61 time: 0.073
2025-07-30 04:06:02,728: Train batch 101400: loss: 0.02 grad norm: 4.85 time: 0.083
2025-07-30 04:06:33,405: Train batch 101600: loss: 0.01 grad norm: 0.25 time: 0.089
2025-07-30 04:07:03,659: Train batch 101800: loss: 0.02 grad norm: 1.89 time: 0.094
2025-07-30 04:07:34,410: Train batch 102000: loss: 0.01 grad norm: 2.00 time: 0.086
2025-07-30 04:07:34,410: Running test after training batch: 102000
2025-07-30 04:07:47,948: Val batch 102000: PER (avg): 0.1022 CTC Loss (avg): 22.7011 time: 13.537
2025-07-30 04:07:47,948: t15.2023.08.13 val PER: 0.0686
2025-07-30 04:07:47,948: t15.2023.08.18 val PER: 0.0595
2025-07-30 04:07:47,948: t15.2023.08.20 val PER: 0.0516
2025-07-30 04:07:47,948: t15.2023.08.25 val PER: 0.0858
2025-07-30 04:07:47,949: t15.2023.08.27 val PER: 0.1543
2025-07-30 04:07:47,949: t15.2023.09.01 val PER: 0.0455
2025-07-30 04:07:47,949: t15.2023.09.03 val PER: 0.1116
2025-07-30 04:07:47,949: t15.2023.09.24 val PER: 0.0716
2025-07-30 04:07:47,949: t15.2023.09.29 val PER: 0.0913
2025-07-30 04:07:47,949: t15.2023.10.01 val PER: 0.1288
2025-07-30 04:07:47,949: t15.2023.10.06 val PER: 0.0581
2025-07-30 04:07:47,949: t15.2023.10.08 val PER: 0.1732
2025-07-30 04:07:47,949: t15.2023.10.13 val PER: 0.1412
2025-07-30 04:07:47,950: t15.2023.10.15 val PER: 0.1114
2025-07-30 04:07:47,950: t15.2023.10.20 val PER: 0.1577
2025-07-30 04:07:47,950: t15.2023.10.22 val PER: 0.0958
2025-07-30 04:07:47,950: t15.2023.11.03 val PER: 0.1526
2025-07-30 04:07:47,950: t15.2023.11.04 val PER: 0.0205
2025-07-30 04:07:47,950: t15.2023.11.17 val PER: 0.0233
2025-07-30 04:07:47,950: t15.2023.11.19 val PER: 0.0120
2025-07-30 04:07:47,950: t15.2023.11.26 val PER: 0.0435
2025-07-30 04:07:47,950: t15.2023.12.03 val PER: 0.0473
2025-07-30 04:07:47,951: t15.2023.12.08 val PER: 0.0320
2025-07-30 04:07:47,951: t15.2023.12.10 val PER: 0.0263
2025-07-30 04:07:47,951: t15.2023.12.17 val PER: 0.0686
2025-07-30 04:07:47,951: t15.2023.12.29 val PER: 0.0714
2025-07-30 04:07:47,951: t15.2024.02.25 val PER: 0.0744
2025-07-30 04:07:47,951: t15.2024.03.08 val PER: 0.1750
2025-07-30 04:07:47,951: t15.2024.03.15 val PER: 0.1764
2025-07-30 04:07:47,951: t15.2024.03.17 val PER: 0.0732
2025-07-30 04:07:47,951: t15.2024.05.10 val PER: 0.1233
2025-07-30 04:07:47,952: t15.2024.06.14 val PER: 0.0915
2025-07-30 04:07:47,952: t15.2024.07.19 val PER: 0.1477
2025-07-30 04:07:47,952: t15.2024.07.21 val PER: 0.0545
2025-07-30 04:07:47,952: t15.2024.07.28 val PER: 0.0897
2025-07-30 04:07:47,952: t15.2025.01.10 val PER: 0.2452
2025-07-30 04:07:47,952: t15.2025.01.12 val PER: 0.0862
2025-07-30 04:07:47,952: t15.2025.03.14 val PER: 0.3180
2025-07-30 04:07:47,952: t15.2025.03.16 val PER: 0.1479
2025-07-30 04:07:47,953: t15.2025.03.30 val PER: 0.2172
2025-07-30 04:07:47,953: t15.2025.04.13 val PER: 0.1869
2025-07-30 04:08:17,570: Train batch 102200: loss: 0.03 grad norm: 5.40 time: 0.087
2025-07-30 04:08:47,334: Train batch 102400: loss: 0.01 grad norm: 0.37 time: 0.101
2025-07-30 04:09:17,652: Train batch 102600: loss: 0.03 grad norm: 2.63 time: 0.103
2025-07-30 04:09:47,806: Train batch 102800: loss: 0.04 grad norm: 3.98 time: 0.083
2025-07-30 04:10:18,044: Train batch 103000: loss: 0.01 grad norm: 0.43 time: 0.088
2025-07-30 04:10:48,314: Train batch 103200: loss: 0.01 grad norm: 0.41 time: 0.121
2025-07-30 04:11:18,960: Train batch 103400: loss: 0.01 grad norm: 0.85 time: 0.108
2025-07-30 04:11:49,377: Train batch 103600: loss: 0.01 grad norm: 1.80 time: 0.077
2025-07-30 04:12:20,110: Train batch 103800: loss: 0.06 grad norm: 1.26 time: 0.097
2025-07-30 04:12:50,933: Train batch 104000: loss: 0.01 grad norm: 0.21 time: 0.117
2025-07-30 04:12:50,933: Running test after training batch: 104000
2025-07-30 04:13:04,812: Val batch 104000: PER (avg): 0.1020 CTC Loss (avg): 22.7237 time: 13.879
2025-07-30 04:13:04,812: t15.2023.08.13 val PER: 0.0707
2025-07-30 04:13:04,813: t15.2023.08.18 val PER: 0.0595
2025-07-30 04:13:04,813: t15.2023.08.20 val PER: 0.0540
2025-07-30 04:13:04,813: t15.2023.08.25 val PER: 0.0828
2025-07-30 04:13:04,813: t15.2023.08.27 val PER: 0.1495
2025-07-30 04:13:04,813: t15.2023.09.01 val PER: 0.0455
2025-07-30 04:13:04,813: t15.2023.09.03 val PER: 0.1116
2025-07-30 04:13:04,813: t15.2023.09.24 val PER: 0.0716
2025-07-30 04:13:04,813: t15.2023.09.29 val PER: 0.0919
2025-07-30 04:13:04,813: t15.2023.10.01 val PER: 0.1295
2025-07-30 04:13:04,813: t15.2023.10.06 val PER: 0.0592
2025-07-30 04:13:04,814: t15.2023.10.08 val PER: 0.1691
2025-07-30 04:13:04,814: t15.2023.10.13 val PER: 0.1396
2025-07-30 04:13:04,814: t15.2023.10.15 val PER: 0.1094
2025-07-30 04:13:04,814: t15.2023.10.20 val PER: 0.1577
2025-07-30 04:13:04,814: t15.2023.10.22 val PER: 0.0902
2025-07-30 04:13:04,814: t15.2023.11.03 val PER: 0.1479
2025-07-30 04:13:04,814: t15.2023.11.04 val PER: 0.0171
2025-07-30 04:13:04,814: t15.2023.11.17 val PER: 0.0249
2025-07-30 04:13:04,814: t15.2023.11.19 val PER: 0.0120
2025-07-30 04:13:04,814: t15.2023.11.26 val PER: 0.0442
2025-07-30 04:13:04,815: t15.2023.12.03 val PER: 0.0494
2025-07-30 04:13:04,815: t15.2023.12.08 val PER: 0.0320
2025-07-30 04:13:04,815: t15.2023.12.10 val PER: 0.0223
2025-07-30 04:13:04,815: t15.2023.12.17 val PER: 0.0696
2025-07-30 04:13:04,815: t15.2023.12.29 val PER: 0.0707
2025-07-30 04:13:04,815: t15.2024.02.25 val PER: 0.0758
2025-07-30 04:13:04,815: t15.2024.03.08 val PER: 0.1821
2025-07-30 04:13:04,815: t15.2024.03.15 val PER: 0.1764
2025-07-30 04:13:04,815: t15.2024.03.17 val PER: 0.0746
2025-07-30 04:13:04,816: t15.2024.05.10 val PER: 0.1189
2025-07-30 04:13:04,816: t15.2024.06.14 val PER: 0.0946
2025-07-30 04:13:04,816: t15.2024.07.19 val PER: 0.1490
2025-07-30 04:13:04,816: t15.2024.07.21 val PER: 0.0552
2025-07-30 04:13:04,816: t15.2024.07.28 val PER: 0.0904
2025-07-30 04:13:04,816: t15.2025.01.10 val PER: 0.2479
2025-07-30 04:13:04,816: t15.2025.01.12 val PER: 0.0870
2025-07-30 04:13:04,816: t15.2025.03.14 val PER: 0.3166
2025-07-30 04:13:04,816: t15.2025.03.16 val PER: 0.1453
2025-07-30 04:13:04,817: t15.2025.03.30 val PER: 0.2195
2025-07-30 04:13:04,817: t15.2025.04.13 val PER: 0.1797
2025-07-30 04:13:34,581: Train batch 104200: loss: 0.01 grad norm: 1.32 time: 0.088
2025-07-30 04:14:05,075: Train batch 104400: loss: 0.01 grad norm: 0.31 time: 0.121
2025-07-30 04:14:35,426: Train batch 104600: loss: 0.01 grad norm: 0.57 time: 0.091
2025-07-30 04:15:05,540: Train batch 104800: loss: 0.01 grad norm: 0.27 time: 0.090
2025-07-30 04:15:35,378: Train batch 105000: loss: 0.00 grad norm: 0.67 time: 0.078
2025-07-30 04:16:05,445: Train batch 105200: loss: 0.01 grad norm: 1.07 time: 0.084
2025-07-30 04:16:35,667: Train batch 105400: loss: 0.01 grad norm: 1.84 time: 0.098
2025-07-30 04:17:05,713: Train batch 105600: loss: 0.00 grad norm: 0.23 time: 0.086
2025-07-30 04:17:35,916: Train batch 105800: loss: 0.01 grad norm: 0.87 time: 0.079
2025-07-30 04:18:05,992: Train batch 106000: loss: 0.01 grad norm: 1.01 time: 0.086
2025-07-30 04:18:05,992: Running test after training batch: 106000
2025-07-30 04:18:19,574: Val batch 106000: PER (avg): 0.1019 CTC Loss (avg): 22.7569 time: 13.581
2025-07-30 04:18:19,574: t15.2023.08.13 val PER: 0.0717
2025-07-30 04:18:19,575: t15.2023.08.18 val PER: 0.0587
2025-07-30 04:18:19,575: t15.2023.08.20 val PER: 0.0485
2025-07-30 04:18:19,575: t15.2023.08.25 val PER: 0.0828
2025-07-30 04:18:19,575: t15.2023.08.27 val PER: 0.1527
2025-07-30 04:18:19,575: t15.2023.09.01 val PER: 0.0455
2025-07-30 04:18:19,575: t15.2023.09.03 val PER: 0.1105
2025-07-30 04:18:19,576: t15.2023.09.24 val PER: 0.0680
2025-07-30 04:18:19,576: t15.2023.09.29 val PER: 0.0964
2025-07-30 04:18:19,576: t15.2023.10.01 val PER: 0.1281
2025-07-30 04:18:19,576: t15.2023.10.06 val PER: 0.0571
2025-07-30 04:18:19,576: t15.2023.10.08 val PER: 0.1678
2025-07-30 04:18:19,576: t15.2023.10.13 val PER: 0.1389
2025-07-30 04:18:19,576: t15.2023.10.15 val PER: 0.1068
2025-07-30 04:18:19,576: t15.2023.10.20 val PER: 0.1644
2025-07-30 04:18:19,577: t15.2023.10.22 val PER: 0.0947
2025-07-30 04:18:19,577: t15.2023.11.03 val PER: 0.1479
2025-07-30 04:18:19,577: t15.2023.11.04 val PER: 0.0171
2025-07-30 04:18:19,577: t15.2023.11.17 val PER: 0.0233
2025-07-30 04:18:19,577: t15.2023.11.19 val PER: 0.0140
2025-07-30 04:18:19,577: t15.2023.11.26 val PER: 0.0435
2025-07-30 04:18:19,577: t15.2023.12.03 val PER: 0.0473
2025-07-30 04:18:19,577: t15.2023.12.08 val PER: 0.0300
2025-07-30 04:18:19,577: t15.2023.12.10 val PER: 0.0250
2025-07-30 04:18:19,577: t15.2023.12.17 val PER: 0.0707
2025-07-30 04:18:19,578: t15.2023.12.29 val PER: 0.0707
2025-07-30 04:18:19,578: t15.2024.02.25 val PER: 0.0744
2025-07-30 04:18:19,578: t15.2024.03.08 val PER: 0.1792
2025-07-30 04:18:19,578: t15.2024.03.15 val PER: 0.1789
2025-07-30 04:18:19,578: t15.2024.03.17 val PER: 0.0767
2025-07-30 04:18:19,578: t15.2024.05.10 val PER: 0.1218
2025-07-30 04:18:19,578: t15.2024.06.14 val PER: 0.0915
2025-07-30 04:18:19,578: t15.2024.07.19 val PER: 0.1516
2025-07-30 04:18:19,578: t15.2024.07.21 val PER: 0.0572
2025-07-30 04:18:19,578: t15.2024.07.28 val PER: 0.0868
2025-07-30 04:18:19,579: t15.2025.01.10 val PER: 0.2424
2025-07-30 04:18:19,579: t15.2025.01.12 val PER: 0.0878
2025-07-30 04:18:19,579: t15.2025.03.14 val PER: 0.3151
2025-07-30 04:18:19,579: t15.2025.03.16 val PER: 0.1479
2025-07-30 04:18:19,579: t15.2025.03.30 val PER: 0.2184
2025-07-30 04:18:19,579: t15.2025.04.13 val PER: 0.1840
2025-07-30 04:18:49,336: Train batch 106200: loss: 0.01 grad norm: 0.58 time: 0.080
2025-07-30 04:19:19,762: Train batch 106400: loss: 0.01 grad norm: 0.37 time: 0.131
2025-07-30 04:19:49,659: Train batch 106600: loss: 0.00 grad norm: 0.36 time: 0.103
2025-07-30 04:20:19,805: Train batch 106800: loss: 0.02 grad norm: 1.20 time: 0.093
2025-07-30 04:20:50,262: Train batch 107000: loss: 0.06 grad norm: 4.90 time: 0.108
2025-07-30 04:21:20,218: Train batch 107200: loss: 0.02 grad norm: 2.12 time: 0.090
2025-07-30 04:21:50,276: Train batch 107400: loss: 0.02 grad norm: 1.07 time: 0.088
2025-07-30 04:22:20,254: Train batch 107600: loss: 0.02 grad norm: 1.54 time: 0.092
2025-07-30 04:22:50,398: Train batch 107800: loss: 0.01 grad norm: 0.30 time: 0.094
2025-07-30 04:23:20,396: Train batch 108000: loss: 0.01 grad norm: 0.40 time: 0.090
2025-07-30 04:23:20,396: Running test after training batch: 108000
2025-07-30 04:23:33,730: Val batch 108000: PER (avg): 0.1017 CTC Loss (avg): 22.5969 time: 13.334
2025-07-30 04:23:33,731: t15.2023.08.13 val PER: 0.0717
2025-07-30 04:23:33,731: t15.2023.08.18 val PER: 0.0629
2025-07-30 04:23:33,731: t15.2023.08.20 val PER: 0.0524
2025-07-30 04:23:33,731: t15.2023.08.25 val PER: 0.0858
2025-07-30 04:23:33,732: t15.2023.08.27 val PER: 0.1479
2025-07-30 04:23:33,732: t15.2023.09.01 val PER: 0.0422
2025-07-30 04:23:33,732: t15.2023.09.03 val PER: 0.1128
2025-07-30 04:23:33,732: t15.2023.09.24 val PER: 0.0680
2025-07-30 04:23:33,732: t15.2023.09.29 val PER: 0.0970
2025-07-30 04:23:33,732: t15.2023.10.01 val PER: 0.1288
2025-07-30 04:23:33,733: t15.2023.10.06 val PER: 0.0581
2025-07-30 04:23:33,733: t15.2023.10.08 val PER: 0.1624
2025-07-30 04:23:33,733: t15.2023.10.13 val PER: 0.1404
2025-07-30 04:23:33,733: t15.2023.10.15 val PER: 0.1081
2025-07-30 04:23:33,733: t15.2023.10.20 val PER: 0.1577
2025-07-30 04:23:33,733: t15.2023.10.22 val PER: 0.0958
2025-07-30 04:23:33,733: t15.2023.11.03 val PER: 0.1472
2025-07-30 04:23:33,734: t15.2023.11.04 val PER: 0.0137
2025-07-30 04:23:33,734: t15.2023.11.17 val PER: 0.0249
2025-07-30 04:23:33,734: t15.2023.11.19 val PER: 0.0120
2025-07-30 04:23:33,734: t15.2023.11.26 val PER: 0.0406
2025-07-30 04:23:33,734: t15.2023.12.03 val PER: 0.0483
2025-07-30 04:23:33,734: t15.2023.12.08 val PER: 0.0293
2025-07-30 04:23:33,735: t15.2023.12.10 val PER: 0.0250
2025-07-30 04:23:33,735: t15.2023.12.17 val PER: 0.0696
2025-07-30 04:23:33,735: t15.2023.12.29 val PER: 0.0728
2025-07-30 04:23:33,735: t15.2024.02.25 val PER: 0.0758
2025-07-30 04:23:33,735: t15.2024.03.08 val PER: 0.1792
2025-07-30 04:23:33,735: t15.2024.03.15 val PER: 0.1745
2025-07-30 04:23:33,735: t15.2024.03.17 val PER: 0.0746
2025-07-30 04:23:33,736: t15.2024.05.10 val PER: 0.1233
2025-07-30 04:23:33,736: t15.2024.06.14 val PER: 0.0931
2025-07-30 04:23:33,736: t15.2024.07.19 val PER: 0.1470
2025-07-30 04:23:33,736: t15.2024.07.21 val PER: 0.0545
2025-07-30 04:23:33,736: t15.2024.07.28 val PER: 0.0882
2025-07-30 04:23:33,736: t15.2025.01.10 val PER: 0.2466
2025-07-30 04:23:33,737: t15.2025.01.12 val PER: 0.0878
2025-07-30 04:23:33,737: t15.2025.03.14 val PER: 0.3225
2025-07-30 04:23:33,737: t15.2025.03.16 val PER: 0.1440
2025-07-30 04:23:33,737: t15.2025.03.30 val PER: 0.2172
2025-07-30 04:23:33,737: t15.2025.04.13 val PER: 0.1826
2025-07-30 04:24:03,248: Train batch 108200: loss: 0.01 grad norm: 0.61 time: 0.095
2025-07-30 04:24:33,689: Train batch 108400: loss: 0.02 grad norm: 2.38 time: 0.079
2025-07-30 04:25:03,615: Train batch 108600: loss: 0.02 grad norm: 1.87 time: 0.079
2025-07-30 04:25:33,816: Train batch 108800: loss: 0.01 grad norm: 0.52 time: 0.091
2025-07-30 04:26:04,333: Train batch 109000: loss: 0.00 grad norm: 0.21 time: 0.072
2025-07-30 04:26:34,614: Train batch 109200: loss: 0.02 grad norm: 1.06 time: 0.109
2025-07-30 04:27:05,296: Train batch 109400: loss: 0.01 grad norm: 0.39 time: 0.087
2025-07-30 04:27:36,506: Train batch 109600: loss: 0.01 grad norm: 0.47 time: 0.094
2025-07-30 04:28:06,266: Train batch 109800: loss: 0.02 grad norm: 2.88 time: 0.111
2025-07-30 04:28:36,065: Train batch 110000: loss: 0.02 grad norm: 2.15 time: 0.094
2025-07-30 04:28:36,065: Running test after training batch: 110000
2025-07-30 04:28:49,183: Val batch 110000: PER (avg): 0.1012 CTC Loss (avg): 22.5975 time: 13.118
2025-07-30 04:28:49,184: t15.2023.08.13 val PER: 0.0707
2025-07-30 04:28:49,184: t15.2023.08.18 val PER: 0.0604
2025-07-30 04:28:49,184: t15.2023.08.20 val PER: 0.0500
2025-07-30 04:28:49,184: t15.2023.08.25 val PER: 0.0828
2025-07-30 04:28:49,184: t15.2023.08.27 val PER: 0.1495
2025-07-30 04:28:49,184: t15.2023.09.01 val PER: 0.0430
2025-07-30 04:28:49,184: t15.2023.09.03 val PER: 0.1081
2025-07-30 04:28:49,185: t15.2023.09.24 val PER: 0.0704
2025-07-30 04:28:49,185: t15.2023.09.29 val PER: 0.0938
2025-07-30 04:28:49,185: t15.2023.10.01 val PER: 0.1288
2025-07-30 04:28:49,185: t15.2023.10.06 val PER: 0.0571
2025-07-30 04:28:49,185: t15.2023.10.08 val PER: 0.1664
2025-07-30 04:28:49,185: t15.2023.10.13 val PER: 0.1365
2025-07-30 04:28:49,185: t15.2023.10.15 val PER: 0.1074
2025-07-30 04:28:49,185: t15.2023.10.20 val PER: 0.1644
2025-07-30 04:28:49,186: t15.2023.10.22 val PER: 0.0958
2025-07-30 04:28:49,186: t15.2023.11.03 val PER: 0.1486
2025-07-30 04:28:49,186: t15.2023.11.04 val PER: 0.0137
2025-07-30 04:28:49,186: t15.2023.11.17 val PER: 0.0233
2025-07-30 04:28:49,186: t15.2023.11.19 val PER: 0.0140
2025-07-30 04:28:49,186: t15.2023.11.26 val PER: 0.0413
2025-07-30 04:28:49,186: t15.2023.12.03 val PER: 0.0494
2025-07-30 04:28:49,186: t15.2023.12.08 val PER: 0.0306
2025-07-30 04:28:49,186: t15.2023.12.10 val PER: 0.0237
2025-07-30 04:28:49,187: t15.2023.12.17 val PER: 0.0665
2025-07-30 04:28:49,187: t15.2023.12.29 val PER: 0.0700
2025-07-30 04:28:49,187: t15.2024.02.25 val PER: 0.0744
2025-07-30 04:28:49,187: t15.2024.03.08 val PER: 0.1821
2025-07-30 04:28:49,187: t15.2024.03.15 val PER: 0.1745
2025-07-30 04:28:49,187: t15.2024.03.17 val PER: 0.0746
2025-07-30 04:28:49,187: t15.2024.05.10 val PER: 0.1204
2025-07-30 04:28:49,187: t15.2024.06.14 val PER: 0.0962
2025-07-30 04:28:49,188: t15.2024.07.19 val PER: 0.1490
2025-07-30 04:28:49,188: t15.2024.07.21 val PER: 0.0531
2025-07-30 04:28:49,188: t15.2024.07.28 val PER: 0.0882
2025-07-30 04:28:49,188: t15.2025.01.10 val PER: 0.2452
2025-07-30 04:28:49,188: t15.2025.01.12 val PER: 0.0847
2025-07-30 04:28:49,188: t15.2025.03.14 val PER: 0.3240
2025-07-30 04:28:49,188: t15.2025.03.16 val PER: 0.1466
2025-07-30 04:28:49,188: t15.2025.03.30 val PER: 0.2161
2025-07-30 04:28:49,189: t15.2025.04.13 val PER: 0.1783
2025-07-30 04:28:49,189: New best test PER 0.1016 --> 0.1012
2025-07-30 04:28:49,189: Checkpointing model
2025-07-30 04:28:50,378: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-07-30 04:29:20,440: Train batch 110200: loss: 0.01 grad norm: 0.29 time: 0.110
2025-07-30 04:29:50,272: Train batch 110400: loss: 0.00 grad norm: 0.26 time: 0.081
2025-07-30 04:30:20,800: Train batch 110600: loss: 0.01 grad norm: 0.41 time: 0.111
2025-07-30 04:30:51,252: Train batch 110800: loss: 0.01 grad norm: 0.41 time: 0.091
2025-07-30 04:31:21,693: Train batch 111000: loss: 0.01 grad norm: 0.88 time: 0.130
2025-07-30 04:31:52,146: Train batch 111200: loss: 0.00 grad norm: 0.13 time: 0.113
2025-07-30 04:32:22,781: Train batch 111400: loss: 0.01 grad norm: 0.99 time: 0.100
2025-07-30 04:32:52,869: Train batch 111600: loss: 0.01 grad norm: 0.58 time: 0.081
2025-07-30 04:33:23,657: Train batch 111800: loss: 0.01 grad norm: 0.32 time: 0.107
2025-07-30 04:33:53,676: Train batch 112000: loss: 0.01 grad norm: 0.21 time: 0.094
2025-07-30 04:33:53,676: Running test after training batch: 112000
2025-07-30 04:34:06,901: Val batch 112000: PER (avg): 0.1005 CTC Loss (avg): 22.6048 time: 13.225
2025-07-30 04:34:06,901: t15.2023.08.13 val PER: 0.0686
2025-07-30 04:34:06,901: t15.2023.08.18 val PER: 0.0604
2025-07-30 04:34:06,901: t15.2023.08.20 val PER: 0.0492
2025-07-30 04:34:06,902: t15.2023.08.25 val PER: 0.0828
2025-07-30 04:34:06,902: t15.2023.08.27 val PER: 0.1495
2025-07-30 04:34:06,902: t15.2023.09.01 val PER: 0.0398
2025-07-30 04:34:06,902: t15.2023.09.03 val PER: 0.1081
2025-07-30 04:34:06,902: t15.2023.09.24 val PER: 0.0680
2025-07-30 04:34:06,902: t15.2023.09.29 val PER: 0.0938
2025-07-30 04:34:06,902: t15.2023.10.01 val PER: 0.1275
2025-07-30 04:34:06,903: t15.2023.10.06 val PER: 0.0592
2025-07-30 04:34:06,903: t15.2023.10.08 val PER: 0.1664
2025-07-30 04:34:06,903: t15.2023.10.13 val PER: 0.1373
2025-07-30 04:34:06,903: t15.2023.10.15 val PER: 0.1081
2025-07-30 04:34:06,903: t15.2023.10.20 val PER: 0.1611
2025-07-30 04:34:06,903: t15.2023.10.22 val PER: 0.0947
2025-07-30 04:34:06,903: t15.2023.11.03 val PER: 0.1513
2025-07-30 04:34:06,903: t15.2023.11.04 val PER: 0.0137
2025-07-30 04:34:06,903: t15.2023.11.17 val PER: 0.0233
2025-07-30 04:34:06,904: t15.2023.11.19 val PER: 0.0120
2025-07-30 04:34:06,904: t15.2023.11.26 val PER: 0.0406
2025-07-30 04:34:06,904: t15.2023.12.03 val PER: 0.0473
2025-07-30 04:34:06,904: t15.2023.12.08 val PER: 0.0313
2025-07-30 04:34:06,904: t15.2023.12.10 val PER: 0.0223
2025-07-30 04:34:06,904: t15.2023.12.17 val PER: 0.0686
2025-07-30 04:34:06,904: t15.2023.12.29 val PER: 0.0693
2025-07-30 04:34:06,904: t15.2024.02.25 val PER: 0.0758
2025-07-30 04:34:06,904: t15.2024.03.08 val PER: 0.1750
2025-07-30 04:34:06,905: t15.2024.03.15 val PER: 0.1732
2025-07-30 04:34:06,905: t15.2024.03.17 val PER: 0.0739
2025-07-30 04:34:06,905: t15.2024.05.10 val PER: 0.1189
2025-07-30 04:34:06,905: t15.2024.06.14 val PER: 0.0962
2025-07-30 04:34:06,905: t15.2024.07.19 val PER: 0.1450
2025-07-30 04:34:06,905: t15.2024.07.21 val PER: 0.0538
2025-07-30 04:34:06,905: t15.2024.07.28 val PER: 0.0875
2025-07-30 04:34:06,905: t15.2025.01.10 val PER: 0.2397
2025-07-30 04:34:06,906: t15.2025.01.12 val PER: 0.0878
2025-07-30 04:34:06,906: t15.2025.03.14 val PER: 0.3166
2025-07-30 04:34:06,906: t15.2025.03.16 val PER: 0.1440
2025-07-30 04:34:06,906: t15.2025.03.30 val PER: 0.2161
2025-07-30 04:34:06,906: t15.2025.04.13 val PER: 0.1797
2025-07-30 04:34:06,906: New best test PER 0.1012 --> 0.1005
2025-07-30 04:34:06,906: Checkpointing model
2025-07-30 04:34:08,091: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-07-30 04:34:38,059: Train batch 112200: loss: 0.02 grad norm: 1.41 time: 0.076
2025-07-30 04:35:08,003: Train batch 112400: loss: 0.01 grad norm: 0.68 time: 0.100
2025-07-30 04:35:38,031: Train batch 112600: loss: 0.01 grad norm: 0.66 time: 0.073
2025-07-30 04:36:08,494: Train batch 112800: loss: 0.10 grad norm: 2.78 time: 0.092
2025-07-30 04:36:39,155: Train batch 113000: loss: 0.03 grad norm: 8.00 time: 0.089
2025-07-30 04:37:09,800: Train batch 113200: loss: 0.02 grad norm: 1.05 time: 0.100
2025-07-30 04:37:40,219: Train batch 113400: loss: 0.00 grad norm: 0.19 time: 0.106
2025-07-30 04:38:10,571: Train batch 113600: loss: 0.01 grad norm: 0.68 time: 0.102
2025-07-30 04:38:41,277: Train batch 113800: loss: 0.01 grad norm: 0.96 time: 0.089
2025-07-30 04:39:12,093: Train batch 114000: loss: 0.00 grad norm: 0.31 time: 0.116
2025-07-30 04:39:12,094: Running test after training batch: 114000
2025-07-30 04:39:25,701: Val batch 114000: PER (avg): 0.1005 CTC Loss (avg): 22.5647 time: 13.607
2025-07-30 04:39:25,701: t15.2023.08.13 val PER: 0.0696
2025-07-30 04:39:25,701: t15.2023.08.18 val PER: 0.0578
2025-07-30 04:39:25,701: t15.2023.08.20 val PER: 0.0500
2025-07-30 04:39:25,701: t15.2023.08.25 val PER: 0.0828
2025-07-30 04:39:25,702: t15.2023.08.27 val PER: 0.1495
2025-07-30 04:39:25,702: t15.2023.09.01 val PER: 0.0455
2025-07-30 04:39:25,702: t15.2023.09.03 val PER: 0.1128
2025-07-30 04:39:25,702: t15.2023.09.24 val PER: 0.0655
2025-07-30 04:39:25,702: t15.2023.09.29 val PER: 0.0932
2025-07-30 04:39:25,702: t15.2023.10.01 val PER: 0.1262
2025-07-30 04:39:25,702: t15.2023.10.06 val PER: 0.0560
2025-07-30 04:39:25,702: t15.2023.10.08 val PER: 0.1678
2025-07-30 04:39:25,702: t15.2023.10.13 val PER: 0.1373
2025-07-30 04:39:25,703: t15.2023.10.15 val PER: 0.1081
2025-07-30 04:39:25,703: t15.2023.10.20 val PER: 0.1644
2025-07-30 04:39:25,703: t15.2023.10.22 val PER: 0.0947
2025-07-30 04:39:25,703: t15.2023.11.03 val PER: 0.1499
2025-07-30 04:39:25,703: t15.2023.11.04 val PER: 0.0137
2025-07-30 04:39:25,703: t15.2023.11.17 val PER: 0.0249
2025-07-30 04:39:25,703: t15.2023.11.19 val PER: 0.0120
2025-07-30 04:39:25,703: t15.2023.11.26 val PER: 0.0420
2025-07-30 04:39:25,703: t15.2023.12.03 val PER: 0.0483
2025-07-30 04:39:25,703: t15.2023.12.08 val PER: 0.0313
2025-07-30 04:39:25,704: t15.2023.12.10 val PER: 0.0223
2025-07-30 04:39:25,704: t15.2023.12.17 val PER: 0.0644
2025-07-30 04:39:25,704: t15.2023.12.29 val PER: 0.0679
2025-07-30 04:39:25,704: t15.2024.02.25 val PER: 0.0730
2025-07-30 04:39:25,704: t15.2024.03.08 val PER: 0.1792
2025-07-30 04:39:25,704: t15.2024.03.15 val PER: 0.1739
2025-07-30 04:39:25,704: t15.2024.03.17 val PER: 0.0732
2025-07-30 04:39:25,704: t15.2024.05.10 val PER: 0.1174
2025-07-30 04:39:25,705: t15.2024.06.14 val PER: 0.0946
2025-07-30 04:39:25,705: t15.2024.07.19 val PER: 0.1490
2025-07-30 04:39:25,705: t15.2024.07.21 val PER: 0.0531
2025-07-30 04:39:25,705: t15.2024.07.28 val PER: 0.0890
2025-07-30 04:39:25,705: t15.2025.01.10 val PER: 0.2383
2025-07-30 04:39:25,705: t15.2025.01.12 val PER: 0.0862
2025-07-30 04:39:25,705: t15.2025.03.14 val PER: 0.3166
2025-07-30 04:39:25,705: t15.2025.03.16 val PER: 0.1414
2025-07-30 04:39:25,706: t15.2025.03.30 val PER: 0.2184
2025-07-30 04:39:25,706: t15.2025.04.13 val PER: 0.1755
2025-07-30 04:39:25,706: New best test loss 22.6048 --> 22.5647
2025-07-30 04:39:25,706: Checkpointing model
2025-07-30 04:39:26,858: Saved model to checkpoint: trained_models/baseline_rnn/checkpoint/best_checkpoint
2025-07-30 04:39:57,200: Train batch 114200: loss: 0.00 grad norm: 0.16 time: 0.089
2025-07-30 04:40:27,272: Train batch 114400: loss: 0.00 grad norm: 0.22 time: 0.083
2025-07-30 04:40:57,706: Train batch 114600: loss: 0.03 grad norm: 2.80 time: 0.088
2025-07-30 04:41:28,172: Train batch 114800: loss: 0.01 grad norm: 0.25 time: 0.103
2025-07-30 04:41:58,448: Train batch 115000: loss: 0.00 grad norm: 0.12 time: 0.082
2025-07-30 04:42:28,706: Train batch 115200: loss: 0.01 grad norm: 3.53 time: 0.084
2025-07-30 04:42:58,816: Train batch 115400: loss: 0.01 grad norm: 1.07 time: 0.103
2025-07-30 04:43:29,688: Train batch 115600: loss: 0.01 grad norm: 0.28 time: 0.094
2025-07-30 04:43:59,857: Train batch 115800: loss: 0.01 grad norm: 1.16 time: 0.078
2025-07-30 04:44:30,197: Train batch 116000: loss: 0.01 grad norm: 1.21 time: 0.083
2025-07-30 04:44:30,197: Running test after training batch: 116000
2025-07-30 04:44:43,367: Val batch 116000: PER (avg): 0.1009 CTC Loss (avg): 22.5608 time: 13.170
2025-07-30 04:44:43,368: t15.2023.08.13 val PER: 0.0717
2025-07-30 04:44:43,368: t15.2023.08.18 val PER: 0.0587
2025-07-30 04:44:43,368: t15.2023.08.20 val PER: 0.0508
2025-07-30 04:44:43,368: t15.2023.08.25 val PER: 0.0828
2025-07-30 04:44:43,368: t15.2023.08.27 val PER: 0.1511
2025-07-30 04:44:43,368: t15.2023.09.01 val PER: 0.0487
2025-07-30 04:44:43,368: t15.2023.09.03 val PER: 0.1116
2025-07-30 04:44:43,369: t15.2023.09.24 val PER: 0.0692
2025-07-30 04:44:43,369: t15.2023.09.29 val PER: 0.0944
2025-07-30 04:44:43,369: t15.2023.10.01 val PER: 0.1262
2025-07-30 04:44:43,369: t15.2023.10.06 val PER: 0.0581
2025-07-30 04:44:43,369: t15.2023.10.08 val PER: 0.1719
2025-07-30 04:44:43,369: t15.2023.10.13 val PER: 0.1365
2025-07-30 04:44:43,369: t15.2023.10.15 val PER: 0.1101
2025-07-30 04:44:43,369: t15.2023.10.20 val PER: 0.1644
2025-07-30 04:44:43,369: t15.2023.10.22 val PER: 0.0958
2025-07-30 04:44:43,369: t15.2023.11.03 val PER: 0.1513
2025-07-30 04:44:43,370: t15.2023.11.04 val PER: 0.0137
2025-07-30 04:44:43,370: t15.2023.11.17 val PER: 0.0249
2025-07-30 04:44:43,370: t15.2023.11.19 val PER: 0.0120
2025-07-30 04:44:43,370: t15.2023.11.26 val PER: 0.0399
2025-07-30 04:44:43,370: t15.2023.12.03 val PER: 0.0483
2025-07-30 04:44:43,370: t15.2023.12.08 val PER: 0.0326
2025-07-30 04:44:43,370: t15.2023.12.10 val PER: 0.0237
2025-07-30 04:44:43,370: t15.2023.12.17 val PER: 0.0624
2025-07-30 04:44:43,370: t15.2023.12.29 val PER: 0.0679
2025-07-30 04:44:43,370: t15.2024.02.25 val PER: 0.0744
2025-07-30 04:44:43,371: t15.2024.03.08 val PER: 0.1792
2025-07-30 04:44:43,371: t15.2024.03.15 val PER: 0.1726
2025-07-30 04:44:43,371: t15.2024.03.17 val PER: 0.0725
2025-07-30 04:44:43,371: t15.2024.05.10 val PER: 0.1174
2025-07-30 04:44:43,371: t15.2024.06.14 val PER: 0.0978
2025-07-30 04:44:43,371: t15.2024.07.19 val PER: 0.1470
2025-07-30 04:44:43,371: t15.2024.07.21 val PER: 0.0538
2025-07-30 04:44:43,371: t15.2024.07.28 val PER: 0.0875
2025-07-30 04:44:43,372: t15.2025.01.10 val PER: 0.2397
2025-07-30 04:44:43,372: t15.2025.01.12 val PER: 0.0862
2025-07-30 04:44:43,372: t15.2025.03.14 val PER: 0.3151
2025-07-30 04:44:43,372: t15.2025.03.16 val PER: 0.1427
2025-07-30 04:44:43,372: t15.2025.03.30 val PER: 0.2172
2025-07-30 04:44:43,372: t15.2025.04.13 val PER: 0.1769
2025-07-30 04:45:13,671: Train batch 116200: loss: 0.01 grad norm: 1.20 time: 0.089
2025-07-30 04:45:43,855: Train batch 116400: loss: 0.01 grad norm: 0.44 time: 0.100
2025-07-30 04:46:14,522: Train batch 116600: loss: 0.06 grad norm: 2.97 time: 0.086
2025-07-30 04:46:44,941: Train batch 116800: loss: 0.04 grad norm: 6.78 time: 0.110
2025-07-30 04:47:15,041: Train batch 117000: loss: 0.00 grad norm: 0.45 time: 0.087
2025-07-30 04:47:45,334: Train batch 117200: loss: 0.01 grad norm: 0.86 time: 0.097
2025-07-30 04:48:15,385: Train batch 117400: loss: 0.01 grad norm: 0.60 time: 0.091
2025-07-30 04:48:46,337: Train batch 117600: loss: 0.01 grad norm: 0.75 time: 0.127
2025-07-30 04:49:16,505: Train batch 117800: loss: 0.19 grad norm: 8.36 time: 0.086
2025-07-30 04:49:46,866: Train batch 118000: loss: 0.02 grad norm: 7.71 time: 0.071
2025-07-30 04:49:46,866: Running test after training batch: 118000
2025-07-30 04:50:00,511: Val batch 118000: PER (avg): 0.1007 CTC Loss (avg): 22.5261 time: 13.645
2025-07-30 04:50:00,512: t15.2023.08.13 val PER: 0.0717
2025-07-30 04:50:00,512: t15.2023.08.18 val PER: 0.0604
2025-07-30 04:50:00,512: t15.2023.08.20 val PER: 0.0508
2025-07-30 04:50:00,512: t15.2023.08.25 val PER: 0.0828
2025-07-30 04:50:00,512: t15.2023.08.27 val PER: 0.1495
2025-07-30 04:50:00,512: t15.2023.09.01 val PER: 0.0479
2025-07-30 04:50:00,512: t15.2023.09.03 val PER: 0.1140
2025-07-30 04:50:00,512: t15.2023.09.24 val PER: 0.0692
2025-07-30 04:50:00,513: t15.2023.09.29 val PER: 0.0925
2025-07-30 04:50:00,513: t15.2023.10.01 val PER: 0.1262
2025-07-30 04:50:00,513: t15.2023.10.06 val PER: 0.0581
2025-07-30 04:50:00,513: t15.2023.10.08 val PER: 0.1678
2025-07-30 04:50:00,513: t15.2023.10.13 val PER: 0.1373
2025-07-30 04:50:00,513: t15.2023.10.15 val PER: 0.1088
2025-07-30 04:50:00,513: t15.2023.10.20 val PER: 0.1611
2025-07-30 04:50:00,513: t15.2023.10.22 val PER: 0.0958
2025-07-30 04:50:00,513: t15.2023.11.03 val PER: 0.1499
2025-07-30 04:50:00,513: t15.2023.11.04 val PER: 0.0137
2025-07-30 04:50:00,514: t15.2023.11.17 val PER: 0.0249
2025-07-30 04:50:00,514: t15.2023.11.19 val PER: 0.0120
2025-07-30 04:50:00,514: t15.2023.11.26 val PER: 0.0420
2025-07-30 04:50:00,514: t15.2023.12.03 val PER: 0.0473
2025-07-30 04:50:00,514: t15.2023.12.08 val PER: 0.0313
2025-07-30 04:50:00,514: t15.2023.12.10 val PER: 0.0237
2025-07-30 04:50:00,514: t15.2023.12.17 val PER: 0.0665
2025-07-30 04:50:00,514: t15.2023.12.29 val PER: 0.0679
2025-07-30 04:50:00,514: t15.2024.02.25 val PER: 0.0730
2025-07-30 04:50:00,515: t15.2024.03.08 val PER: 0.1792
2025-07-30 04:50:00,515: t15.2024.03.15 val PER: 0.1732
2025-07-30 04:50:00,515: t15.2024.03.17 val PER: 0.0718
2025-07-30 04:50:00,515: t15.2024.05.10 val PER: 0.1189
2025-07-30 04:50:00,515: t15.2024.06.14 val PER: 0.0994
2025-07-30 04:50:00,515: t15.2024.07.19 val PER: 0.1463
2025-07-30 04:50:00,515: t15.2024.07.21 val PER: 0.0538
2025-07-30 04:50:00,515: t15.2024.07.28 val PER: 0.0875
2025-07-30 04:50:00,516: t15.2025.01.10 val PER: 0.2369
2025-07-30 04:50:00,516: t15.2025.01.12 val PER: 0.0855
2025-07-30 04:50:00,516: t15.2025.03.14 val PER: 0.3077
2025-07-30 04:50:00,516: t15.2025.03.16 val PER: 0.1414
2025-07-30 04:50:00,516: t15.2025.03.30 val PER: 0.2172
2025-07-30 04:50:00,516: t15.2025.04.13 val PER: 0.1812
2025-07-30 04:50:30,422: Train batch 118200: loss: 0.03 grad norm: 2.67 time: 0.085
2025-07-30 04:51:00,632: Train batch 118400: loss: 0.01 grad norm: 0.62 time: 0.085
2025-07-30 04:51:30,786: Train batch 118600: loss: 0.03 grad norm: 2.48 time: 0.089
2025-07-30 04:52:01,091: Train batch 118800: loss: 0.03 grad norm: 2.17 time: 0.121
2025-07-30 04:52:31,463: Train batch 119000: loss: 0.00 grad norm: 0.16 time: 0.102
2025-07-30 04:53:01,668: Train batch 119200: loss: 0.00 grad norm: 4.88 time: 0.094
2025-07-30 04:53:32,203: Train batch 119400: loss: 0.01 grad norm: 0.62 time: 0.088
2025-07-30 04:54:02,714: Train batch 119600: loss: 0.01 grad norm: 0.77 time: 0.132
2025-07-30 04:54:33,211: Train batch 119800: loss: 0.01 grad norm: 0.40 time: 0.076
2025-07-30 04:55:04,045: Running test after training batch: 119999
2025-07-30 04:55:16,991: Val batch 119999: PER (avg): 0.1006 CTC Loss (avg): 22.5030 time: 12.946
2025-07-30 04:55:16,991: t15.2023.08.13 val PER: 0.0707
2025-07-30 04:55:16,991: t15.2023.08.18 val PER: 0.0612
2025-07-30 04:55:16,992: t15.2023.08.20 val PER: 0.0477
2025-07-30 04:55:16,992: t15.2023.08.25 val PER: 0.0828
2025-07-30 04:55:16,992: t15.2023.08.27 val PER: 0.1511
2025-07-30 04:55:16,992: t15.2023.09.01 val PER: 0.0471
2025-07-30 04:55:16,992: t15.2023.09.03 val PER: 0.1128
2025-07-30 04:55:16,992: t15.2023.09.24 val PER: 0.0655
2025-07-30 04:55:16,992: t15.2023.09.29 val PER: 0.0919
2025-07-30 04:55:16,992: t15.2023.10.01 val PER: 0.1288
2025-07-30 04:55:16,993: t15.2023.10.06 val PER: 0.0603
2025-07-30 04:55:16,993: t15.2023.10.08 val PER: 0.1678
2025-07-30 04:55:16,993: t15.2023.10.13 val PER: 0.1404
2025-07-30 04:55:16,993: t15.2023.10.15 val PER: 0.1081
2025-07-30 04:55:16,993: t15.2023.10.20 val PER: 0.1577
2025-07-30 04:55:16,993: t15.2023.10.22 val PER: 0.0935
2025-07-30 04:55:16,993: t15.2023.11.03 val PER: 0.1506
2025-07-30 04:55:16,993: t15.2023.11.04 val PER: 0.0137
2025-07-30 04:55:16,993: t15.2023.11.17 val PER: 0.0249
2025-07-30 04:55:16,994: t15.2023.11.19 val PER: 0.0120
2025-07-30 04:55:16,994: t15.2023.11.26 val PER: 0.0413
2025-07-30 04:55:16,994: t15.2023.12.03 val PER: 0.0473
2025-07-30 04:55:16,994: t15.2023.12.08 val PER: 0.0313
2025-07-30 04:55:16,994: t15.2023.12.10 val PER: 0.0250
2025-07-30 04:55:16,994: t15.2023.12.17 val PER: 0.0655
2025-07-30 04:55:16,994: t15.2023.12.29 val PER: 0.0693
2025-07-30 04:55:16,994: t15.2024.02.25 val PER: 0.0730
2025-07-30 04:55:16,995: t15.2024.03.08 val PER: 0.1750
2025-07-30 04:55:16,995: t15.2024.03.15 val PER: 0.1726
2025-07-30 04:55:16,995: t15.2024.03.17 val PER: 0.0711
2025-07-30 04:55:16,995: t15.2024.05.10 val PER: 0.1189
2025-07-30 04:55:16,995: t15.2024.06.14 val PER: 0.0994
2025-07-30 04:55:16,995: t15.2024.07.19 val PER: 0.1457
2025-07-30 04:55:16,995: t15.2024.07.21 val PER: 0.0545
2025-07-30 04:55:16,995: t15.2024.07.28 val PER: 0.0868
2025-07-30 04:55:16,996: t15.2025.01.10 val PER: 0.2355
2025-07-30 04:55:16,996: t15.2025.01.12 val PER: 0.0831
2025-07-30 04:55:16,996: t15.2025.03.14 val PER: 0.3151
2025-07-30 04:55:16,996: t15.2025.03.16 val PER: 0.1427
2025-07-30 04:55:16,996: t15.2025.03.30 val PER: 0.2195
2025-07-30 04:55:16,996: t15.2025.04.13 val PER: 0.1812
2025-07-30 04:55:17,052: Best avg val PER achieved: 0.10050
2025-07-30 04:55:17,052: Total training time: 316.94 minutes
